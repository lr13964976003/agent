// RA+SP Model Deployment DAG
digraph {
	graph [nodesep=0.8 rankdir=TB ranksep=1.0 splines=ortho]
	node [style=filled]
	Input_Total [label="Input\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]" fillcolor=lightblue shape=ellipse]
	Sequence_Split [label="Sequence Split\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: 16Ã—[batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightyellow shape=parallelogram]
	Input_Total -> Sequence_Split
	Layer0_Device0_Input [label="Layer 0 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Sequence_Split -> Layer0_Device0_Input
	Layer0_Device0_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device0_Input -> Layer0_Device0_LayerNorm1
	Layer0_Device0_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device0_LayerNorm1 -> Layer0_Device0_QKVProj
	Layer0_Device0_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device0_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage0_RecvKV -> Layer0_Device0_Stage0_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage0_Attention [label=Q_local]
	Layer0_Device0_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage0_Attention -> Layer0_Device0_Stage0_Accumulate
	Layer0_Device0_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage0_RecvKV -> Layer0_Device0_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage1_RecvKV -> Layer0_Device0_Stage1_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage1_Attention [label=Q_local]
	Layer0_Device0_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage1_Attention -> Layer0_Device0_Stage1_Accumulate
	Layer0_Device0_Stage0_Accumulate -> Layer0_Device0_Stage1_Accumulate
	Layer0_Device0_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage1_RecvKV -> Layer0_Device0_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage2_RecvKV -> Layer0_Device0_Stage2_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage2_Attention [label=Q_local]
	Layer0_Device0_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage2_Attention -> Layer0_Device0_Stage2_Accumulate
	Layer0_Device0_Stage1_Accumulate -> Layer0_Device0_Stage2_Accumulate
	Layer0_Device0_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage2_RecvKV -> Layer0_Device0_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage3_RecvKV -> Layer0_Device0_Stage3_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage3_Attention [label=Q_local]
	Layer0_Device0_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage3_Attention -> Layer0_Device0_Stage3_Accumulate
	Layer0_Device0_Stage2_Accumulate -> Layer0_Device0_Stage3_Accumulate
	Layer0_Device0_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage3_RecvKV -> Layer0_Device0_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage4_RecvKV -> Layer0_Device0_Stage4_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage4_Attention [label=Q_local]
	Layer0_Device0_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage4_Attention -> Layer0_Device0_Stage4_Accumulate
	Layer0_Device0_Stage3_Accumulate -> Layer0_Device0_Stage4_Accumulate
	Layer0_Device0_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage4_RecvKV -> Layer0_Device0_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage5_RecvKV -> Layer0_Device0_Stage5_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage5_Attention [label=Q_local]
	Layer0_Device0_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage5_Attention -> Layer0_Device0_Stage5_Accumulate
	Layer0_Device0_Stage4_Accumulate -> Layer0_Device0_Stage5_Accumulate
	Layer0_Device0_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage5_RecvKV -> Layer0_Device0_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage6_RecvKV -> Layer0_Device0_Stage6_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage6_Attention [label=Q_local]
	Layer0_Device0_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage6_Attention -> Layer0_Device0_Stage6_Accumulate
	Layer0_Device0_Stage5_Accumulate -> Layer0_Device0_Stage6_Accumulate
	Layer0_Device0_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage6_RecvKV -> Layer0_Device0_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage7_RecvKV -> Layer0_Device0_Stage7_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage7_Attention [label=Q_local]
	Layer0_Device0_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage7_Attention -> Layer0_Device0_Stage7_Accumulate
	Layer0_Device0_Stage6_Accumulate -> Layer0_Device0_Stage7_Accumulate
	Layer0_Device0_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage7_RecvKV -> Layer0_Device0_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage8_RecvKV -> Layer0_Device0_Stage8_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage8_Attention [label=Q_local]
	Layer0_Device0_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage8_Attention -> Layer0_Device0_Stage8_Accumulate
	Layer0_Device0_Stage7_Accumulate -> Layer0_Device0_Stage8_Accumulate
	Layer0_Device0_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage8_RecvKV -> Layer0_Device0_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage9_RecvKV -> Layer0_Device0_Stage9_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage9_Attention [label=Q_local]
	Layer0_Device0_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage9_Attention -> Layer0_Device0_Stage9_Accumulate
	Layer0_Device0_Stage8_Accumulate -> Layer0_Device0_Stage9_Accumulate
	Layer0_Device0_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage9_RecvKV -> Layer0_Device0_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage10_RecvKV -> Layer0_Device0_Stage10_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage10_Attention [label=Q_local]
	Layer0_Device0_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage10_Attention -> Layer0_Device0_Stage10_Accumulate
	Layer0_Device0_Stage9_Accumulate -> Layer0_Device0_Stage10_Accumulate
	Layer0_Device0_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage10_RecvKV -> Layer0_Device0_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage11_RecvKV -> Layer0_Device0_Stage11_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage11_Attention [label=Q_local]
	Layer0_Device0_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage11_Attention -> Layer0_Device0_Stage11_Accumulate
	Layer0_Device0_Stage10_Accumulate -> Layer0_Device0_Stage11_Accumulate
	Layer0_Device0_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage11_RecvKV -> Layer0_Device0_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage12_RecvKV -> Layer0_Device0_Stage12_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage12_Attention [label=Q_local]
	Layer0_Device0_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage12_Attention -> Layer0_Device0_Stage12_Accumulate
	Layer0_Device0_Stage11_Accumulate -> Layer0_Device0_Stage12_Accumulate
	Layer0_Device0_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage12_RecvKV -> Layer0_Device0_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage13_RecvKV -> Layer0_Device0_Stage13_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage13_Attention [label=Q_local]
	Layer0_Device0_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage13_Attention -> Layer0_Device0_Stage13_Accumulate
	Layer0_Device0_Stage12_Accumulate -> Layer0_Device0_Stage13_Accumulate
	Layer0_Device0_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage13_RecvKV -> Layer0_Device0_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage14_RecvKV -> Layer0_Device0_Stage14_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage14_Attention [label=Q_local]
	Layer0_Device0_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage14_Attention -> Layer0_Device0_Stage14_Accumulate
	Layer0_Device0_Stage13_Accumulate -> Layer0_Device0_Stage14_Accumulate
	Layer0_Device0_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device0_Stage14_RecvKV -> Layer0_Device0_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device0_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device0_Stage15_RecvKV -> Layer0_Device0_Stage15_Attention
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage15_Attention [label=Q_local]
	Layer0_Device0_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device0_Stage15_Attention -> Layer0_Device0_Stage15_Accumulate
	Layer0_Device0_Stage14_Accumulate -> Layer0_Device0_Stage15_Accumulate
	Layer0_Device0_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device0_Stage15_Accumulate -> Layer0_Device0_ConcatHeads
	Layer0_Device0_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device0_ConcatHeads -> Layer0_Device0_OutputProj
	Layer0_Device0_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device0_OutputProj -> Layer0_Device0_Residual1
	Layer0_Device0_Input -> Layer0_Device0_Residual1
	Layer0_Device0_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device0_Residual1 -> Layer0_Device0_LayerNorm2
	Layer0_Device0_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device0_LayerNorm2 -> Layer0_Device0_GateProj
	Layer0_Device0_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device0_LayerNorm2 -> Layer0_Device0_UpProj
	Layer0_Device0_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device0_GateProj -> Layer0_Device0_Activation
	Layer0_Device0_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device0_Activation -> Layer0_Device0_ElemMul
	Layer0_Device0_UpProj -> Layer0_Device0_ElemMul
	Layer0_Device0_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device0_ElemMul -> Layer0_Device0_DownProj
	Layer0_Device0_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device0_DownProj -> Layer0_Device0_Residual2
	Layer0_Device0_Residual1 -> Layer0_Device0_Residual2
	Layer0_Device0_Output [label="Layer 0 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device0_Residual2 -> Layer0_Device0_Output
	Layer0_Device1_Input [label="Layer 0 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device1_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device1_Input -> Layer0_Device1_LayerNorm1
	Layer0_Device1_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device1_LayerNorm1 -> Layer0_Device1_QKVProj
	Layer0_Device1_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device1_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage0_RecvKV -> Layer0_Device1_Stage0_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage0_Attention [label=Q_local]
	Layer0_Device1_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage0_Attention -> Layer0_Device1_Stage0_Accumulate
	Layer0_Device1_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage0_RecvKV -> Layer0_Device1_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage1_RecvKV -> Layer0_Device1_Stage1_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage1_Attention [label=Q_local]
	Layer0_Device1_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage1_Attention -> Layer0_Device1_Stage1_Accumulate
	Layer0_Device1_Stage0_Accumulate -> Layer0_Device1_Stage1_Accumulate
	Layer0_Device1_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage1_RecvKV -> Layer0_Device1_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage2_RecvKV -> Layer0_Device1_Stage2_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage2_Attention [label=Q_local]
	Layer0_Device1_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage2_Attention -> Layer0_Device1_Stage2_Accumulate
	Layer0_Device1_Stage1_Accumulate -> Layer0_Device1_Stage2_Accumulate
	Layer0_Device1_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage2_RecvKV -> Layer0_Device1_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage3_RecvKV -> Layer0_Device1_Stage3_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage3_Attention [label=Q_local]
	Layer0_Device1_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage3_Attention -> Layer0_Device1_Stage3_Accumulate
	Layer0_Device1_Stage2_Accumulate -> Layer0_Device1_Stage3_Accumulate
	Layer0_Device1_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage3_RecvKV -> Layer0_Device1_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage4_RecvKV -> Layer0_Device1_Stage4_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage4_Attention [label=Q_local]
	Layer0_Device1_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage4_Attention -> Layer0_Device1_Stage4_Accumulate
	Layer0_Device1_Stage3_Accumulate -> Layer0_Device1_Stage4_Accumulate
	Layer0_Device1_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage4_RecvKV -> Layer0_Device1_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage5_RecvKV -> Layer0_Device1_Stage5_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage5_Attention [label=Q_local]
	Layer0_Device1_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage5_Attention -> Layer0_Device1_Stage5_Accumulate
	Layer0_Device1_Stage4_Accumulate -> Layer0_Device1_Stage5_Accumulate
	Layer0_Device1_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage5_RecvKV -> Layer0_Device1_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage6_RecvKV -> Layer0_Device1_Stage6_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage6_Attention [label=Q_local]
	Layer0_Device1_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage6_Attention -> Layer0_Device1_Stage6_Accumulate
	Layer0_Device1_Stage5_Accumulate -> Layer0_Device1_Stage6_Accumulate
	Layer0_Device1_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage6_RecvKV -> Layer0_Device1_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage7_RecvKV -> Layer0_Device1_Stage7_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage7_Attention [label=Q_local]
	Layer0_Device1_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage7_Attention -> Layer0_Device1_Stage7_Accumulate
	Layer0_Device1_Stage6_Accumulate -> Layer0_Device1_Stage7_Accumulate
	Layer0_Device1_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage7_RecvKV -> Layer0_Device1_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage8_RecvKV -> Layer0_Device1_Stage8_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage8_Attention [label=Q_local]
	Layer0_Device1_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage8_Attention -> Layer0_Device1_Stage8_Accumulate
	Layer0_Device1_Stage7_Accumulate -> Layer0_Device1_Stage8_Accumulate
	Layer0_Device1_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage8_RecvKV -> Layer0_Device1_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage9_RecvKV -> Layer0_Device1_Stage9_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage9_Attention [label=Q_local]
	Layer0_Device1_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage9_Attention -> Layer0_Device1_Stage9_Accumulate
	Layer0_Device1_Stage8_Accumulate -> Layer0_Device1_Stage9_Accumulate
	Layer0_Device1_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage9_RecvKV -> Layer0_Device1_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage10_RecvKV -> Layer0_Device1_Stage10_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage10_Attention [label=Q_local]
	Layer0_Device1_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage10_Attention -> Layer0_Device1_Stage10_Accumulate
	Layer0_Device1_Stage9_Accumulate -> Layer0_Device1_Stage10_Accumulate
	Layer0_Device1_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage10_RecvKV -> Layer0_Device1_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage11_RecvKV -> Layer0_Device1_Stage11_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage11_Attention [label=Q_local]
	Layer0_Device1_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage11_Attention -> Layer0_Device1_Stage11_Accumulate
	Layer0_Device1_Stage10_Accumulate -> Layer0_Device1_Stage11_Accumulate
	Layer0_Device1_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage11_RecvKV -> Layer0_Device1_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage12_RecvKV -> Layer0_Device1_Stage12_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage12_Attention [label=Q_local]
	Layer0_Device1_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage12_Attention -> Layer0_Device1_Stage12_Accumulate
	Layer0_Device1_Stage11_Accumulate -> Layer0_Device1_Stage12_Accumulate
	Layer0_Device1_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage12_RecvKV -> Layer0_Device1_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage13_RecvKV -> Layer0_Device1_Stage13_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage13_Attention [label=Q_local]
	Layer0_Device1_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage13_Attention -> Layer0_Device1_Stage13_Accumulate
	Layer0_Device1_Stage12_Accumulate -> Layer0_Device1_Stage13_Accumulate
	Layer0_Device1_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage13_RecvKV -> Layer0_Device1_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage14_RecvKV -> Layer0_Device1_Stage14_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage14_Attention [label=Q_local]
	Layer0_Device1_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage14_Attention -> Layer0_Device1_Stage14_Accumulate
	Layer0_Device1_Stage13_Accumulate -> Layer0_Device1_Stage14_Accumulate
	Layer0_Device1_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device1_Stage14_RecvKV -> Layer0_Device1_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device1_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device1_Stage15_RecvKV -> Layer0_Device1_Stage15_Attention
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage15_Attention [label=Q_local]
	Layer0_Device1_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device1_Stage15_Attention -> Layer0_Device1_Stage15_Accumulate
	Layer0_Device1_Stage14_Accumulate -> Layer0_Device1_Stage15_Accumulate
	Layer0_Device1_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device1_Stage15_Accumulate -> Layer0_Device1_ConcatHeads
	Layer0_Device1_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device1_ConcatHeads -> Layer0_Device1_OutputProj
	Layer0_Device1_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device1_OutputProj -> Layer0_Device1_Residual1
	Layer0_Device1_Input -> Layer0_Device1_Residual1
	Layer0_Device1_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device1_Residual1 -> Layer0_Device1_LayerNorm2
	Layer0_Device1_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device1_LayerNorm2 -> Layer0_Device1_GateProj
	Layer0_Device1_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device1_LayerNorm2 -> Layer0_Device1_UpProj
	Layer0_Device1_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device1_GateProj -> Layer0_Device1_Activation
	Layer0_Device1_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device1_Activation -> Layer0_Device1_ElemMul
	Layer0_Device1_UpProj -> Layer0_Device1_ElemMul
	Layer0_Device1_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device1_ElemMul -> Layer0_Device1_DownProj
	Layer0_Device1_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device1_DownProj -> Layer0_Device1_Residual2
	Layer0_Device1_Residual1 -> Layer0_Device1_Residual2
	Layer0_Device1_Output [label="Layer 0 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device1_Residual2 -> Layer0_Device1_Output
	Layer0_Device2_Input [label="Layer 0 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device2_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device2_Input -> Layer0_Device2_LayerNorm1
	Layer0_Device2_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device2_LayerNorm1 -> Layer0_Device2_QKVProj
	Layer0_Device2_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device2_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage0_RecvKV -> Layer0_Device2_Stage0_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage0_Attention [label=Q_local]
	Layer0_Device2_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage0_Attention -> Layer0_Device2_Stage0_Accumulate
	Layer0_Device2_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage0_RecvKV -> Layer0_Device2_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage1_RecvKV -> Layer0_Device2_Stage1_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage1_Attention [label=Q_local]
	Layer0_Device2_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage1_Attention -> Layer0_Device2_Stage1_Accumulate
	Layer0_Device2_Stage0_Accumulate -> Layer0_Device2_Stage1_Accumulate
	Layer0_Device2_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage1_RecvKV -> Layer0_Device2_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage2_RecvKV -> Layer0_Device2_Stage2_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage2_Attention [label=Q_local]
	Layer0_Device2_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage2_Attention -> Layer0_Device2_Stage2_Accumulate
	Layer0_Device2_Stage1_Accumulate -> Layer0_Device2_Stage2_Accumulate
	Layer0_Device2_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage2_RecvKV -> Layer0_Device2_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage3_RecvKV -> Layer0_Device2_Stage3_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage3_Attention [label=Q_local]
	Layer0_Device2_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage3_Attention -> Layer0_Device2_Stage3_Accumulate
	Layer0_Device2_Stage2_Accumulate -> Layer0_Device2_Stage3_Accumulate
	Layer0_Device2_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage3_RecvKV -> Layer0_Device2_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage4_RecvKV -> Layer0_Device2_Stage4_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage4_Attention [label=Q_local]
	Layer0_Device2_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage4_Attention -> Layer0_Device2_Stage4_Accumulate
	Layer0_Device2_Stage3_Accumulate -> Layer0_Device2_Stage4_Accumulate
	Layer0_Device2_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage4_RecvKV -> Layer0_Device2_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage5_RecvKV -> Layer0_Device2_Stage5_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage5_Attention [label=Q_local]
	Layer0_Device2_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage5_Attention -> Layer0_Device2_Stage5_Accumulate
	Layer0_Device2_Stage4_Accumulate -> Layer0_Device2_Stage5_Accumulate
	Layer0_Device2_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage5_RecvKV -> Layer0_Device2_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage6_RecvKV -> Layer0_Device2_Stage6_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage6_Attention [label=Q_local]
	Layer0_Device2_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage6_Attention -> Layer0_Device2_Stage6_Accumulate
	Layer0_Device2_Stage5_Accumulate -> Layer0_Device2_Stage6_Accumulate
	Layer0_Device2_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage6_RecvKV -> Layer0_Device2_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage7_RecvKV -> Layer0_Device2_Stage7_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage7_Attention [label=Q_local]
	Layer0_Device2_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage7_Attention -> Layer0_Device2_Stage7_Accumulate
	Layer0_Device2_Stage6_Accumulate -> Layer0_Device2_Stage7_Accumulate
	Layer0_Device2_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage7_RecvKV -> Layer0_Device2_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage8_RecvKV -> Layer0_Device2_Stage8_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage8_Attention [label=Q_local]
	Layer0_Device2_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage8_Attention -> Layer0_Device2_Stage8_Accumulate
	Layer0_Device2_Stage7_Accumulate -> Layer0_Device2_Stage8_Accumulate
	Layer0_Device2_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage8_RecvKV -> Layer0_Device2_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage9_RecvKV -> Layer0_Device2_Stage9_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage9_Attention [label=Q_local]
	Layer0_Device2_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage9_Attention -> Layer0_Device2_Stage9_Accumulate
	Layer0_Device2_Stage8_Accumulate -> Layer0_Device2_Stage9_Accumulate
	Layer0_Device2_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage9_RecvKV -> Layer0_Device2_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage10_RecvKV -> Layer0_Device2_Stage10_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage10_Attention [label=Q_local]
	Layer0_Device2_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage10_Attention -> Layer0_Device2_Stage10_Accumulate
	Layer0_Device2_Stage9_Accumulate -> Layer0_Device2_Stage10_Accumulate
	Layer0_Device2_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage10_RecvKV -> Layer0_Device2_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage11_RecvKV -> Layer0_Device2_Stage11_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage11_Attention [label=Q_local]
	Layer0_Device2_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage11_Attention -> Layer0_Device2_Stage11_Accumulate
	Layer0_Device2_Stage10_Accumulate -> Layer0_Device2_Stage11_Accumulate
	Layer0_Device2_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage11_RecvKV -> Layer0_Device2_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage12_RecvKV -> Layer0_Device2_Stage12_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage12_Attention [label=Q_local]
	Layer0_Device2_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage12_Attention -> Layer0_Device2_Stage12_Accumulate
	Layer0_Device2_Stage11_Accumulate -> Layer0_Device2_Stage12_Accumulate
	Layer0_Device2_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage12_RecvKV -> Layer0_Device2_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage13_RecvKV -> Layer0_Device2_Stage13_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage13_Attention [label=Q_local]
	Layer0_Device2_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage13_Attention -> Layer0_Device2_Stage13_Accumulate
	Layer0_Device2_Stage12_Accumulate -> Layer0_Device2_Stage13_Accumulate
	Layer0_Device2_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage13_RecvKV -> Layer0_Device2_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage14_RecvKV -> Layer0_Device2_Stage14_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage14_Attention [label=Q_local]
	Layer0_Device2_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage14_Attention -> Layer0_Device2_Stage14_Accumulate
	Layer0_Device2_Stage13_Accumulate -> Layer0_Device2_Stage14_Accumulate
	Layer0_Device2_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device2_Stage14_RecvKV -> Layer0_Device2_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device2_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device2_Stage15_RecvKV -> Layer0_Device2_Stage15_Attention
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage15_Attention [label=Q_local]
	Layer0_Device2_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device2_Stage15_Attention -> Layer0_Device2_Stage15_Accumulate
	Layer0_Device2_Stage14_Accumulate -> Layer0_Device2_Stage15_Accumulate
	Layer0_Device2_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device2_Stage15_Accumulate -> Layer0_Device2_ConcatHeads
	Layer0_Device2_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device2_ConcatHeads -> Layer0_Device2_OutputProj
	Layer0_Device2_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device2_OutputProj -> Layer0_Device2_Residual1
	Layer0_Device2_Input -> Layer0_Device2_Residual1
	Layer0_Device2_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device2_Residual1 -> Layer0_Device2_LayerNorm2
	Layer0_Device2_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device2_LayerNorm2 -> Layer0_Device2_GateProj
	Layer0_Device2_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device2_LayerNorm2 -> Layer0_Device2_UpProj
	Layer0_Device2_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device2_GateProj -> Layer0_Device2_Activation
	Layer0_Device2_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device2_Activation -> Layer0_Device2_ElemMul
	Layer0_Device2_UpProj -> Layer0_Device2_ElemMul
	Layer0_Device2_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device2_ElemMul -> Layer0_Device2_DownProj
	Layer0_Device2_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device2_DownProj -> Layer0_Device2_Residual2
	Layer0_Device2_Residual1 -> Layer0_Device2_Residual2
	Layer0_Device2_Output [label="Layer 0 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device2_Residual2 -> Layer0_Device2_Output
	Layer0_Device3_Input [label="Layer 0 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device3_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device3_Input -> Layer0_Device3_LayerNorm1
	Layer0_Device3_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device3_LayerNorm1 -> Layer0_Device3_QKVProj
	Layer0_Device3_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device3_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage0_RecvKV -> Layer0_Device3_Stage0_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage0_Attention [label=Q_local]
	Layer0_Device3_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage0_Attention -> Layer0_Device3_Stage0_Accumulate
	Layer0_Device3_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage0_RecvKV -> Layer0_Device3_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage1_RecvKV -> Layer0_Device3_Stage1_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage1_Attention [label=Q_local]
	Layer0_Device3_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage1_Attention -> Layer0_Device3_Stage1_Accumulate
	Layer0_Device3_Stage0_Accumulate -> Layer0_Device3_Stage1_Accumulate
	Layer0_Device3_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage1_RecvKV -> Layer0_Device3_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage2_RecvKV -> Layer0_Device3_Stage2_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage2_Attention [label=Q_local]
	Layer0_Device3_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage2_Attention -> Layer0_Device3_Stage2_Accumulate
	Layer0_Device3_Stage1_Accumulate -> Layer0_Device3_Stage2_Accumulate
	Layer0_Device3_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage2_RecvKV -> Layer0_Device3_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage3_RecvKV -> Layer0_Device3_Stage3_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage3_Attention [label=Q_local]
	Layer0_Device3_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage3_Attention -> Layer0_Device3_Stage3_Accumulate
	Layer0_Device3_Stage2_Accumulate -> Layer0_Device3_Stage3_Accumulate
	Layer0_Device3_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage3_RecvKV -> Layer0_Device3_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage4_RecvKV -> Layer0_Device3_Stage4_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage4_Attention [label=Q_local]
	Layer0_Device3_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage4_Attention -> Layer0_Device3_Stage4_Accumulate
	Layer0_Device3_Stage3_Accumulate -> Layer0_Device3_Stage4_Accumulate
	Layer0_Device3_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage4_RecvKV -> Layer0_Device3_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage5_RecvKV -> Layer0_Device3_Stage5_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage5_Attention [label=Q_local]
	Layer0_Device3_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage5_Attention -> Layer0_Device3_Stage5_Accumulate
	Layer0_Device3_Stage4_Accumulate -> Layer0_Device3_Stage5_Accumulate
	Layer0_Device3_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage5_RecvKV -> Layer0_Device3_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage6_RecvKV -> Layer0_Device3_Stage6_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage6_Attention [label=Q_local]
	Layer0_Device3_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage6_Attention -> Layer0_Device3_Stage6_Accumulate
	Layer0_Device3_Stage5_Accumulate -> Layer0_Device3_Stage6_Accumulate
	Layer0_Device3_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage6_RecvKV -> Layer0_Device3_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage7_RecvKV -> Layer0_Device3_Stage7_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage7_Attention [label=Q_local]
	Layer0_Device3_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage7_Attention -> Layer0_Device3_Stage7_Accumulate
	Layer0_Device3_Stage6_Accumulate -> Layer0_Device3_Stage7_Accumulate
	Layer0_Device3_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage7_RecvKV -> Layer0_Device3_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage8_RecvKV -> Layer0_Device3_Stage8_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage8_Attention [label=Q_local]
	Layer0_Device3_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage8_Attention -> Layer0_Device3_Stage8_Accumulate
	Layer0_Device3_Stage7_Accumulate -> Layer0_Device3_Stage8_Accumulate
	Layer0_Device3_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage8_RecvKV -> Layer0_Device3_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage9_RecvKV -> Layer0_Device3_Stage9_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage9_Attention [label=Q_local]
	Layer0_Device3_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage9_Attention -> Layer0_Device3_Stage9_Accumulate
	Layer0_Device3_Stage8_Accumulate -> Layer0_Device3_Stage9_Accumulate
	Layer0_Device3_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage9_RecvKV -> Layer0_Device3_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage10_RecvKV -> Layer0_Device3_Stage10_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage10_Attention [label=Q_local]
	Layer0_Device3_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage10_Attention -> Layer0_Device3_Stage10_Accumulate
	Layer0_Device3_Stage9_Accumulate -> Layer0_Device3_Stage10_Accumulate
	Layer0_Device3_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage10_RecvKV -> Layer0_Device3_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage11_RecvKV -> Layer0_Device3_Stage11_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage11_Attention [label=Q_local]
	Layer0_Device3_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage11_Attention -> Layer0_Device3_Stage11_Accumulate
	Layer0_Device3_Stage10_Accumulate -> Layer0_Device3_Stage11_Accumulate
	Layer0_Device3_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage11_RecvKV -> Layer0_Device3_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage12_RecvKV -> Layer0_Device3_Stage12_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage12_Attention [label=Q_local]
	Layer0_Device3_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage12_Attention -> Layer0_Device3_Stage12_Accumulate
	Layer0_Device3_Stage11_Accumulate -> Layer0_Device3_Stage12_Accumulate
	Layer0_Device3_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage12_RecvKV -> Layer0_Device3_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage13_RecvKV -> Layer0_Device3_Stage13_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage13_Attention [label=Q_local]
	Layer0_Device3_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage13_Attention -> Layer0_Device3_Stage13_Accumulate
	Layer0_Device3_Stage12_Accumulate -> Layer0_Device3_Stage13_Accumulate
	Layer0_Device3_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage13_RecvKV -> Layer0_Device3_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage14_RecvKV -> Layer0_Device3_Stage14_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage14_Attention [label=Q_local]
	Layer0_Device3_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage14_Attention -> Layer0_Device3_Stage14_Accumulate
	Layer0_Device3_Stage13_Accumulate -> Layer0_Device3_Stage14_Accumulate
	Layer0_Device3_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device3_Stage14_RecvKV -> Layer0_Device3_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device3_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device3_Stage15_RecvKV -> Layer0_Device3_Stage15_Attention
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage15_Attention [label=Q_local]
	Layer0_Device3_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device3_Stage15_Attention -> Layer0_Device3_Stage15_Accumulate
	Layer0_Device3_Stage14_Accumulate -> Layer0_Device3_Stage15_Accumulate
	Layer0_Device3_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device3_Stage15_Accumulate -> Layer0_Device3_ConcatHeads
	Layer0_Device3_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device3_ConcatHeads -> Layer0_Device3_OutputProj
	Layer0_Device3_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device3_OutputProj -> Layer0_Device3_Residual1
	Layer0_Device3_Input -> Layer0_Device3_Residual1
	Layer0_Device3_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device3_Residual1 -> Layer0_Device3_LayerNorm2
	Layer0_Device3_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device3_LayerNorm2 -> Layer0_Device3_GateProj
	Layer0_Device3_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device3_LayerNorm2 -> Layer0_Device3_UpProj
	Layer0_Device3_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device3_GateProj -> Layer0_Device3_Activation
	Layer0_Device3_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device3_Activation -> Layer0_Device3_ElemMul
	Layer0_Device3_UpProj -> Layer0_Device3_ElemMul
	Layer0_Device3_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device3_ElemMul -> Layer0_Device3_DownProj
	Layer0_Device3_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device3_DownProj -> Layer0_Device3_Residual2
	Layer0_Device3_Residual1 -> Layer0_Device3_Residual2
	Layer0_Device3_Output [label="Layer 0 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device3_Residual2 -> Layer0_Device3_Output
	Layer0_Device4_Input [label="Layer 0 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device4_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device4_Input -> Layer0_Device4_LayerNorm1
	Layer0_Device4_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device4_LayerNorm1 -> Layer0_Device4_QKVProj
	Layer0_Device4_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device4_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage0_RecvKV -> Layer0_Device4_Stage0_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage0_Attention [label=Q_local]
	Layer0_Device4_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage0_Attention -> Layer0_Device4_Stage0_Accumulate
	Layer0_Device4_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage0_RecvKV -> Layer0_Device4_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage1_RecvKV -> Layer0_Device4_Stage1_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage1_Attention [label=Q_local]
	Layer0_Device4_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage1_Attention -> Layer0_Device4_Stage1_Accumulate
	Layer0_Device4_Stage0_Accumulate -> Layer0_Device4_Stage1_Accumulate
	Layer0_Device4_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage1_RecvKV -> Layer0_Device4_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage2_RecvKV -> Layer0_Device4_Stage2_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage2_Attention [label=Q_local]
	Layer0_Device4_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage2_Attention -> Layer0_Device4_Stage2_Accumulate
	Layer0_Device4_Stage1_Accumulate -> Layer0_Device4_Stage2_Accumulate
	Layer0_Device4_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage2_RecvKV -> Layer0_Device4_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage3_RecvKV -> Layer0_Device4_Stage3_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage3_Attention [label=Q_local]
	Layer0_Device4_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage3_Attention -> Layer0_Device4_Stage3_Accumulate
	Layer0_Device4_Stage2_Accumulate -> Layer0_Device4_Stage3_Accumulate
	Layer0_Device4_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage3_RecvKV -> Layer0_Device4_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage4_RecvKV -> Layer0_Device4_Stage4_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage4_Attention [label=Q_local]
	Layer0_Device4_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage4_Attention -> Layer0_Device4_Stage4_Accumulate
	Layer0_Device4_Stage3_Accumulate -> Layer0_Device4_Stage4_Accumulate
	Layer0_Device4_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage4_RecvKV -> Layer0_Device4_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage5_RecvKV -> Layer0_Device4_Stage5_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage5_Attention [label=Q_local]
	Layer0_Device4_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage5_Attention -> Layer0_Device4_Stage5_Accumulate
	Layer0_Device4_Stage4_Accumulate -> Layer0_Device4_Stage5_Accumulate
	Layer0_Device4_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage5_RecvKV -> Layer0_Device4_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage6_RecvKV -> Layer0_Device4_Stage6_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage6_Attention [label=Q_local]
	Layer0_Device4_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage6_Attention -> Layer0_Device4_Stage6_Accumulate
	Layer0_Device4_Stage5_Accumulate -> Layer0_Device4_Stage6_Accumulate
	Layer0_Device4_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage6_RecvKV -> Layer0_Device4_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage7_RecvKV -> Layer0_Device4_Stage7_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage7_Attention [label=Q_local]
	Layer0_Device4_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage7_Attention -> Layer0_Device4_Stage7_Accumulate
	Layer0_Device4_Stage6_Accumulate -> Layer0_Device4_Stage7_Accumulate
	Layer0_Device4_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage7_RecvKV -> Layer0_Device4_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage8_RecvKV -> Layer0_Device4_Stage8_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage8_Attention [label=Q_local]
	Layer0_Device4_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage8_Attention -> Layer0_Device4_Stage8_Accumulate
	Layer0_Device4_Stage7_Accumulate -> Layer0_Device4_Stage8_Accumulate
	Layer0_Device4_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage8_RecvKV -> Layer0_Device4_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage9_RecvKV -> Layer0_Device4_Stage9_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage9_Attention [label=Q_local]
	Layer0_Device4_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage9_Attention -> Layer0_Device4_Stage9_Accumulate
	Layer0_Device4_Stage8_Accumulate -> Layer0_Device4_Stage9_Accumulate
	Layer0_Device4_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage9_RecvKV -> Layer0_Device4_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage10_RecvKV -> Layer0_Device4_Stage10_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage10_Attention [label=Q_local]
	Layer0_Device4_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage10_Attention -> Layer0_Device4_Stage10_Accumulate
	Layer0_Device4_Stage9_Accumulate -> Layer0_Device4_Stage10_Accumulate
	Layer0_Device4_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage10_RecvKV -> Layer0_Device4_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage11_RecvKV -> Layer0_Device4_Stage11_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage11_Attention [label=Q_local]
	Layer0_Device4_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage11_Attention -> Layer0_Device4_Stage11_Accumulate
	Layer0_Device4_Stage10_Accumulate -> Layer0_Device4_Stage11_Accumulate
	Layer0_Device4_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage11_RecvKV -> Layer0_Device4_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage12_RecvKV -> Layer0_Device4_Stage12_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage12_Attention [label=Q_local]
	Layer0_Device4_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage12_Attention -> Layer0_Device4_Stage12_Accumulate
	Layer0_Device4_Stage11_Accumulate -> Layer0_Device4_Stage12_Accumulate
	Layer0_Device4_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage12_RecvKV -> Layer0_Device4_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage13_RecvKV -> Layer0_Device4_Stage13_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage13_Attention [label=Q_local]
	Layer0_Device4_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage13_Attention -> Layer0_Device4_Stage13_Accumulate
	Layer0_Device4_Stage12_Accumulate -> Layer0_Device4_Stage13_Accumulate
	Layer0_Device4_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage13_RecvKV -> Layer0_Device4_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage14_RecvKV -> Layer0_Device4_Stage14_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage14_Attention [label=Q_local]
	Layer0_Device4_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage14_Attention -> Layer0_Device4_Stage14_Accumulate
	Layer0_Device4_Stage13_Accumulate -> Layer0_Device4_Stage14_Accumulate
	Layer0_Device4_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device4_Stage14_RecvKV -> Layer0_Device4_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device4_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device4_Stage15_RecvKV -> Layer0_Device4_Stage15_Attention
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage15_Attention [label=Q_local]
	Layer0_Device4_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device4_Stage15_Attention -> Layer0_Device4_Stage15_Accumulate
	Layer0_Device4_Stage14_Accumulate -> Layer0_Device4_Stage15_Accumulate
	Layer0_Device4_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device4_Stage15_Accumulate -> Layer0_Device4_ConcatHeads
	Layer0_Device4_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device4_ConcatHeads -> Layer0_Device4_OutputProj
	Layer0_Device4_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device4_OutputProj -> Layer0_Device4_Residual1
	Layer0_Device4_Input -> Layer0_Device4_Residual1
	Layer0_Device4_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device4_Residual1 -> Layer0_Device4_LayerNorm2
	Layer0_Device4_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device4_LayerNorm2 -> Layer0_Device4_GateProj
	Layer0_Device4_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device4_LayerNorm2 -> Layer0_Device4_UpProj
	Layer0_Device4_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device4_GateProj -> Layer0_Device4_Activation
	Layer0_Device4_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device4_Activation -> Layer0_Device4_ElemMul
	Layer0_Device4_UpProj -> Layer0_Device4_ElemMul
	Layer0_Device4_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device4_ElemMul -> Layer0_Device4_DownProj
	Layer0_Device4_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device4_DownProj -> Layer0_Device4_Residual2
	Layer0_Device4_Residual1 -> Layer0_Device4_Residual2
	Layer0_Device4_Output [label="Layer 0 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device4_Residual2 -> Layer0_Device4_Output
	Layer0_Device5_Input [label="Layer 0 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device5_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device5_Input -> Layer0_Device5_LayerNorm1
	Layer0_Device5_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device5_LayerNorm1 -> Layer0_Device5_QKVProj
	Layer0_Device5_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device5_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage0_RecvKV -> Layer0_Device5_Stage0_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage0_Attention [label=Q_local]
	Layer0_Device5_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage0_Attention -> Layer0_Device5_Stage0_Accumulate
	Layer0_Device5_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage0_RecvKV -> Layer0_Device5_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage1_RecvKV -> Layer0_Device5_Stage1_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage1_Attention [label=Q_local]
	Layer0_Device5_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage1_Attention -> Layer0_Device5_Stage1_Accumulate
	Layer0_Device5_Stage0_Accumulate -> Layer0_Device5_Stage1_Accumulate
	Layer0_Device5_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage1_RecvKV -> Layer0_Device5_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage2_RecvKV -> Layer0_Device5_Stage2_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage2_Attention [label=Q_local]
	Layer0_Device5_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage2_Attention -> Layer0_Device5_Stage2_Accumulate
	Layer0_Device5_Stage1_Accumulate -> Layer0_Device5_Stage2_Accumulate
	Layer0_Device5_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage2_RecvKV -> Layer0_Device5_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage3_RecvKV -> Layer0_Device5_Stage3_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage3_Attention [label=Q_local]
	Layer0_Device5_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage3_Attention -> Layer0_Device5_Stage3_Accumulate
	Layer0_Device5_Stage2_Accumulate -> Layer0_Device5_Stage3_Accumulate
	Layer0_Device5_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage3_RecvKV -> Layer0_Device5_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage4_RecvKV -> Layer0_Device5_Stage4_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage4_Attention [label=Q_local]
	Layer0_Device5_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage4_Attention -> Layer0_Device5_Stage4_Accumulate
	Layer0_Device5_Stage3_Accumulate -> Layer0_Device5_Stage4_Accumulate
	Layer0_Device5_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage4_RecvKV -> Layer0_Device5_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage5_RecvKV -> Layer0_Device5_Stage5_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage5_Attention [label=Q_local]
	Layer0_Device5_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage5_Attention -> Layer0_Device5_Stage5_Accumulate
	Layer0_Device5_Stage4_Accumulate -> Layer0_Device5_Stage5_Accumulate
	Layer0_Device5_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage5_RecvKV -> Layer0_Device5_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage6_RecvKV -> Layer0_Device5_Stage6_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage6_Attention [label=Q_local]
	Layer0_Device5_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage6_Attention -> Layer0_Device5_Stage6_Accumulate
	Layer0_Device5_Stage5_Accumulate -> Layer0_Device5_Stage6_Accumulate
	Layer0_Device5_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage6_RecvKV -> Layer0_Device5_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage7_RecvKV -> Layer0_Device5_Stage7_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage7_Attention [label=Q_local]
	Layer0_Device5_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage7_Attention -> Layer0_Device5_Stage7_Accumulate
	Layer0_Device5_Stage6_Accumulate -> Layer0_Device5_Stage7_Accumulate
	Layer0_Device5_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage7_RecvKV -> Layer0_Device5_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage8_RecvKV -> Layer0_Device5_Stage8_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage8_Attention [label=Q_local]
	Layer0_Device5_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage8_Attention -> Layer0_Device5_Stage8_Accumulate
	Layer0_Device5_Stage7_Accumulate -> Layer0_Device5_Stage8_Accumulate
	Layer0_Device5_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage8_RecvKV -> Layer0_Device5_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage9_RecvKV -> Layer0_Device5_Stage9_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage9_Attention [label=Q_local]
	Layer0_Device5_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage9_Attention -> Layer0_Device5_Stage9_Accumulate
	Layer0_Device5_Stage8_Accumulate -> Layer0_Device5_Stage9_Accumulate
	Layer0_Device5_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage9_RecvKV -> Layer0_Device5_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage10_RecvKV -> Layer0_Device5_Stage10_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage10_Attention [label=Q_local]
	Layer0_Device5_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage10_Attention -> Layer0_Device5_Stage10_Accumulate
	Layer0_Device5_Stage9_Accumulate -> Layer0_Device5_Stage10_Accumulate
	Layer0_Device5_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage10_RecvKV -> Layer0_Device5_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage11_RecvKV -> Layer0_Device5_Stage11_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage11_Attention [label=Q_local]
	Layer0_Device5_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage11_Attention -> Layer0_Device5_Stage11_Accumulate
	Layer0_Device5_Stage10_Accumulate -> Layer0_Device5_Stage11_Accumulate
	Layer0_Device5_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage11_RecvKV -> Layer0_Device5_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage12_RecvKV -> Layer0_Device5_Stage12_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage12_Attention [label=Q_local]
	Layer0_Device5_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage12_Attention -> Layer0_Device5_Stage12_Accumulate
	Layer0_Device5_Stage11_Accumulate -> Layer0_Device5_Stage12_Accumulate
	Layer0_Device5_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage12_RecvKV -> Layer0_Device5_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage13_RecvKV -> Layer0_Device5_Stage13_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage13_Attention [label=Q_local]
	Layer0_Device5_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage13_Attention -> Layer0_Device5_Stage13_Accumulate
	Layer0_Device5_Stage12_Accumulate -> Layer0_Device5_Stage13_Accumulate
	Layer0_Device5_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage13_RecvKV -> Layer0_Device5_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage14_RecvKV -> Layer0_Device5_Stage14_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage14_Attention [label=Q_local]
	Layer0_Device5_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage14_Attention -> Layer0_Device5_Stage14_Accumulate
	Layer0_Device5_Stage13_Accumulate -> Layer0_Device5_Stage14_Accumulate
	Layer0_Device5_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device5_Stage14_RecvKV -> Layer0_Device5_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device5_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device5_Stage15_RecvKV -> Layer0_Device5_Stage15_Attention
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage15_Attention [label=Q_local]
	Layer0_Device5_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device5_Stage15_Attention -> Layer0_Device5_Stage15_Accumulate
	Layer0_Device5_Stage14_Accumulate -> Layer0_Device5_Stage15_Accumulate
	Layer0_Device5_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device5_Stage15_Accumulate -> Layer0_Device5_ConcatHeads
	Layer0_Device5_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device5_ConcatHeads -> Layer0_Device5_OutputProj
	Layer0_Device5_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device5_OutputProj -> Layer0_Device5_Residual1
	Layer0_Device5_Input -> Layer0_Device5_Residual1
	Layer0_Device5_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device5_Residual1 -> Layer0_Device5_LayerNorm2
	Layer0_Device5_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device5_LayerNorm2 -> Layer0_Device5_GateProj
	Layer0_Device5_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device5_LayerNorm2 -> Layer0_Device5_UpProj
	Layer0_Device5_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device5_GateProj -> Layer0_Device5_Activation
	Layer0_Device5_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device5_Activation -> Layer0_Device5_ElemMul
	Layer0_Device5_UpProj -> Layer0_Device5_ElemMul
	Layer0_Device5_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device5_ElemMul -> Layer0_Device5_DownProj
	Layer0_Device5_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device5_DownProj -> Layer0_Device5_Residual2
	Layer0_Device5_Residual1 -> Layer0_Device5_Residual2
	Layer0_Device5_Output [label="Layer 0 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device5_Residual2 -> Layer0_Device5_Output
	Layer0_Device6_Input [label="Layer 0 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device6_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device6_Input -> Layer0_Device6_LayerNorm1
	Layer0_Device6_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device6_LayerNorm1 -> Layer0_Device6_QKVProj
	Layer0_Device6_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device6_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage0_RecvKV -> Layer0_Device6_Stage0_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage0_Attention [label=Q_local]
	Layer0_Device6_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage0_Attention -> Layer0_Device6_Stage0_Accumulate
	Layer0_Device6_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage0_RecvKV -> Layer0_Device6_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage1_RecvKV -> Layer0_Device6_Stage1_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage1_Attention [label=Q_local]
	Layer0_Device6_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage1_Attention -> Layer0_Device6_Stage1_Accumulate
	Layer0_Device6_Stage0_Accumulate -> Layer0_Device6_Stage1_Accumulate
	Layer0_Device6_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage1_RecvKV -> Layer0_Device6_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage2_RecvKV -> Layer0_Device6_Stage2_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage2_Attention [label=Q_local]
	Layer0_Device6_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage2_Attention -> Layer0_Device6_Stage2_Accumulate
	Layer0_Device6_Stage1_Accumulate -> Layer0_Device6_Stage2_Accumulate
	Layer0_Device6_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage2_RecvKV -> Layer0_Device6_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage3_RecvKV -> Layer0_Device6_Stage3_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage3_Attention [label=Q_local]
	Layer0_Device6_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage3_Attention -> Layer0_Device6_Stage3_Accumulate
	Layer0_Device6_Stage2_Accumulate -> Layer0_Device6_Stage3_Accumulate
	Layer0_Device6_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage3_RecvKV -> Layer0_Device6_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage4_RecvKV -> Layer0_Device6_Stage4_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage4_Attention [label=Q_local]
	Layer0_Device6_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage4_Attention -> Layer0_Device6_Stage4_Accumulate
	Layer0_Device6_Stage3_Accumulate -> Layer0_Device6_Stage4_Accumulate
	Layer0_Device6_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage4_RecvKV -> Layer0_Device6_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage5_RecvKV -> Layer0_Device6_Stage5_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage5_Attention [label=Q_local]
	Layer0_Device6_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage5_Attention -> Layer0_Device6_Stage5_Accumulate
	Layer0_Device6_Stage4_Accumulate -> Layer0_Device6_Stage5_Accumulate
	Layer0_Device6_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage5_RecvKV -> Layer0_Device6_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage6_RecvKV -> Layer0_Device6_Stage6_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage6_Attention [label=Q_local]
	Layer0_Device6_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage6_Attention -> Layer0_Device6_Stage6_Accumulate
	Layer0_Device6_Stage5_Accumulate -> Layer0_Device6_Stage6_Accumulate
	Layer0_Device6_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage6_RecvKV -> Layer0_Device6_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage7_RecvKV -> Layer0_Device6_Stage7_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage7_Attention [label=Q_local]
	Layer0_Device6_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage7_Attention -> Layer0_Device6_Stage7_Accumulate
	Layer0_Device6_Stage6_Accumulate -> Layer0_Device6_Stage7_Accumulate
	Layer0_Device6_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage7_RecvKV -> Layer0_Device6_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage8_RecvKV -> Layer0_Device6_Stage8_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage8_Attention [label=Q_local]
	Layer0_Device6_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage8_Attention -> Layer0_Device6_Stage8_Accumulate
	Layer0_Device6_Stage7_Accumulate -> Layer0_Device6_Stage8_Accumulate
	Layer0_Device6_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage8_RecvKV -> Layer0_Device6_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage9_RecvKV -> Layer0_Device6_Stage9_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage9_Attention [label=Q_local]
	Layer0_Device6_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage9_Attention -> Layer0_Device6_Stage9_Accumulate
	Layer0_Device6_Stage8_Accumulate -> Layer0_Device6_Stage9_Accumulate
	Layer0_Device6_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage9_RecvKV -> Layer0_Device6_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage10_RecvKV -> Layer0_Device6_Stage10_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage10_Attention [label=Q_local]
	Layer0_Device6_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage10_Attention -> Layer0_Device6_Stage10_Accumulate
	Layer0_Device6_Stage9_Accumulate -> Layer0_Device6_Stage10_Accumulate
	Layer0_Device6_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage10_RecvKV -> Layer0_Device6_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage11_RecvKV -> Layer0_Device6_Stage11_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage11_Attention [label=Q_local]
	Layer0_Device6_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage11_Attention -> Layer0_Device6_Stage11_Accumulate
	Layer0_Device6_Stage10_Accumulate -> Layer0_Device6_Stage11_Accumulate
	Layer0_Device6_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage11_RecvKV -> Layer0_Device6_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage12_RecvKV -> Layer0_Device6_Stage12_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage12_Attention [label=Q_local]
	Layer0_Device6_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage12_Attention -> Layer0_Device6_Stage12_Accumulate
	Layer0_Device6_Stage11_Accumulate -> Layer0_Device6_Stage12_Accumulate
	Layer0_Device6_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage12_RecvKV -> Layer0_Device6_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage13_RecvKV -> Layer0_Device6_Stage13_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage13_Attention [label=Q_local]
	Layer0_Device6_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage13_Attention -> Layer0_Device6_Stage13_Accumulate
	Layer0_Device6_Stage12_Accumulate -> Layer0_Device6_Stage13_Accumulate
	Layer0_Device6_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage13_RecvKV -> Layer0_Device6_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage14_RecvKV -> Layer0_Device6_Stage14_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage14_Attention [label=Q_local]
	Layer0_Device6_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage14_Attention -> Layer0_Device6_Stage14_Accumulate
	Layer0_Device6_Stage13_Accumulate -> Layer0_Device6_Stage14_Accumulate
	Layer0_Device6_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device6_Stage14_RecvKV -> Layer0_Device6_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device6_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device6_Stage15_RecvKV -> Layer0_Device6_Stage15_Attention
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage15_Attention [label=Q_local]
	Layer0_Device6_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device6_Stage15_Attention -> Layer0_Device6_Stage15_Accumulate
	Layer0_Device6_Stage14_Accumulate -> Layer0_Device6_Stage15_Accumulate
	Layer0_Device6_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device6_Stage15_Accumulate -> Layer0_Device6_ConcatHeads
	Layer0_Device6_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device6_ConcatHeads -> Layer0_Device6_OutputProj
	Layer0_Device6_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device6_OutputProj -> Layer0_Device6_Residual1
	Layer0_Device6_Input -> Layer0_Device6_Residual1
	Layer0_Device6_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device6_Residual1 -> Layer0_Device6_LayerNorm2
	Layer0_Device6_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device6_LayerNorm2 -> Layer0_Device6_GateProj
	Layer0_Device6_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device6_LayerNorm2 -> Layer0_Device6_UpProj
	Layer0_Device6_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device6_GateProj -> Layer0_Device6_Activation
	Layer0_Device6_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device6_Activation -> Layer0_Device6_ElemMul
	Layer0_Device6_UpProj -> Layer0_Device6_ElemMul
	Layer0_Device6_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device6_ElemMul -> Layer0_Device6_DownProj
	Layer0_Device6_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device6_DownProj -> Layer0_Device6_Residual2
	Layer0_Device6_Residual1 -> Layer0_Device6_Residual2
	Layer0_Device6_Output [label="Layer 0 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device6_Residual2 -> Layer0_Device6_Output
	Layer0_Device7_Input [label="Layer 0 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device7_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device7_Input -> Layer0_Device7_LayerNorm1
	Layer0_Device7_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device7_LayerNorm1 -> Layer0_Device7_QKVProj
	Layer0_Device7_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device7_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage0_RecvKV -> Layer0_Device7_Stage0_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage0_Attention [label=Q_local]
	Layer0_Device7_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage0_Attention -> Layer0_Device7_Stage0_Accumulate
	Layer0_Device7_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage0_RecvKV -> Layer0_Device7_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage1_RecvKV -> Layer0_Device7_Stage1_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage1_Attention [label=Q_local]
	Layer0_Device7_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage1_Attention -> Layer0_Device7_Stage1_Accumulate
	Layer0_Device7_Stage0_Accumulate -> Layer0_Device7_Stage1_Accumulate
	Layer0_Device7_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage1_RecvKV -> Layer0_Device7_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage2_RecvKV -> Layer0_Device7_Stage2_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage2_Attention [label=Q_local]
	Layer0_Device7_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage2_Attention -> Layer0_Device7_Stage2_Accumulate
	Layer0_Device7_Stage1_Accumulate -> Layer0_Device7_Stage2_Accumulate
	Layer0_Device7_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage2_RecvKV -> Layer0_Device7_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage3_RecvKV -> Layer0_Device7_Stage3_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage3_Attention [label=Q_local]
	Layer0_Device7_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage3_Attention -> Layer0_Device7_Stage3_Accumulate
	Layer0_Device7_Stage2_Accumulate -> Layer0_Device7_Stage3_Accumulate
	Layer0_Device7_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage3_RecvKV -> Layer0_Device7_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage4_RecvKV -> Layer0_Device7_Stage4_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage4_Attention [label=Q_local]
	Layer0_Device7_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage4_Attention -> Layer0_Device7_Stage4_Accumulate
	Layer0_Device7_Stage3_Accumulate -> Layer0_Device7_Stage4_Accumulate
	Layer0_Device7_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage4_RecvKV -> Layer0_Device7_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage5_RecvKV -> Layer0_Device7_Stage5_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage5_Attention [label=Q_local]
	Layer0_Device7_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage5_Attention -> Layer0_Device7_Stage5_Accumulate
	Layer0_Device7_Stage4_Accumulate -> Layer0_Device7_Stage5_Accumulate
	Layer0_Device7_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage5_RecvKV -> Layer0_Device7_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage6_RecvKV -> Layer0_Device7_Stage6_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage6_Attention [label=Q_local]
	Layer0_Device7_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage6_Attention -> Layer0_Device7_Stage6_Accumulate
	Layer0_Device7_Stage5_Accumulate -> Layer0_Device7_Stage6_Accumulate
	Layer0_Device7_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage6_RecvKV -> Layer0_Device7_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage7_RecvKV -> Layer0_Device7_Stage7_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage7_Attention [label=Q_local]
	Layer0_Device7_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage7_Attention -> Layer0_Device7_Stage7_Accumulate
	Layer0_Device7_Stage6_Accumulate -> Layer0_Device7_Stage7_Accumulate
	Layer0_Device7_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage7_RecvKV -> Layer0_Device7_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage8_RecvKV -> Layer0_Device7_Stage8_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage8_Attention [label=Q_local]
	Layer0_Device7_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage8_Attention -> Layer0_Device7_Stage8_Accumulate
	Layer0_Device7_Stage7_Accumulate -> Layer0_Device7_Stage8_Accumulate
	Layer0_Device7_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage8_RecvKV -> Layer0_Device7_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage9_RecvKV -> Layer0_Device7_Stage9_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage9_Attention [label=Q_local]
	Layer0_Device7_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage9_Attention -> Layer0_Device7_Stage9_Accumulate
	Layer0_Device7_Stage8_Accumulate -> Layer0_Device7_Stage9_Accumulate
	Layer0_Device7_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage9_RecvKV -> Layer0_Device7_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage10_RecvKV -> Layer0_Device7_Stage10_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage10_Attention [label=Q_local]
	Layer0_Device7_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage10_Attention -> Layer0_Device7_Stage10_Accumulate
	Layer0_Device7_Stage9_Accumulate -> Layer0_Device7_Stage10_Accumulate
	Layer0_Device7_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage10_RecvKV -> Layer0_Device7_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage11_RecvKV -> Layer0_Device7_Stage11_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage11_Attention [label=Q_local]
	Layer0_Device7_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage11_Attention -> Layer0_Device7_Stage11_Accumulate
	Layer0_Device7_Stage10_Accumulate -> Layer0_Device7_Stage11_Accumulate
	Layer0_Device7_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage11_RecvKV -> Layer0_Device7_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage12_RecvKV -> Layer0_Device7_Stage12_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage12_Attention [label=Q_local]
	Layer0_Device7_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage12_Attention -> Layer0_Device7_Stage12_Accumulate
	Layer0_Device7_Stage11_Accumulate -> Layer0_Device7_Stage12_Accumulate
	Layer0_Device7_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage12_RecvKV -> Layer0_Device7_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage13_RecvKV -> Layer0_Device7_Stage13_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage13_Attention [label=Q_local]
	Layer0_Device7_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage13_Attention -> Layer0_Device7_Stage13_Accumulate
	Layer0_Device7_Stage12_Accumulate -> Layer0_Device7_Stage13_Accumulate
	Layer0_Device7_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage13_RecvKV -> Layer0_Device7_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage14_RecvKV -> Layer0_Device7_Stage14_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage14_Attention [label=Q_local]
	Layer0_Device7_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage14_Attention -> Layer0_Device7_Stage14_Accumulate
	Layer0_Device7_Stage13_Accumulate -> Layer0_Device7_Stage14_Accumulate
	Layer0_Device7_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device7_Stage14_RecvKV -> Layer0_Device7_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device7_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device7_Stage15_RecvKV -> Layer0_Device7_Stage15_Attention
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage15_Attention [label=Q_local]
	Layer0_Device7_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device7_Stage15_Attention -> Layer0_Device7_Stage15_Accumulate
	Layer0_Device7_Stage14_Accumulate -> Layer0_Device7_Stage15_Accumulate
	Layer0_Device7_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device7_Stage15_Accumulate -> Layer0_Device7_ConcatHeads
	Layer0_Device7_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device7_ConcatHeads -> Layer0_Device7_OutputProj
	Layer0_Device7_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device7_OutputProj -> Layer0_Device7_Residual1
	Layer0_Device7_Input -> Layer0_Device7_Residual1
	Layer0_Device7_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device7_Residual1 -> Layer0_Device7_LayerNorm2
	Layer0_Device7_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device7_LayerNorm2 -> Layer0_Device7_GateProj
	Layer0_Device7_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device7_LayerNorm2 -> Layer0_Device7_UpProj
	Layer0_Device7_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device7_GateProj -> Layer0_Device7_Activation
	Layer0_Device7_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device7_Activation -> Layer0_Device7_ElemMul
	Layer0_Device7_UpProj -> Layer0_Device7_ElemMul
	Layer0_Device7_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device7_ElemMul -> Layer0_Device7_DownProj
	Layer0_Device7_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device7_DownProj -> Layer0_Device7_Residual2
	Layer0_Device7_Residual1 -> Layer0_Device7_Residual2
	Layer0_Device7_Output [label="Layer 0 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device7_Residual2 -> Layer0_Device7_Output
	Layer0_Device8_Input [label="Layer 0 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device8_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device8_Input -> Layer0_Device8_LayerNorm1
	Layer0_Device8_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device8_LayerNorm1 -> Layer0_Device8_QKVProj
	Layer0_Device8_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device8_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage0_RecvKV -> Layer0_Device8_Stage0_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage0_Attention [label=Q_local]
	Layer0_Device8_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage0_Attention -> Layer0_Device8_Stage0_Accumulate
	Layer0_Device8_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage0_RecvKV -> Layer0_Device8_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage1_RecvKV -> Layer0_Device8_Stage1_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage1_Attention [label=Q_local]
	Layer0_Device8_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage1_Attention -> Layer0_Device8_Stage1_Accumulate
	Layer0_Device8_Stage0_Accumulate -> Layer0_Device8_Stage1_Accumulate
	Layer0_Device8_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage1_RecvKV -> Layer0_Device8_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage2_RecvKV -> Layer0_Device8_Stage2_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage2_Attention [label=Q_local]
	Layer0_Device8_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage2_Attention -> Layer0_Device8_Stage2_Accumulate
	Layer0_Device8_Stage1_Accumulate -> Layer0_Device8_Stage2_Accumulate
	Layer0_Device8_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage2_RecvKV -> Layer0_Device8_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage3_RecvKV -> Layer0_Device8_Stage3_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage3_Attention [label=Q_local]
	Layer0_Device8_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage3_Attention -> Layer0_Device8_Stage3_Accumulate
	Layer0_Device8_Stage2_Accumulate -> Layer0_Device8_Stage3_Accumulate
	Layer0_Device8_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage3_RecvKV -> Layer0_Device8_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage4_RecvKV -> Layer0_Device8_Stage4_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage4_Attention [label=Q_local]
	Layer0_Device8_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage4_Attention -> Layer0_Device8_Stage4_Accumulate
	Layer0_Device8_Stage3_Accumulate -> Layer0_Device8_Stage4_Accumulate
	Layer0_Device8_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage4_RecvKV -> Layer0_Device8_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage5_RecvKV -> Layer0_Device8_Stage5_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage5_Attention [label=Q_local]
	Layer0_Device8_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage5_Attention -> Layer0_Device8_Stage5_Accumulate
	Layer0_Device8_Stage4_Accumulate -> Layer0_Device8_Stage5_Accumulate
	Layer0_Device8_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage5_RecvKV -> Layer0_Device8_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage6_RecvKV -> Layer0_Device8_Stage6_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage6_Attention [label=Q_local]
	Layer0_Device8_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage6_Attention -> Layer0_Device8_Stage6_Accumulate
	Layer0_Device8_Stage5_Accumulate -> Layer0_Device8_Stage6_Accumulate
	Layer0_Device8_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage6_RecvKV -> Layer0_Device8_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage7_RecvKV -> Layer0_Device8_Stage7_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage7_Attention [label=Q_local]
	Layer0_Device8_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage7_Attention -> Layer0_Device8_Stage7_Accumulate
	Layer0_Device8_Stage6_Accumulate -> Layer0_Device8_Stage7_Accumulate
	Layer0_Device8_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage7_RecvKV -> Layer0_Device8_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage8_RecvKV -> Layer0_Device8_Stage8_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage8_Attention [label=Q_local]
	Layer0_Device8_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage8_Attention -> Layer0_Device8_Stage8_Accumulate
	Layer0_Device8_Stage7_Accumulate -> Layer0_Device8_Stage8_Accumulate
	Layer0_Device8_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage8_RecvKV -> Layer0_Device8_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage9_RecvKV -> Layer0_Device8_Stage9_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage9_Attention [label=Q_local]
	Layer0_Device8_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage9_Attention -> Layer0_Device8_Stage9_Accumulate
	Layer0_Device8_Stage8_Accumulate -> Layer0_Device8_Stage9_Accumulate
	Layer0_Device8_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage9_RecvKV -> Layer0_Device8_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage10_RecvKV -> Layer0_Device8_Stage10_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage10_Attention [label=Q_local]
	Layer0_Device8_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage10_Attention -> Layer0_Device8_Stage10_Accumulate
	Layer0_Device8_Stage9_Accumulate -> Layer0_Device8_Stage10_Accumulate
	Layer0_Device8_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage10_RecvKV -> Layer0_Device8_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage11_RecvKV -> Layer0_Device8_Stage11_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage11_Attention [label=Q_local]
	Layer0_Device8_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage11_Attention -> Layer0_Device8_Stage11_Accumulate
	Layer0_Device8_Stage10_Accumulate -> Layer0_Device8_Stage11_Accumulate
	Layer0_Device8_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage11_RecvKV -> Layer0_Device8_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage12_RecvKV -> Layer0_Device8_Stage12_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage12_Attention [label=Q_local]
	Layer0_Device8_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage12_Attention -> Layer0_Device8_Stage12_Accumulate
	Layer0_Device8_Stage11_Accumulate -> Layer0_Device8_Stage12_Accumulate
	Layer0_Device8_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage12_RecvKV -> Layer0_Device8_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage13_RecvKV -> Layer0_Device8_Stage13_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage13_Attention [label=Q_local]
	Layer0_Device8_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage13_Attention -> Layer0_Device8_Stage13_Accumulate
	Layer0_Device8_Stage12_Accumulate -> Layer0_Device8_Stage13_Accumulate
	Layer0_Device8_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage13_RecvKV -> Layer0_Device8_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage14_RecvKV -> Layer0_Device8_Stage14_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage14_Attention [label=Q_local]
	Layer0_Device8_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage14_Attention -> Layer0_Device8_Stage14_Accumulate
	Layer0_Device8_Stage13_Accumulate -> Layer0_Device8_Stage14_Accumulate
	Layer0_Device8_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device8_Stage14_RecvKV -> Layer0_Device8_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device8_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device8_Stage15_RecvKV -> Layer0_Device8_Stage15_Attention
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage15_Attention [label=Q_local]
	Layer0_Device8_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device8_Stage15_Attention -> Layer0_Device8_Stage15_Accumulate
	Layer0_Device8_Stage14_Accumulate -> Layer0_Device8_Stage15_Accumulate
	Layer0_Device8_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device8_Stage15_Accumulate -> Layer0_Device8_ConcatHeads
	Layer0_Device8_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device8_ConcatHeads -> Layer0_Device8_OutputProj
	Layer0_Device8_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device8_OutputProj -> Layer0_Device8_Residual1
	Layer0_Device8_Input -> Layer0_Device8_Residual1
	Layer0_Device8_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device8_Residual1 -> Layer0_Device8_LayerNorm2
	Layer0_Device8_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device8_LayerNorm2 -> Layer0_Device8_GateProj
	Layer0_Device8_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device8_LayerNorm2 -> Layer0_Device8_UpProj
	Layer0_Device8_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device8_GateProj -> Layer0_Device8_Activation
	Layer0_Device8_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device8_Activation -> Layer0_Device8_ElemMul
	Layer0_Device8_UpProj -> Layer0_Device8_ElemMul
	Layer0_Device8_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device8_ElemMul -> Layer0_Device8_DownProj
	Layer0_Device8_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device8_DownProj -> Layer0_Device8_Residual2
	Layer0_Device8_Residual1 -> Layer0_Device8_Residual2
	Layer0_Device8_Output [label="Layer 0 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device8_Residual2 -> Layer0_Device8_Output
	Layer0_Device9_Input [label="Layer 0 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device9_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device9_Input -> Layer0_Device9_LayerNorm1
	Layer0_Device9_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device9_LayerNorm1 -> Layer0_Device9_QKVProj
	Layer0_Device9_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device9_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage0_RecvKV -> Layer0_Device9_Stage0_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage0_Attention [label=Q_local]
	Layer0_Device9_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage0_Attention -> Layer0_Device9_Stage0_Accumulate
	Layer0_Device9_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage0_RecvKV -> Layer0_Device9_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage1_RecvKV -> Layer0_Device9_Stage1_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage1_Attention [label=Q_local]
	Layer0_Device9_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage1_Attention -> Layer0_Device9_Stage1_Accumulate
	Layer0_Device9_Stage0_Accumulate -> Layer0_Device9_Stage1_Accumulate
	Layer0_Device9_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage1_RecvKV -> Layer0_Device9_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage2_RecvKV -> Layer0_Device9_Stage2_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage2_Attention [label=Q_local]
	Layer0_Device9_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage2_Attention -> Layer0_Device9_Stage2_Accumulate
	Layer0_Device9_Stage1_Accumulate -> Layer0_Device9_Stage2_Accumulate
	Layer0_Device9_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage2_RecvKV -> Layer0_Device9_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage3_RecvKV -> Layer0_Device9_Stage3_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage3_Attention [label=Q_local]
	Layer0_Device9_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage3_Attention -> Layer0_Device9_Stage3_Accumulate
	Layer0_Device9_Stage2_Accumulate -> Layer0_Device9_Stage3_Accumulate
	Layer0_Device9_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage3_RecvKV -> Layer0_Device9_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage4_RecvKV -> Layer0_Device9_Stage4_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage4_Attention [label=Q_local]
	Layer0_Device9_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage4_Attention -> Layer0_Device9_Stage4_Accumulate
	Layer0_Device9_Stage3_Accumulate -> Layer0_Device9_Stage4_Accumulate
	Layer0_Device9_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage4_RecvKV -> Layer0_Device9_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage5_RecvKV -> Layer0_Device9_Stage5_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage5_Attention [label=Q_local]
	Layer0_Device9_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage5_Attention -> Layer0_Device9_Stage5_Accumulate
	Layer0_Device9_Stage4_Accumulate -> Layer0_Device9_Stage5_Accumulate
	Layer0_Device9_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage5_RecvKV -> Layer0_Device9_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage6_RecvKV -> Layer0_Device9_Stage6_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage6_Attention [label=Q_local]
	Layer0_Device9_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage6_Attention -> Layer0_Device9_Stage6_Accumulate
	Layer0_Device9_Stage5_Accumulate -> Layer0_Device9_Stage6_Accumulate
	Layer0_Device9_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage6_RecvKV -> Layer0_Device9_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage7_RecvKV -> Layer0_Device9_Stage7_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage7_Attention [label=Q_local]
	Layer0_Device9_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage7_Attention -> Layer0_Device9_Stage7_Accumulate
	Layer0_Device9_Stage6_Accumulate -> Layer0_Device9_Stage7_Accumulate
	Layer0_Device9_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage7_RecvKV -> Layer0_Device9_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage8_RecvKV -> Layer0_Device9_Stage8_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage8_Attention [label=Q_local]
	Layer0_Device9_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage8_Attention -> Layer0_Device9_Stage8_Accumulate
	Layer0_Device9_Stage7_Accumulate -> Layer0_Device9_Stage8_Accumulate
	Layer0_Device9_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage8_RecvKV -> Layer0_Device9_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage9_RecvKV -> Layer0_Device9_Stage9_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage9_Attention [label=Q_local]
	Layer0_Device9_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage9_Attention -> Layer0_Device9_Stage9_Accumulate
	Layer0_Device9_Stage8_Accumulate -> Layer0_Device9_Stage9_Accumulate
	Layer0_Device9_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage9_RecvKV -> Layer0_Device9_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage10_RecvKV -> Layer0_Device9_Stage10_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage10_Attention [label=Q_local]
	Layer0_Device9_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage10_Attention -> Layer0_Device9_Stage10_Accumulate
	Layer0_Device9_Stage9_Accumulate -> Layer0_Device9_Stage10_Accumulate
	Layer0_Device9_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage10_RecvKV -> Layer0_Device9_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage11_RecvKV -> Layer0_Device9_Stage11_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage11_Attention [label=Q_local]
	Layer0_Device9_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage11_Attention -> Layer0_Device9_Stage11_Accumulate
	Layer0_Device9_Stage10_Accumulate -> Layer0_Device9_Stage11_Accumulate
	Layer0_Device9_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage11_RecvKV -> Layer0_Device9_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage12_RecvKV -> Layer0_Device9_Stage12_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage12_Attention [label=Q_local]
	Layer0_Device9_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage12_Attention -> Layer0_Device9_Stage12_Accumulate
	Layer0_Device9_Stage11_Accumulate -> Layer0_Device9_Stage12_Accumulate
	Layer0_Device9_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage12_RecvKV -> Layer0_Device9_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage13_RecvKV -> Layer0_Device9_Stage13_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage13_Attention [label=Q_local]
	Layer0_Device9_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage13_Attention -> Layer0_Device9_Stage13_Accumulate
	Layer0_Device9_Stage12_Accumulate -> Layer0_Device9_Stage13_Accumulate
	Layer0_Device9_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage13_RecvKV -> Layer0_Device9_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage14_RecvKV -> Layer0_Device9_Stage14_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage14_Attention [label=Q_local]
	Layer0_Device9_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage14_Attention -> Layer0_Device9_Stage14_Accumulate
	Layer0_Device9_Stage13_Accumulate -> Layer0_Device9_Stage14_Accumulate
	Layer0_Device9_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device9_Stage14_RecvKV -> Layer0_Device9_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device9_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device9_Stage15_RecvKV -> Layer0_Device9_Stage15_Attention
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage15_Attention [label=Q_local]
	Layer0_Device9_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device9_Stage15_Attention -> Layer0_Device9_Stage15_Accumulate
	Layer0_Device9_Stage14_Accumulate -> Layer0_Device9_Stage15_Accumulate
	Layer0_Device9_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device9_Stage15_Accumulate -> Layer0_Device9_ConcatHeads
	Layer0_Device9_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device9_ConcatHeads -> Layer0_Device9_OutputProj
	Layer0_Device9_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device9_OutputProj -> Layer0_Device9_Residual1
	Layer0_Device9_Input -> Layer0_Device9_Residual1
	Layer0_Device9_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device9_Residual1 -> Layer0_Device9_LayerNorm2
	Layer0_Device9_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device9_LayerNorm2 -> Layer0_Device9_GateProj
	Layer0_Device9_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device9_LayerNorm2 -> Layer0_Device9_UpProj
	Layer0_Device9_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device9_GateProj -> Layer0_Device9_Activation
	Layer0_Device9_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device9_Activation -> Layer0_Device9_ElemMul
	Layer0_Device9_UpProj -> Layer0_Device9_ElemMul
	Layer0_Device9_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device9_ElemMul -> Layer0_Device9_DownProj
	Layer0_Device9_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device9_DownProj -> Layer0_Device9_Residual2
	Layer0_Device9_Residual1 -> Layer0_Device9_Residual2
	Layer0_Device9_Output [label="Layer 0 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device9_Residual2 -> Layer0_Device9_Output
	Layer0_Device10_Input [label="Layer 0 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device10_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device10_Input -> Layer0_Device10_LayerNorm1
	Layer0_Device10_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device10_LayerNorm1 -> Layer0_Device10_QKVProj
	Layer0_Device10_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device10_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage0_RecvKV -> Layer0_Device10_Stage0_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage0_Attention [label=Q_local]
	Layer0_Device10_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage0_Attention -> Layer0_Device10_Stage0_Accumulate
	Layer0_Device10_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage0_RecvKV -> Layer0_Device10_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage1_RecvKV -> Layer0_Device10_Stage1_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage1_Attention [label=Q_local]
	Layer0_Device10_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage1_Attention -> Layer0_Device10_Stage1_Accumulate
	Layer0_Device10_Stage0_Accumulate -> Layer0_Device10_Stage1_Accumulate
	Layer0_Device10_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage1_RecvKV -> Layer0_Device10_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage2_RecvKV -> Layer0_Device10_Stage2_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage2_Attention [label=Q_local]
	Layer0_Device10_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage2_Attention -> Layer0_Device10_Stage2_Accumulate
	Layer0_Device10_Stage1_Accumulate -> Layer0_Device10_Stage2_Accumulate
	Layer0_Device10_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage2_RecvKV -> Layer0_Device10_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage3_RecvKV -> Layer0_Device10_Stage3_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage3_Attention [label=Q_local]
	Layer0_Device10_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage3_Attention -> Layer0_Device10_Stage3_Accumulate
	Layer0_Device10_Stage2_Accumulate -> Layer0_Device10_Stage3_Accumulate
	Layer0_Device10_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage3_RecvKV -> Layer0_Device10_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage4_RecvKV -> Layer0_Device10_Stage4_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage4_Attention [label=Q_local]
	Layer0_Device10_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage4_Attention -> Layer0_Device10_Stage4_Accumulate
	Layer0_Device10_Stage3_Accumulate -> Layer0_Device10_Stage4_Accumulate
	Layer0_Device10_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage4_RecvKV -> Layer0_Device10_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage5_RecvKV -> Layer0_Device10_Stage5_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage5_Attention [label=Q_local]
	Layer0_Device10_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage5_Attention -> Layer0_Device10_Stage5_Accumulate
	Layer0_Device10_Stage4_Accumulate -> Layer0_Device10_Stage5_Accumulate
	Layer0_Device10_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage5_RecvKV -> Layer0_Device10_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage6_RecvKV -> Layer0_Device10_Stage6_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage6_Attention [label=Q_local]
	Layer0_Device10_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage6_Attention -> Layer0_Device10_Stage6_Accumulate
	Layer0_Device10_Stage5_Accumulate -> Layer0_Device10_Stage6_Accumulate
	Layer0_Device10_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage6_RecvKV -> Layer0_Device10_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage7_RecvKV -> Layer0_Device10_Stage7_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage7_Attention [label=Q_local]
	Layer0_Device10_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage7_Attention -> Layer0_Device10_Stage7_Accumulate
	Layer0_Device10_Stage6_Accumulate -> Layer0_Device10_Stage7_Accumulate
	Layer0_Device10_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage7_RecvKV -> Layer0_Device10_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage8_RecvKV -> Layer0_Device10_Stage8_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage8_Attention [label=Q_local]
	Layer0_Device10_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage8_Attention -> Layer0_Device10_Stage8_Accumulate
	Layer0_Device10_Stage7_Accumulate -> Layer0_Device10_Stage8_Accumulate
	Layer0_Device10_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage8_RecvKV -> Layer0_Device10_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage9_RecvKV -> Layer0_Device10_Stage9_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage9_Attention [label=Q_local]
	Layer0_Device10_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage9_Attention -> Layer0_Device10_Stage9_Accumulate
	Layer0_Device10_Stage8_Accumulate -> Layer0_Device10_Stage9_Accumulate
	Layer0_Device10_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage9_RecvKV -> Layer0_Device10_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage10_RecvKV -> Layer0_Device10_Stage10_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage10_Attention [label=Q_local]
	Layer0_Device10_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage10_Attention -> Layer0_Device10_Stage10_Accumulate
	Layer0_Device10_Stage9_Accumulate -> Layer0_Device10_Stage10_Accumulate
	Layer0_Device10_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage10_RecvKV -> Layer0_Device10_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage11_RecvKV -> Layer0_Device10_Stage11_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage11_Attention [label=Q_local]
	Layer0_Device10_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage11_Attention -> Layer0_Device10_Stage11_Accumulate
	Layer0_Device10_Stage10_Accumulate -> Layer0_Device10_Stage11_Accumulate
	Layer0_Device10_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage11_RecvKV -> Layer0_Device10_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage12_RecvKV -> Layer0_Device10_Stage12_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage12_Attention [label=Q_local]
	Layer0_Device10_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage12_Attention -> Layer0_Device10_Stage12_Accumulate
	Layer0_Device10_Stage11_Accumulate -> Layer0_Device10_Stage12_Accumulate
	Layer0_Device10_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage12_RecvKV -> Layer0_Device10_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage13_RecvKV -> Layer0_Device10_Stage13_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage13_Attention [label=Q_local]
	Layer0_Device10_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage13_Attention -> Layer0_Device10_Stage13_Accumulate
	Layer0_Device10_Stage12_Accumulate -> Layer0_Device10_Stage13_Accumulate
	Layer0_Device10_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage13_RecvKV -> Layer0_Device10_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage14_RecvKV -> Layer0_Device10_Stage14_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage14_Attention [label=Q_local]
	Layer0_Device10_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage14_Attention -> Layer0_Device10_Stage14_Accumulate
	Layer0_Device10_Stage13_Accumulate -> Layer0_Device10_Stage14_Accumulate
	Layer0_Device10_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device10_Stage14_RecvKV -> Layer0_Device10_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device10_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device10_Stage15_RecvKV -> Layer0_Device10_Stage15_Attention
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage15_Attention [label=Q_local]
	Layer0_Device10_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device10_Stage15_Attention -> Layer0_Device10_Stage15_Accumulate
	Layer0_Device10_Stage14_Accumulate -> Layer0_Device10_Stage15_Accumulate
	Layer0_Device10_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device10_Stage15_Accumulate -> Layer0_Device10_ConcatHeads
	Layer0_Device10_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device10_ConcatHeads -> Layer0_Device10_OutputProj
	Layer0_Device10_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device10_OutputProj -> Layer0_Device10_Residual1
	Layer0_Device10_Input -> Layer0_Device10_Residual1
	Layer0_Device10_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device10_Residual1 -> Layer0_Device10_LayerNorm2
	Layer0_Device10_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device10_LayerNorm2 -> Layer0_Device10_GateProj
	Layer0_Device10_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device10_LayerNorm2 -> Layer0_Device10_UpProj
	Layer0_Device10_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device10_GateProj -> Layer0_Device10_Activation
	Layer0_Device10_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device10_Activation -> Layer0_Device10_ElemMul
	Layer0_Device10_UpProj -> Layer0_Device10_ElemMul
	Layer0_Device10_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device10_ElemMul -> Layer0_Device10_DownProj
	Layer0_Device10_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device10_DownProj -> Layer0_Device10_Residual2
	Layer0_Device10_Residual1 -> Layer0_Device10_Residual2
	Layer0_Device10_Output [label="Layer 0 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device10_Residual2 -> Layer0_Device10_Output
	Layer0_Device11_Input [label="Layer 0 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device11_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device11_Input -> Layer0_Device11_LayerNorm1
	Layer0_Device11_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device11_LayerNorm1 -> Layer0_Device11_QKVProj
	Layer0_Device11_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device11_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage0_RecvKV -> Layer0_Device11_Stage0_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage0_Attention [label=Q_local]
	Layer0_Device11_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage0_Attention -> Layer0_Device11_Stage0_Accumulate
	Layer0_Device11_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage0_RecvKV -> Layer0_Device11_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage1_RecvKV -> Layer0_Device11_Stage1_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage1_Attention [label=Q_local]
	Layer0_Device11_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage1_Attention -> Layer0_Device11_Stage1_Accumulate
	Layer0_Device11_Stage0_Accumulate -> Layer0_Device11_Stage1_Accumulate
	Layer0_Device11_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage1_RecvKV -> Layer0_Device11_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage2_RecvKV -> Layer0_Device11_Stage2_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage2_Attention [label=Q_local]
	Layer0_Device11_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage2_Attention -> Layer0_Device11_Stage2_Accumulate
	Layer0_Device11_Stage1_Accumulate -> Layer0_Device11_Stage2_Accumulate
	Layer0_Device11_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage2_RecvKV -> Layer0_Device11_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage3_RecvKV -> Layer0_Device11_Stage3_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage3_Attention [label=Q_local]
	Layer0_Device11_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage3_Attention -> Layer0_Device11_Stage3_Accumulate
	Layer0_Device11_Stage2_Accumulate -> Layer0_Device11_Stage3_Accumulate
	Layer0_Device11_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage3_RecvKV -> Layer0_Device11_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage4_RecvKV -> Layer0_Device11_Stage4_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage4_Attention [label=Q_local]
	Layer0_Device11_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage4_Attention -> Layer0_Device11_Stage4_Accumulate
	Layer0_Device11_Stage3_Accumulate -> Layer0_Device11_Stage4_Accumulate
	Layer0_Device11_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage4_RecvKV -> Layer0_Device11_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage5_RecvKV -> Layer0_Device11_Stage5_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage5_Attention [label=Q_local]
	Layer0_Device11_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage5_Attention -> Layer0_Device11_Stage5_Accumulate
	Layer0_Device11_Stage4_Accumulate -> Layer0_Device11_Stage5_Accumulate
	Layer0_Device11_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage5_RecvKV -> Layer0_Device11_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage6_RecvKV -> Layer0_Device11_Stage6_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage6_Attention [label=Q_local]
	Layer0_Device11_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage6_Attention -> Layer0_Device11_Stage6_Accumulate
	Layer0_Device11_Stage5_Accumulate -> Layer0_Device11_Stage6_Accumulate
	Layer0_Device11_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage6_RecvKV -> Layer0_Device11_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage7_RecvKV -> Layer0_Device11_Stage7_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage7_Attention [label=Q_local]
	Layer0_Device11_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage7_Attention -> Layer0_Device11_Stage7_Accumulate
	Layer0_Device11_Stage6_Accumulate -> Layer0_Device11_Stage7_Accumulate
	Layer0_Device11_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage7_RecvKV -> Layer0_Device11_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage8_RecvKV -> Layer0_Device11_Stage8_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage8_Attention [label=Q_local]
	Layer0_Device11_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage8_Attention -> Layer0_Device11_Stage8_Accumulate
	Layer0_Device11_Stage7_Accumulate -> Layer0_Device11_Stage8_Accumulate
	Layer0_Device11_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage8_RecvKV -> Layer0_Device11_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage9_RecvKV -> Layer0_Device11_Stage9_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage9_Attention [label=Q_local]
	Layer0_Device11_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage9_Attention -> Layer0_Device11_Stage9_Accumulate
	Layer0_Device11_Stage8_Accumulate -> Layer0_Device11_Stage9_Accumulate
	Layer0_Device11_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage9_RecvKV -> Layer0_Device11_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage10_RecvKV -> Layer0_Device11_Stage10_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage10_Attention [label=Q_local]
	Layer0_Device11_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage10_Attention -> Layer0_Device11_Stage10_Accumulate
	Layer0_Device11_Stage9_Accumulate -> Layer0_Device11_Stage10_Accumulate
	Layer0_Device11_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage10_RecvKV -> Layer0_Device11_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage11_RecvKV -> Layer0_Device11_Stage11_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage11_Attention [label=Q_local]
	Layer0_Device11_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage11_Attention -> Layer0_Device11_Stage11_Accumulate
	Layer0_Device11_Stage10_Accumulate -> Layer0_Device11_Stage11_Accumulate
	Layer0_Device11_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage11_RecvKV -> Layer0_Device11_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage12_RecvKV -> Layer0_Device11_Stage12_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage12_Attention [label=Q_local]
	Layer0_Device11_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage12_Attention -> Layer0_Device11_Stage12_Accumulate
	Layer0_Device11_Stage11_Accumulate -> Layer0_Device11_Stage12_Accumulate
	Layer0_Device11_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage12_RecvKV -> Layer0_Device11_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage13_RecvKV -> Layer0_Device11_Stage13_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage13_Attention [label=Q_local]
	Layer0_Device11_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage13_Attention -> Layer0_Device11_Stage13_Accumulate
	Layer0_Device11_Stage12_Accumulate -> Layer0_Device11_Stage13_Accumulate
	Layer0_Device11_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage13_RecvKV -> Layer0_Device11_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage14_RecvKV -> Layer0_Device11_Stage14_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage14_Attention [label=Q_local]
	Layer0_Device11_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage14_Attention -> Layer0_Device11_Stage14_Accumulate
	Layer0_Device11_Stage13_Accumulate -> Layer0_Device11_Stage14_Accumulate
	Layer0_Device11_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device11_Stage14_RecvKV -> Layer0_Device11_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device11_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device11_Stage15_RecvKV -> Layer0_Device11_Stage15_Attention
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage15_Attention [label=Q_local]
	Layer0_Device11_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device11_Stage15_Attention -> Layer0_Device11_Stage15_Accumulate
	Layer0_Device11_Stage14_Accumulate -> Layer0_Device11_Stage15_Accumulate
	Layer0_Device11_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device11_Stage15_Accumulate -> Layer0_Device11_ConcatHeads
	Layer0_Device11_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device11_ConcatHeads -> Layer0_Device11_OutputProj
	Layer0_Device11_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device11_OutputProj -> Layer0_Device11_Residual1
	Layer0_Device11_Input -> Layer0_Device11_Residual1
	Layer0_Device11_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device11_Residual1 -> Layer0_Device11_LayerNorm2
	Layer0_Device11_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device11_LayerNorm2 -> Layer0_Device11_GateProj
	Layer0_Device11_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device11_LayerNorm2 -> Layer0_Device11_UpProj
	Layer0_Device11_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device11_GateProj -> Layer0_Device11_Activation
	Layer0_Device11_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device11_Activation -> Layer0_Device11_ElemMul
	Layer0_Device11_UpProj -> Layer0_Device11_ElemMul
	Layer0_Device11_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device11_ElemMul -> Layer0_Device11_DownProj
	Layer0_Device11_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device11_DownProj -> Layer0_Device11_Residual2
	Layer0_Device11_Residual1 -> Layer0_Device11_Residual2
	Layer0_Device11_Output [label="Layer 0 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device11_Residual2 -> Layer0_Device11_Output
	Layer0_Device12_Input [label="Layer 0 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device12_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device12_Input -> Layer0_Device12_LayerNorm1
	Layer0_Device12_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device12_LayerNorm1 -> Layer0_Device12_QKVProj
	Layer0_Device12_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device12_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage0_RecvKV -> Layer0_Device12_Stage0_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage0_Attention [label=Q_local]
	Layer0_Device12_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage0_Attention -> Layer0_Device12_Stage0_Accumulate
	Layer0_Device12_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage0_RecvKV -> Layer0_Device12_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage1_RecvKV -> Layer0_Device12_Stage1_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage1_Attention [label=Q_local]
	Layer0_Device12_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage1_Attention -> Layer0_Device12_Stage1_Accumulate
	Layer0_Device12_Stage0_Accumulate -> Layer0_Device12_Stage1_Accumulate
	Layer0_Device12_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage1_RecvKV -> Layer0_Device12_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage2_RecvKV -> Layer0_Device12_Stage2_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage2_Attention [label=Q_local]
	Layer0_Device12_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage2_Attention -> Layer0_Device12_Stage2_Accumulate
	Layer0_Device12_Stage1_Accumulate -> Layer0_Device12_Stage2_Accumulate
	Layer0_Device12_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage2_RecvKV -> Layer0_Device12_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage3_RecvKV -> Layer0_Device12_Stage3_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage3_Attention [label=Q_local]
	Layer0_Device12_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage3_Attention -> Layer0_Device12_Stage3_Accumulate
	Layer0_Device12_Stage2_Accumulate -> Layer0_Device12_Stage3_Accumulate
	Layer0_Device12_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage3_RecvKV -> Layer0_Device12_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage4_RecvKV -> Layer0_Device12_Stage4_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage4_Attention [label=Q_local]
	Layer0_Device12_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage4_Attention -> Layer0_Device12_Stage4_Accumulate
	Layer0_Device12_Stage3_Accumulate -> Layer0_Device12_Stage4_Accumulate
	Layer0_Device12_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage4_RecvKV -> Layer0_Device12_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage5_RecvKV -> Layer0_Device12_Stage5_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage5_Attention [label=Q_local]
	Layer0_Device12_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage5_Attention -> Layer0_Device12_Stage5_Accumulate
	Layer0_Device12_Stage4_Accumulate -> Layer0_Device12_Stage5_Accumulate
	Layer0_Device12_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage5_RecvKV -> Layer0_Device12_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage6_RecvKV -> Layer0_Device12_Stage6_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage6_Attention [label=Q_local]
	Layer0_Device12_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage6_Attention -> Layer0_Device12_Stage6_Accumulate
	Layer0_Device12_Stage5_Accumulate -> Layer0_Device12_Stage6_Accumulate
	Layer0_Device12_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage6_RecvKV -> Layer0_Device12_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage7_RecvKV -> Layer0_Device12_Stage7_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage7_Attention [label=Q_local]
	Layer0_Device12_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage7_Attention -> Layer0_Device12_Stage7_Accumulate
	Layer0_Device12_Stage6_Accumulate -> Layer0_Device12_Stage7_Accumulate
	Layer0_Device12_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage7_RecvKV -> Layer0_Device12_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage8_RecvKV -> Layer0_Device12_Stage8_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage8_Attention [label=Q_local]
	Layer0_Device12_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage8_Attention -> Layer0_Device12_Stage8_Accumulate
	Layer0_Device12_Stage7_Accumulate -> Layer0_Device12_Stage8_Accumulate
	Layer0_Device12_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage8_RecvKV -> Layer0_Device12_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage9_RecvKV -> Layer0_Device12_Stage9_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage9_Attention [label=Q_local]
	Layer0_Device12_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage9_Attention -> Layer0_Device12_Stage9_Accumulate
	Layer0_Device12_Stage8_Accumulate -> Layer0_Device12_Stage9_Accumulate
	Layer0_Device12_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage9_RecvKV -> Layer0_Device12_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage10_RecvKV -> Layer0_Device12_Stage10_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage10_Attention [label=Q_local]
	Layer0_Device12_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage10_Attention -> Layer0_Device12_Stage10_Accumulate
	Layer0_Device12_Stage9_Accumulate -> Layer0_Device12_Stage10_Accumulate
	Layer0_Device12_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage10_RecvKV -> Layer0_Device12_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage11_RecvKV -> Layer0_Device12_Stage11_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage11_Attention [label=Q_local]
	Layer0_Device12_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage11_Attention -> Layer0_Device12_Stage11_Accumulate
	Layer0_Device12_Stage10_Accumulate -> Layer0_Device12_Stage11_Accumulate
	Layer0_Device12_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage11_RecvKV -> Layer0_Device12_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage12_RecvKV -> Layer0_Device12_Stage12_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage12_Attention [label=Q_local]
	Layer0_Device12_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage12_Attention -> Layer0_Device12_Stage12_Accumulate
	Layer0_Device12_Stage11_Accumulate -> Layer0_Device12_Stage12_Accumulate
	Layer0_Device12_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage12_RecvKV -> Layer0_Device12_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage13_RecvKV -> Layer0_Device12_Stage13_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage13_Attention [label=Q_local]
	Layer0_Device12_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage13_Attention -> Layer0_Device12_Stage13_Accumulate
	Layer0_Device12_Stage12_Accumulate -> Layer0_Device12_Stage13_Accumulate
	Layer0_Device12_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage13_RecvKV -> Layer0_Device12_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage14_RecvKV -> Layer0_Device12_Stage14_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage14_Attention [label=Q_local]
	Layer0_Device12_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage14_Attention -> Layer0_Device12_Stage14_Accumulate
	Layer0_Device12_Stage13_Accumulate -> Layer0_Device12_Stage14_Accumulate
	Layer0_Device12_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device12_Stage14_RecvKV -> Layer0_Device12_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device12_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device12_Stage15_RecvKV -> Layer0_Device12_Stage15_Attention
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage15_Attention [label=Q_local]
	Layer0_Device12_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device12_Stage15_Attention -> Layer0_Device12_Stage15_Accumulate
	Layer0_Device12_Stage14_Accumulate -> Layer0_Device12_Stage15_Accumulate
	Layer0_Device12_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device12_Stage15_Accumulate -> Layer0_Device12_ConcatHeads
	Layer0_Device12_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device12_ConcatHeads -> Layer0_Device12_OutputProj
	Layer0_Device12_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device12_OutputProj -> Layer0_Device12_Residual1
	Layer0_Device12_Input -> Layer0_Device12_Residual1
	Layer0_Device12_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device12_Residual1 -> Layer0_Device12_LayerNorm2
	Layer0_Device12_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device12_LayerNorm2 -> Layer0_Device12_GateProj
	Layer0_Device12_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device12_LayerNorm2 -> Layer0_Device12_UpProj
	Layer0_Device12_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device12_GateProj -> Layer0_Device12_Activation
	Layer0_Device12_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device12_Activation -> Layer0_Device12_ElemMul
	Layer0_Device12_UpProj -> Layer0_Device12_ElemMul
	Layer0_Device12_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device12_ElemMul -> Layer0_Device12_DownProj
	Layer0_Device12_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device12_DownProj -> Layer0_Device12_Residual2
	Layer0_Device12_Residual1 -> Layer0_Device12_Residual2
	Layer0_Device12_Output [label="Layer 0 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device12_Residual2 -> Layer0_Device12_Output
	Layer0_Device13_Input [label="Layer 0 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device13_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device13_Input -> Layer0_Device13_LayerNorm1
	Layer0_Device13_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device13_LayerNorm1 -> Layer0_Device13_QKVProj
	Layer0_Device13_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device13_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage0_RecvKV -> Layer0_Device13_Stage0_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage0_Attention [label=Q_local]
	Layer0_Device13_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage0_Attention -> Layer0_Device13_Stage0_Accumulate
	Layer0_Device13_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage0_RecvKV -> Layer0_Device13_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage1_RecvKV -> Layer0_Device13_Stage1_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage1_Attention [label=Q_local]
	Layer0_Device13_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage1_Attention -> Layer0_Device13_Stage1_Accumulate
	Layer0_Device13_Stage0_Accumulate -> Layer0_Device13_Stage1_Accumulate
	Layer0_Device13_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage1_RecvKV -> Layer0_Device13_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage2_RecvKV -> Layer0_Device13_Stage2_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage2_Attention [label=Q_local]
	Layer0_Device13_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage2_Attention -> Layer0_Device13_Stage2_Accumulate
	Layer0_Device13_Stage1_Accumulate -> Layer0_Device13_Stage2_Accumulate
	Layer0_Device13_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage2_RecvKV -> Layer0_Device13_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage3_RecvKV -> Layer0_Device13_Stage3_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage3_Attention [label=Q_local]
	Layer0_Device13_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage3_Attention -> Layer0_Device13_Stage3_Accumulate
	Layer0_Device13_Stage2_Accumulate -> Layer0_Device13_Stage3_Accumulate
	Layer0_Device13_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage3_RecvKV -> Layer0_Device13_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage4_RecvKV -> Layer0_Device13_Stage4_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage4_Attention [label=Q_local]
	Layer0_Device13_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage4_Attention -> Layer0_Device13_Stage4_Accumulate
	Layer0_Device13_Stage3_Accumulate -> Layer0_Device13_Stage4_Accumulate
	Layer0_Device13_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage4_RecvKV -> Layer0_Device13_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage5_RecvKV -> Layer0_Device13_Stage5_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage5_Attention [label=Q_local]
	Layer0_Device13_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage5_Attention -> Layer0_Device13_Stage5_Accumulate
	Layer0_Device13_Stage4_Accumulate -> Layer0_Device13_Stage5_Accumulate
	Layer0_Device13_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage5_RecvKV -> Layer0_Device13_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage6_RecvKV -> Layer0_Device13_Stage6_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage6_Attention [label=Q_local]
	Layer0_Device13_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage6_Attention -> Layer0_Device13_Stage6_Accumulate
	Layer0_Device13_Stage5_Accumulate -> Layer0_Device13_Stage6_Accumulate
	Layer0_Device13_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage6_RecvKV -> Layer0_Device13_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage7_RecvKV -> Layer0_Device13_Stage7_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage7_Attention [label=Q_local]
	Layer0_Device13_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage7_Attention -> Layer0_Device13_Stage7_Accumulate
	Layer0_Device13_Stage6_Accumulate -> Layer0_Device13_Stage7_Accumulate
	Layer0_Device13_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage7_RecvKV -> Layer0_Device13_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage8_RecvKV -> Layer0_Device13_Stage8_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage8_Attention [label=Q_local]
	Layer0_Device13_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage8_Attention -> Layer0_Device13_Stage8_Accumulate
	Layer0_Device13_Stage7_Accumulate -> Layer0_Device13_Stage8_Accumulate
	Layer0_Device13_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage8_RecvKV -> Layer0_Device13_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage9_RecvKV -> Layer0_Device13_Stage9_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage9_Attention [label=Q_local]
	Layer0_Device13_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage9_Attention -> Layer0_Device13_Stage9_Accumulate
	Layer0_Device13_Stage8_Accumulate -> Layer0_Device13_Stage9_Accumulate
	Layer0_Device13_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage9_RecvKV -> Layer0_Device13_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage10_RecvKV -> Layer0_Device13_Stage10_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage10_Attention [label=Q_local]
	Layer0_Device13_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage10_Attention -> Layer0_Device13_Stage10_Accumulate
	Layer0_Device13_Stage9_Accumulate -> Layer0_Device13_Stage10_Accumulate
	Layer0_Device13_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage10_RecvKV -> Layer0_Device13_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage11_RecvKV -> Layer0_Device13_Stage11_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage11_Attention [label=Q_local]
	Layer0_Device13_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage11_Attention -> Layer0_Device13_Stage11_Accumulate
	Layer0_Device13_Stage10_Accumulate -> Layer0_Device13_Stage11_Accumulate
	Layer0_Device13_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage11_RecvKV -> Layer0_Device13_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage12_RecvKV -> Layer0_Device13_Stage12_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage12_Attention [label=Q_local]
	Layer0_Device13_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage12_Attention -> Layer0_Device13_Stage12_Accumulate
	Layer0_Device13_Stage11_Accumulate -> Layer0_Device13_Stage12_Accumulate
	Layer0_Device13_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage12_RecvKV -> Layer0_Device13_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage13_RecvKV -> Layer0_Device13_Stage13_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage13_Attention [label=Q_local]
	Layer0_Device13_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage13_Attention -> Layer0_Device13_Stage13_Accumulate
	Layer0_Device13_Stage12_Accumulate -> Layer0_Device13_Stage13_Accumulate
	Layer0_Device13_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage13_RecvKV -> Layer0_Device13_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage14_RecvKV -> Layer0_Device13_Stage14_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage14_Attention [label=Q_local]
	Layer0_Device13_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage14_Attention -> Layer0_Device13_Stage14_Accumulate
	Layer0_Device13_Stage13_Accumulate -> Layer0_Device13_Stage14_Accumulate
	Layer0_Device13_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device13_Stage14_RecvKV -> Layer0_Device13_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device13_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device13_Stage15_RecvKV -> Layer0_Device13_Stage15_Attention
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage15_Attention [label=Q_local]
	Layer0_Device13_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device13_Stage15_Attention -> Layer0_Device13_Stage15_Accumulate
	Layer0_Device13_Stage14_Accumulate -> Layer0_Device13_Stage15_Accumulate
	Layer0_Device13_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device13_Stage15_Accumulate -> Layer0_Device13_ConcatHeads
	Layer0_Device13_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device13_ConcatHeads -> Layer0_Device13_OutputProj
	Layer0_Device13_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device13_OutputProj -> Layer0_Device13_Residual1
	Layer0_Device13_Input -> Layer0_Device13_Residual1
	Layer0_Device13_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device13_Residual1 -> Layer0_Device13_LayerNorm2
	Layer0_Device13_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device13_LayerNorm2 -> Layer0_Device13_GateProj
	Layer0_Device13_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device13_LayerNorm2 -> Layer0_Device13_UpProj
	Layer0_Device13_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device13_GateProj -> Layer0_Device13_Activation
	Layer0_Device13_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device13_Activation -> Layer0_Device13_ElemMul
	Layer0_Device13_UpProj -> Layer0_Device13_ElemMul
	Layer0_Device13_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device13_ElemMul -> Layer0_Device13_DownProj
	Layer0_Device13_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device13_DownProj -> Layer0_Device13_Residual2
	Layer0_Device13_Residual1 -> Layer0_Device13_Residual2
	Layer0_Device13_Output [label="Layer 0 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device13_Residual2 -> Layer0_Device13_Output
	Layer0_Device14_Input [label="Layer 0 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device14_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device14_Input -> Layer0_Device14_LayerNorm1
	Layer0_Device14_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device14_LayerNorm1 -> Layer0_Device14_QKVProj
	Layer0_Device14_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device14_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage0_RecvKV -> Layer0_Device14_Stage0_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage0_Attention [label=Q_local]
	Layer0_Device14_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage0_Attention -> Layer0_Device14_Stage0_Accumulate
	Layer0_Device14_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage0_RecvKV -> Layer0_Device14_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage1_RecvKV -> Layer0_Device14_Stage1_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage1_Attention [label=Q_local]
	Layer0_Device14_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage1_Attention -> Layer0_Device14_Stage1_Accumulate
	Layer0_Device14_Stage0_Accumulate -> Layer0_Device14_Stage1_Accumulate
	Layer0_Device14_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage1_RecvKV -> Layer0_Device14_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage2_RecvKV -> Layer0_Device14_Stage2_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage2_Attention [label=Q_local]
	Layer0_Device14_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage2_Attention -> Layer0_Device14_Stage2_Accumulate
	Layer0_Device14_Stage1_Accumulate -> Layer0_Device14_Stage2_Accumulate
	Layer0_Device14_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage2_RecvKV -> Layer0_Device14_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage3_RecvKV -> Layer0_Device14_Stage3_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage3_Attention [label=Q_local]
	Layer0_Device14_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage3_Attention -> Layer0_Device14_Stage3_Accumulate
	Layer0_Device14_Stage2_Accumulate -> Layer0_Device14_Stage3_Accumulate
	Layer0_Device14_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage3_RecvKV -> Layer0_Device14_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage4_RecvKV -> Layer0_Device14_Stage4_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage4_Attention [label=Q_local]
	Layer0_Device14_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage4_Attention -> Layer0_Device14_Stage4_Accumulate
	Layer0_Device14_Stage3_Accumulate -> Layer0_Device14_Stage4_Accumulate
	Layer0_Device14_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage4_RecvKV -> Layer0_Device14_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage5_RecvKV -> Layer0_Device14_Stage5_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage5_Attention [label=Q_local]
	Layer0_Device14_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage5_Attention -> Layer0_Device14_Stage5_Accumulate
	Layer0_Device14_Stage4_Accumulate -> Layer0_Device14_Stage5_Accumulate
	Layer0_Device14_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage5_RecvKV -> Layer0_Device14_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage6_RecvKV -> Layer0_Device14_Stage6_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage6_Attention [label=Q_local]
	Layer0_Device14_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage6_Attention -> Layer0_Device14_Stage6_Accumulate
	Layer0_Device14_Stage5_Accumulate -> Layer0_Device14_Stage6_Accumulate
	Layer0_Device14_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage6_RecvKV -> Layer0_Device14_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage7_RecvKV -> Layer0_Device14_Stage7_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage7_Attention [label=Q_local]
	Layer0_Device14_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage7_Attention -> Layer0_Device14_Stage7_Accumulate
	Layer0_Device14_Stage6_Accumulate -> Layer0_Device14_Stage7_Accumulate
	Layer0_Device14_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage7_RecvKV -> Layer0_Device14_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage8_RecvKV -> Layer0_Device14_Stage8_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage8_Attention [label=Q_local]
	Layer0_Device14_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage8_Attention -> Layer0_Device14_Stage8_Accumulate
	Layer0_Device14_Stage7_Accumulate -> Layer0_Device14_Stage8_Accumulate
	Layer0_Device14_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage8_RecvKV -> Layer0_Device14_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage9_RecvKV -> Layer0_Device14_Stage9_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage9_Attention [label=Q_local]
	Layer0_Device14_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage9_Attention -> Layer0_Device14_Stage9_Accumulate
	Layer0_Device14_Stage8_Accumulate -> Layer0_Device14_Stage9_Accumulate
	Layer0_Device14_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage9_RecvKV -> Layer0_Device14_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage10_RecvKV -> Layer0_Device14_Stage10_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage10_Attention [label=Q_local]
	Layer0_Device14_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage10_Attention -> Layer0_Device14_Stage10_Accumulate
	Layer0_Device14_Stage9_Accumulate -> Layer0_Device14_Stage10_Accumulate
	Layer0_Device14_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage10_RecvKV -> Layer0_Device14_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage11_RecvKV -> Layer0_Device14_Stage11_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage11_Attention [label=Q_local]
	Layer0_Device14_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage11_Attention -> Layer0_Device14_Stage11_Accumulate
	Layer0_Device14_Stage10_Accumulate -> Layer0_Device14_Stage11_Accumulate
	Layer0_Device14_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage11_RecvKV -> Layer0_Device14_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage12_RecvKV -> Layer0_Device14_Stage12_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage12_Attention [label=Q_local]
	Layer0_Device14_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage12_Attention -> Layer0_Device14_Stage12_Accumulate
	Layer0_Device14_Stage11_Accumulate -> Layer0_Device14_Stage12_Accumulate
	Layer0_Device14_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage12_RecvKV -> Layer0_Device14_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage13_RecvKV -> Layer0_Device14_Stage13_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage13_Attention [label=Q_local]
	Layer0_Device14_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage13_Attention -> Layer0_Device14_Stage13_Accumulate
	Layer0_Device14_Stage12_Accumulate -> Layer0_Device14_Stage13_Accumulate
	Layer0_Device14_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage13_RecvKV -> Layer0_Device14_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage14_RecvKV -> Layer0_Device14_Stage14_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage14_Attention [label=Q_local]
	Layer0_Device14_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage14_Attention -> Layer0_Device14_Stage14_Accumulate
	Layer0_Device14_Stage13_Accumulate -> Layer0_Device14_Stage14_Accumulate
	Layer0_Device14_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device14_Stage14_RecvKV -> Layer0_Device14_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device14_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device14_Stage15_RecvKV -> Layer0_Device14_Stage15_Attention
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage15_Attention [label=Q_local]
	Layer0_Device14_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device14_Stage15_Attention -> Layer0_Device14_Stage15_Accumulate
	Layer0_Device14_Stage14_Accumulate -> Layer0_Device14_Stage15_Accumulate
	Layer0_Device14_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device14_Stage15_Accumulate -> Layer0_Device14_ConcatHeads
	Layer0_Device14_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device14_ConcatHeads -> Layer0_Device14_OutputProj
	Layer0_Device14_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device14_OutputProj -> Layer0_Device14_Residual1
	Layer0_Device14_Input -> Layer0_Device14_Residual1
	Layer0_Device14_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device14_Residual1 -> Layer0_Device14_LayerNorm2
	Layer0_Device14_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device14_LayerNorm2 -> Layer0_Device14_GateProj
	Layer0_Device14_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device14_LayerNorm2 -> Layer0_Device14_UpProj
	Layer0_Device14_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device14_GateProj -> Layer0_Device14_Activation
	Layer0_Device14_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device14_Activation -> Layer0_Device14_ElemMul
	Layer0_Device14_UpProj -> Layer0_Device14_ElemMul
	Layer0_Device14_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device14_ElemMul -> Layer0_Device14_DownProj
	Layer0_Device14_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device14_DownProj -> Layer0_Device14_Residual2
	Layer0_Device14_Residual1 -> Layer0_Device14_Residual2
	Layer0_Device14_Output [label="Layer 0 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device14_Residual2 -> Layer0_Device14_Output
	Layer0_Device15_Input [label="Layer 0 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device15_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device15_Input -> Layer0_Device15_LayerNorm1
	Layer0_Device15_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device15_LayerNorm1 -> Layer0_Device15_QKVProj
	Layer0_Device15_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage0_RecvKV [label="Local K,V"]
	Layer0_Device15_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage0_RecvKV -> Layer0_Device15_Stage0_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage0_Attention [label=Q_local]
	Layer0_Device15_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage0_Attention -> Layer0_Device15_Stage0_Accumulate
	Layer0_Device15_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage0_RecvKV -> Layer0_Device15_Stage1_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage1_RecvKV -> Layer0_Device15_Stage1_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage1_Attention [label=Q_local]
	Layer0_Device15_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage1_Attention -> Layer0_Device15_Stage1_Accumulate
	Layer0_Device15_Stage0_Accumulate -> Layer0_Device15_Stage1_Accumulate
	Layer0_Device15_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage1_RecvKV -> Layer0_Device15_Stage2_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage2_RecvKV -> Layer0_Device15_Stage2_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage2_Attention [label=Q_local]
	Layer0_Device15_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage2_Attention -> Layer0_Device15_Stage2_Accumulate
	Layer0_Device15_Stage1_Accumulate -> Layer0_Device15_Stage2_Accumulate
	Layer0_Device15_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage2_RecvKV -> Layer0_Device15_Stage3_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage3_RecvKV -> Layer0_Device15_Stage3_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage3_Attention [label=Q_local]
	Layer0_Device15_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage3_Attention -> Layer0_Device15_Stage3_Accumulate
	Layer0_Device15_Stage2_Accumulate -> Layer0_Device15_Stage3_Accumulate
	Layer0_Device15_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage3_RecvKV -> Layer0_Device15_Stage4_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage4_RecvKV -> Layer0_Device15_Stage4_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage4_Attention [label=Q_local]
	Layer0_Device15_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage4_Attention -> Layer0_Device15_Stage4_Accumulate
	Layer0_Device15_Stage3_Accumulate -> Layer0_Device15_Stage4_Accumulate
	Layer0_Device15_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage4_RecvKV -> Layer0_Device15_Stage5_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage5_RecvKV -> Layer0_Device15_Stage5_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage5_Attention [label=Q_local]
	Layer0_Device15_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage5_Attention -> Layer0_Device15_Stage5_Accumulate
	Layer0_Device15_Stage4_Accumulate -> Layer0_Device15_Stage5_Accumulate
	Layer0_Device15_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage5_RecvKV -> Layer0_Device15_Stage6_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage6_RecvKV -> Layer0_Device15_Stage6_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage6_Attention [label=Q_local]
	Layer0_Device15_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage6_Attention -> Layer0_Device15_Stage6_Accumulate
	Layer0_Device15_Stage5_Accumulate -> Layer0_Device15_Stage6_Accumulate
	Layer0_Device15_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage6_RecvKV -> Layer0_Device15_Stage7_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage7_RecvKV -> Layer0_Device15_Stage7_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage7_Attention [label=Q_local]
	Layer0_Device15_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage7_Attention -> Layer0_Device15_Stage7_Accumulate
	Layer0_Device15_Stage6_Accumulate -> Layer0_Device15_Stage7_Accumulate
	Layer0_Device15_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage7_RecvKV -> Layer0_Device15_Stage8_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage8_RecvKV -> Layer0_Device15_Stage8_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage8_Attention [label=Q_local]
	Layer0_Device15_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage8_Attention -> Layer0_Device15_Stage8_Accumulate
	Layer0_Device15_Stage7_Accumulate -> Layer0_Device15_Stage8_Accumulate
	Layer0_Device15_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage8_RecvKV -> Layer0_Device15_Stage9_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage9_RecvKV -> Layer0_Device15_Stage9_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage9_Attention [label=Q_local]
	Layer0_Device15_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage9_Attention -> Layer0_Device15_Stage9_Accumulate
	Layer0_Device15_Stage8_Accumulate -> Layer0_Device15_Stage9_Accumulate
	Layer0_Device15_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage9_RecvKV -> Layer0_Device15_Stage10_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage10_RecvKV -> Layer0_Device15_Stage10_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage10_Attention [label=Q_local]
	Layer0_Device15_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage10_Attention -> Layer0_Device15_Stage10_Accumulate
	Layer0_Device15_Stage9_Accumulate -> Layer0_Device15_Stage10_Accumulate
	Layer0_Device15_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage10_RecvKV -> Layer0_Device15_Stage11_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage11_RecvKV -> Layer0_Device15_Stage11_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage11_Attention [label=Q_local]
	Layer0_Device15_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage11_Attention -> Layer0_Device15_Stage11_Accumulate
	Layer0_Device15_Stage10_Accumulate -> Layer0_Device15_Stage11_Accumulate
	Layer0_Device15_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage11_RecvKV -> Layer0_Device15_Stage12_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage12_RecvKV -> Layer0_Device15_Stage12_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage12_Attention [label=Q_local]
	Layer0_Device15_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage12_Attention -> Layer0_Device15_Stage12_Accumulate
	Layer0_Device15_Stage11_Accumulate -> Layer0_Device15_Stage12_Accumulate
	Layer0_Device15_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage12_RecvKV -> Layer0_Device15_Stage13_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage13_RecvKV -> Layer0_Device15_Stage13_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage13_Attention [label=Q_local]
	Layer0_Device15_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage13_Attention -> Layer0_Device15_Stage13_Accumulate
	Layer0_Device15_Stage12_Accumulate -> Layer0_Device15_Stage13_Accumulate
	Layer0_Device15_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage13_RecvKV -> Layer0_Device15_Stage14_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage14_RecvKV -> Layer0_Device15_Stage14_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage14_Attention [label=Q_local]
	Layer0_Device15_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage14_Attention -> Layer0_Device15_Stage14_Accumulate
	Layer0_Device15_Stage13_Accumulate -> Layer0_Device15_Stage14_Accumulate
	Layer0_Device15_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer0_Device15_Stage14_RecvKV -> Layer0_Device15_Stage15_RecvKV [label="Ring transfer"]
	Layer0_Device15_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer0_Device15_Stage15_RecvKV -> Layer0_Device15_Stage15_Attention
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage15_Attention [label=Q_local]
	Layer0_Device15_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer0_Device15_Stage15_Attention -> Layer0_Device15_Stage15_Accumulate
	Layer0_Device15_Stage14_Accumulate -> Layer0_Device15_Stage15_Accumulate
	Layer0_Device15_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer0_Device15_Stage15_Accumulate -> Layer0_Device15_ConcatHeads
	Layer0_Device15_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device15_ConcatHeads -> Layer0_Device15_OutputProj
	Layer0_Device15_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device15_OutputProj -> Layer0_Device15_Residual1
	Layer0_Device15_Input -> Layer0_Device15_Residual1
	Layer0_Device15_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device15_Residual1 -> Layer0_Device15_LayerNorm2
	Layer0_Device15_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device15_LayerNorm2 -> Layer0_Device15_GateProj
	Layer0_Device15_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device15_LayerNorm2 -> Layer0_Device15_UpProj
	Layer0_Device15_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device15_GateProj -> Layer0_Device15_Activation
	Layer0_Device15_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer0_Device15_Activation -> Layer0_Device15_ElemMul
	Layer0_Device15_UpProj -> Layer0_Device15_ElemMul
	Layer0_Device15_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer0_Device15_ElemMul -> Layer0_Device15_DownProj
	Layer0_Device15_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer0_Device15_DownProj -> Layer0_Device15_Residual2
	Layer0_Device15_Residual1 -> Layer0_Device15_Residual2
	Layer0_Device15_Output [label="Layer 0 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device15_Residual2 -> Layer0_Device15_Output
	Layer1_Device0_Input [label="Layer 1 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device0_Output -> Layer1_Device0_Input
	Layer1_Device0_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device0_Input -> Layer1_Device0_LayerNorm1
	Layer1_Device0_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device0_LayerNorm1 -> Layer1_Device0_QKVProj
	Layer1_Device0_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device0_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage0_RecvKV -> Layer1_Device0_Stage0_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage0_Attention [label=Q_local]
	Layer1_Device0_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage0_Attention -> Layer1_Device0_Stage0_Accumulate
	Layer1_Device0_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage0_RecvKV -> Layer1_Device0_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage1_RecvKV -> Layer1_Device0_Stage1_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage1_Attention [label=Q_local]
	Layer1_Device0_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage1_Attention -> Layer1_Device0_Stage1_Accumulate
	Layer1_Device0_Stage0_Accumulate -> Layer1_Device0_Stage1_Accumulate
	Layer1_Device0_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage1_RecvKV -> Layer1_Device0_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage2_RecvKV -> Layer1_Device0_Stage2_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage2_Attention [label=Q_local]
	Layer1_Device0_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage2_Attention -> Layer1_Device0_Stage2_Accumulate
	Layer1_Device0_Stage1_Accumulate -> Layer1_Device0_Stage2_Accumulate
	Layer1_Device0_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage2_RecvKV -> Layer1_Device0_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage3_RecvKV -> Layer1_Device0_Stage3_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage3_Attention [label=Q_local]
	Layer1_Device0_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage3_Attention -> Layer1_Device0_Stage3_Accumulate
	Layer1_Device0_Stage2_Accumulate -> Layer1_Device0_Stage3_Accumulate
	Layer1_Device0_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage3_RecvKV -> Layer1_Device0_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage4_RecvKV -> Layer1_Device0_Stage4_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage4_Attention [label=Q_local]
	Layer1_Device0_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage4_Attention -> Layer1_Device0_Stage4_Accumulate
	Layer1_Device0_Stage3_Accumulate -> Layer1_Device0_Stage4_Accumulate
	Layer1_Device0_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage4_RecvKV -> Layer1_Device0_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage5_RecvKV -> Layer1_Device0_Stage5_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage5_Attention [label=Q_local]
	Layer1_Device0_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage5_Attention -> Layer1_Device0_Stage5_Accumulate
	Layer1_Device0_Stage4_Accumulate -> Layer1_Device0_Stage5_Accumulate
	Layer1_Device0_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage5_RecvKV -> Layer1_Device0_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage6_RecvKV -> Layer1_Device0_Stage6_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage6_Attention [label=Q_local]
	Layer1_Device0_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage6_Attention -> Layer1_Device0_Stage6_Accumulate
	Layer1_Device0_Stage5_Accumulate -> Layer1_Device0_Stage6_Accumulate
	Layer1_Device0_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage6_RecvKV -> Layer1_Device0_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage7_RecvKV -> Layer1_Device0_Stage7_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage7_Attention [label=Q_local]
	Layer1_Device0_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage7_Attention -> Layer1_Device0_Stage7_Accumulate
	Layer1_Device0_Stage6_Accumulate -> Layer1_Device0_Stage7_Accumulate
	Layer1_Device0_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage7_RecvKV -> Layer1_Device0_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage8_RecvKV -> Layer1_Device0_Stage8_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage8_Attention [label=Q_local]
	Layer1_Device0_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage8_Attention -> Layer1_Device0_Stage8_Accumulate
	Layer1_Device0_Stage7_Accumulate -> Layer1_Device0_Stage8_Accumulate
	Layer1_Device0_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage8_RecvKV -> Layer1_Device0_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage9_RecvKV -> Layer1_Device0_Stage9_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage9_Attention [label=Q_local]
	Layer1_Device0_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage9_Attention -> Layer1_Device0_Stage9_Accumulate
	Layer1_Device0_Stage8_Accumulate -> Layer1_Device0_Stage9_Accumulate
	Layer1_Device0_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage9_RecvKV -> Layer1_Device0_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage10_RecvKV -> Layer1_Device0_Stage10_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage10_Attention [label=Q_local]
	Layer1_Device0_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage10_Attention -> Layer1_Device0_Stage10_Accumulate
	Layer1_Device0_Stage9_Accumulate -> Layer1_Device0_Stage10_Accumulate
	Layer1_Device0_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage10_RecvKV -> Layer1_Device0_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage11_RecvKV -> Layer1_Device0_Stage11_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage11_Attention [label=Q_local]
	Layer1_Device0_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage11_Attention -> Layer1_Device0_Stage11_Accumulate
	Layer1_Device0_Stage10_Accumulate -> Layer1_Device0_Stage11_Accumulate
	Layer1_Device0_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage11_RecvKV -> Layer1_Device0_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage12_RecvKV -> Layer1_Device0_Stage12_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage12_Attention [label=Q_local]
	Layer1_Device0_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage12_Attention -> Layer1_Device0_Stage12_Accumulate
	Layer1_Device0_Stage11_Accumulate -> Layer1_Device0_Stage12_Accumulate
	Layer1_Device0_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage12_RecvKV -> Layer1_Device0_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage13_RecvKV -> Layer1_Device0_Stage13_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage13_Attention [label=Q_local]
	Layer1_Device0_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage13_Attention -> Layer1_Device0_Stage13_Accumulate
	Layer1_Device0_Stage12_Accumulate -> Layer1_Device0_Stage13_Accumulate
	Layer1_Device0_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage13_RecvKV -> Layer1_Device0_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage14_RecvKV -> Layer1_Device0_Stage14_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage14_Attention [label=Q_local]
	Layer1_Device0_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage14_Attention -> Layer1_Device0_Stage14_Accumulate
	Layer1_Device0_Stage13_Accumulate -> Layer1_Device0_Stage14_Accumulate
	Layer1_Device0_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device0_Stage14_RecvKV -> Layer1_Device0_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device0_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device0_Stage15_RecvKV -> Layer1_Device0_Stage15_Attention
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage15_Attention [label=Q_local]
	Layer1_Device0_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device0_Stage15_Attention -> Layer1_Device0_Stage15_Accumulate
	Layer1_Device0_Stage14_Accumulate -> Layer1_Device0_Stage15_Accumulate
	Layer1_Device0_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device0_Stage15_Accumulate -> Layer1_Device0_ConcatHeads
	Layer1_Device0_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device0_ConcatHeads -> Layer1_Device0_OutputProj
	Layer1_Device0_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device0_OutputProj -> Layer1_Device0_Residual1
	Layer1_Device0_Input -> Layer1_Device0_Residual1
	Layer1_Device0_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device0_Residual1 -> Layer1_Device0_LayerNorm2
	Layer1_Device0_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device0_LayerNorm2 -> Layer1_Device0_GateProj
	Layer1_Device0_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device0_LayerNorm2 -> Layer1_Device0_UpProj
	Layer1_Device0_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device0_GateProj -> Layer1_Device0_Activation
	Layer1_Device0_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device0_Activation -> Layer1_Device0_ElemMul
	Layer1_Device0_UpProj -> Layer1_Device0_ElemMul
	Layer1_Device0_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device0_ElemMul -> Layer1_Device0_DownProj
	Layer1_Device0_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device0_DownProj -> Layer1_Device0_Residual2
	Layer1_Device0_Residual1 -> Layer1_Device0_Residual2
	Layer1_Device0_Output [label="Layer 1 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device0_Residual2 -> Layer1_Device0_Output
	Layer1_Device1_Input [label="Layer 1 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device1_Output -> Layer1_Device1_Input
	Layer1_Device1_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device1_Input -> Layer1_Device1_LayerNorm1
	Layer1_Device1_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device1_LayerNorm1 -> Layer1_Device1_QKVProj
	Layer1_Device1_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device1_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage0_RecvKV -> Layer1_Device1_Stage0_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage0_Attention [label=Q_local]
	Layer1_Device1_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage0_Attention -> Layer1_Device1_Stage0_Accumulate
	Layer1_Device1_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage0_RecvKV -> Layer1_Device1_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage1_RecvKV -> Layer1_Device1_Stage1_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage1_Attention [label=Q_local]
	Layer1_Device1_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage1_Attention -> Layer1_Device1_Stage1_Accumulate
	Layer1_Device1_Stage0_Accumulate -> Layer1_Device1_Stage1_Accumulate
	Layer1_Device1_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage1_RecvKV -> Layer1_Device1_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage2_RecvKV -> Layer1_Device1_Stage2_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage2_Attention [label=Q_local]
	Layer1_Device1_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage2_Attention -> Layer1_Device1_Stage2_Accumulate
	Layer1_Device1_Stage1_Accumulate -> Layer1_Device1_Stage2_Accumulate
	Layer1_Device1_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage2_RecvKV -> Layer1_Device1_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage3_RecvKV -> Layer1_Device1_Stage3_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage3_Attention [label=Q_local]
	Layer1_Device1_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage3_Attention -> Layer1_Device1_Stage3_Accumulate
	Layer1_Device1_Stage2_Accumulate -> Layer1_Device1_Stage3_Accumulate
	Layer1_Device1_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage3_RecvKV -> Layer1_Device1_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage4_RecvKV -> Layer1_Device1_Stage4_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage4_Attention [label=Q_local]
	Layer1_Device1_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage4_Attention -> Layer1_Device1_Stage4_Accumulate
	Layer1_Device1_Stage3_Accumulate -> Layer1_Device1_Stage4_Accumulate
	Layer1_Device1_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage4_RecvKV -> Layer1_Device1_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage5_RecvKV -> Layer1_Device1_Stage5_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage5_Attention [label=Q_local]
	Layer1_Device1_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage5_Attention -> Layer1_Device1_Stage5_Accumulate
	Layer1_Device1_Stage4_Accumulate -> Layer1_Device1_Stage5_Accumulate
	Layer1_Device1_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage5_RecvKV -> Layer1_Device1_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage6_RecvKV -> Layer1_Device1_Stage6_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage6_Attention [label=Q_local]
	Layer1_Device1_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage6_Attention -> Layer1_Device1_Stage6_Accumulate
	Layer1_Device1_Stage5_Accumulate -> Layer1_Device1_Stage6_Accumulate
	Layer1_Device1_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage6_RecvKV -> Layer1_Device1_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage7_RecvKV -> Layer1_Device1_Stage7_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage7_Attention [label=Q_local]
	Layer1_Device1_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage7_Attention -> Layer1_Device1_Stage7_Accumulate
	Layer1_Device1_Stage6_Accumulate -> Layer1_Device1_Stage7_Accumulate
	Layer1_Device1_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage7_RecvKV -> Layer1_Device1_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage8_RecvKV -> Layer1_Device1_Stage8_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage8_Attention [label=Q_local]
	Layer1_Device1_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage8_Attention -> Layer1_Device1_Stage8_Accumulate
	Layer1_Device1_Stage7_Accumulate -> Layer1_Device1_Stage8_Accumulate
	Layer1_Device1_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage8_RecvKV -> Layer1_Device1_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage9_RecvKV -> Layer1_Device1_Stage9_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage9_Attention [label=Q_local]
	Layer1_Device1_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage9_Attention -> Layer1_Device1_Stage9_Accumulate
	Layer1_Device1_Stage8_Accumulate -> Layer1_Device1_Stage9_Accumulate
	Layer1_Device1_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage9_RecvKV -> Layer1_Device1_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage10_RecvKV -> Layer1_Device1_Stage10_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage10_Attention [label=Q_local]
	Layer1_Device1_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage10_Attention -> Layer1_Device1_Stage10_Accumulate
	Layer1_Device1_Stage9_Accumulate -> Layer1_Device1_Stage10_Accumulate
	Layer1_Device1_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage10_RecvKV -> Layer1_Device1_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage11_RecvKV -> Layer1_Device1_Stage11_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage11_Attention [label=Q_local]
	Layer1_Device1_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage11_Attention -> Layer1_Device1_Stage11_Accumulate
	Layer1_Device1_Stage10_Accumulate -> Layer1_Device1_Stage11_Accumulate
	Layer1_Device1_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage11_RecvKV -> Layer1_Device1_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage12_RecvKV -> Layer1_Device1_Stage12_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage12_Attention [label=Q_local]
	Layer1_Device1_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage12_Attention -> Layer1_Device1_Stage12_Accumulate
	Layer1_Device1_Stage11_Accumulate -> Layer1_Device1_Stage12_Accumulate
	Layer1_Device1_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage12_RecvKV -> Layer1_Device1_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage13_RecvKV -> Layer1_Device1_Stage13_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage13_Attention [label=Q_local]
	Layer1_Device1_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage13_Attention -> Layer1_Device1_Stage13_Accumulate
	Layer1_Device1_Stage12_Accumulate -> Layer1_Device1_Stage13_Accumulate
	Layer1_Device1_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage13_RecvKV -> Layer1_Device1_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage14_RecvKV -> Layer1_Device1_Stage14_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage14_Attention [label=Q_local]
	Layer1_Device1_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage14_Attention -> Layer1_Device1_Stage14_Accumulate
	Layer1_Device1_Stage13_Accumulate -> Layer1_Device1_Stage14_Accumulate
	Layer1_Device1_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device1_Stage14_RecvKV -> Layer1_Device1_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device1_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device1_Stage15_RecvKV -> Layer1_Device1_Stage15_Attention
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage15_Attention [label=Q_local]
	Layer1_Device1_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device1_Stage15_Attention -> Layer1_Device1_Stage15_Accumulate
	Layer1_Device1_Stage14_Accumulate -> Layer1_Device1_Stage15_Accumulate
	Layer1_Device1_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device1_Stage15_Accumulate -> Layer1_Device1_ConcatHeads
	Layer1_Device1_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device1_ConcatHeads -> Layer1_Device1_OutputProj
	Layer1_Device1_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device1_OutputProj -> Layer1_Device1_Residual1
	Layer1_Device1_Input -> Layer1_Device1_Residual1
	Layer1_Device1_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device1_Residual1 -> Layer1_Device1_LayerNorm2
	Layer1_Device1_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device1_LayerNorm2 -> Layer1_Device1_GateProj
	Layer1_Device1_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device1_LayerNorm2 -> Layer1_Device1_UpProj
	Layer1_Device1_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device1_GateProj -> Layer1_Device1_Activation
	Layer1_Device1_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device1_Activation -> Layer1_Device1_ElemMul
	Layer1_Device1_UpProj -> Layer1_Device1_ElemMul
	Layer1_Device1_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device1_ElemMul -> Layer1_Device1_DownProj
	Layer1_Device1_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device1_DownProj -> Layer1_Device1_Residual2
	Layer1_Device1_Residual1 -> Layer1_Device1_Residual2
	Layer1_Device1_Output [label="Layer 1 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device1_Residual2 -> Layer1_Device1_Output
	Layer1_Device2_Input [label="Layer 1 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device2_Output -> Layer1_Device2_Input
	Layer1_Device2_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device2_Input -> Layer1_Device2_LayerNorm1
	Layer1_Device2_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device2_LayerNorm1 -> Layer1_Device2_QKVProj
	Layer1_Device2_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device2_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage0_RecvKV -> Layer1_Device2_Stage0_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage0_Attention [label=Q_local]
	Layer1_Device2_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage0_Attention -> Layer1_Device2_Stage0_Accumulate
	Layer1_Device2_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage0_RecvKV -> Layer1_Device2_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage1_RecvKV -> Layer1_Device2_Stage1_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage1_Attention [label=Q_local]
	Layer1_Device2_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage1_Attention -> Layer1_Device2_Stage1_Accumulate
	Layer1_Device2_Stage0_Accumulate -> Layer1_Device2_Stage1_Accumulate
	Layer1_Device2_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage1_RecvKV -> Layer1_Device2_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage2_RecvKV -> Layer1_Device2_Stage2_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage2_Attention [label=Q_local]
	Layer1_Device2_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage2_Attention -> Layer1_Device2_Stage2_Accumulate
	Layer1_Device2_Stage1_Accumulate -> Layer1_Device2_Stage2_Accumulate
	Layer1_Device2_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage2_RecvKV -> Layer1_Device2_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage3_RecvKV -> Layer1_Device2_Stage3_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage3_Attention [label=Q_local]
	Layer1_Device2_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage3_Attention -> Layer1_Device2_Stage3_Accumulate
	Layer1_Device2_Stage2_Accumulate -> Layer1_Device2_Stage3_Accumulate
	Layer1_Device2_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage3_RecvKV -> Layer1_Device2_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage4_RecvKV -> Layer1_Device2_Stage4_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage4_Attention [label=Q_local]
	Layer1_Device2_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage4_Attention -> Layer1_Device2_Stage4_Accumulate
	Layer1_Device2_Stage3_Accumulate -> Layer1_Device2_Stage4_Accumulate
	Layer1_Device2_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage4_RecvKV -> Layer1_Device2_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage5_RecvKV -> Layer1_Device2_Stage5_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage5_Attention [label=Q_local]
	Layer1_Device2_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage5_Attention -> Layer1_Device2_Stage5_Accumulate
	Layer1_Device2_Stage4_Accumulate -> Layer1_Device2_Stage5_Accumulate
	Layer1_Device2_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage5_RecvKV -> Layer1_Device2_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage6_RecvKV -> Layer1_Device2_Stage6_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage6_Attention [label=Q_local]
	Layer1_Device2_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage6_Attention -> Layer1_Device2_Stage6_Accumulate
	Layer1_Device2_Stage5_Accumulate -> Layer1_Device2_Stage6_Accumulate
	Layer1_Device2_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage6_RecvKV -> Layer1_Device2_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage7_RecvKV -> Layer1_Device2_Stage7_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage7_Attention [label=Q_local]
	Layer1_Device2_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage7_Attention -> Layer1_Device2_Stage7_Accumulate
	Layer1_Device2_Stage6_Accumulate -> Layer1_Device2_Stage7_Accumulate
	Layer1_Device2_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage7_RecvKV -> Layer1_Device2_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage8_RecvKV -> Layer1_Device2_Stage8_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage8_Attention [label=Q_local]
	Layer1_Device2_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage8_Attention -> Layer1_Device2_Stage8_Accumulate
	Layer1_Device2_Stage7_Accumulate -> Layer1_Device2_Stage8_Accumulate
	Layer1_Device2_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage8_RecvKV -> Layer1_Device2_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage9_RecvKV -> Layer1_Device2_Stage9_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage9_Attention [label=Q_local]
	Layer1_Device2_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage9_Attention -> Layer1_Device2_Stage9_Accumulate
	Layer1_Device2_Stage8_Accumulate -> Layer1_Device2_Stage9_Accumulate
	Layer1_Device2_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage9_RecvKV -> Layer1_Device2_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage10_RecvKV -> Layer1_Device2_Stage10_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage10_Attention [label=Q_local]
	Layer1_Device2_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage10_Attention -> Layer1_Device2_Stage10_Accumulate
	Layer1_Device2_Stage9_Accumulate -> Layer1_Device2_Stage10_Accumulate
	Layer1_Device2_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage10_RecvKV -> Layer1_Device2_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage11_RecvKV -> Layer1_Device2_Stage11_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage11_Attention [label=Q_local]
	Layer1_Device2_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage11_Attention -> Layer1_Device2_Stage11_Accumulate
	Layer1_Device2_Stage10_Accumulate -> Layer1_Device2_Stage11_Accumulate
	Layer1_Device2_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage11_RecvKV -> Layer1_Device2_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage12_RecvKV -> Layer1_Device2_Stage12_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage12_Attention [label=Q_local]
	Layer1_Device2_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage12_Attention -> Layer1_Device2_Stage12_Accumulate
	Layer1_Device2_Stage11_Accumulate -> Layer1_Device2_Stage12_Accumulate
	Layer1_Device2_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage12_RecvKV -> Layer1_Device2_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage13_RecvKV -> Layer1_Device2_Stage13_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage13_Attention [label=Q_local]
	Layer1_Device2_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage13_Attention -> Layer1_Device2_Stage13_Accumulate
	Layer1_Device2_Stage12_Accumulate -> Layer1_Device2_Stage13_Accumulate
	Layer1_Device2_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage13_RecvKV -> Layer1_Device2_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage14_RecvKV -> Layer1_Device2_Stage14_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage14_Attention [label=Q_local]
	Layer1_Device2_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage14_Attention -> Layer1_Device2_Stage14_Accumulate
	Layer1_Device2_Stage13_Accumulate -> Layer1_Device2_Stage14_Accumulate
	Layer1_Device2_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device2_Stage14_RecvKV -> Layer1_Device2_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device2_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device2_Stage15_RecvKV -> Layer1_Device2_Stage15_Attention
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage15_Attention [label=Q_local]
	Layer1_Device2_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device2_Stage15_Attention -> Layer1_Device2_Stage15_Accumulate
	Layer1_Device2_Stage14_Accumulate -> Layer1_Device2_Stage15_Accumulate
	Layer1_Device2_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device2_Stage15_Accumulate -> Layer1_Device2_ConcatHeads
	Layer1_Device2_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device2_ConcatHeads -> Layer1_Device2_OutputProj
	Layer1_Device2_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device2_OutputProj -> Layer1_Device2_Residual1
	Layer1_Device2_Input -> Layer1_Device2_Residual1
	Layer1_Device2_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device2_Residual1 -> Layer1_Device2_LayerNorm2
	Layer1_Device2_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device2_LayerNorm2 -> Layer1_Device2_GateProj
	Layer1_Device2_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device2_LayerNorm2 -> Layer1_Device2_UpProj
	Layer1_Device2_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device2_GateProj -> Layer1_Device2_Activation
	Layer1_Device2_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device2_Activation -> Layer1_Device2_ElemMul
	Layer1_Device2_UpProj -> Layer1_Device2_ElemMul
	Layer1_Device2_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device2_ElemMul -> Layer1_Device2_DownProj
	Layer1_Device2_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device2_DownProj -> Layer1_Device2_Residual2
	Layer1_Device2_Residual1 -> Layer1_Device2_Residual2
	Layer1_Device2_Output [label="Layer 1 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device2_Residual2 -> Layer1_Device2_Output
	Layer1_Device3_Input [label="Layer 1 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device3_Output -> Layer1_Device3_Input
	Layer1_Device3_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device3_Input -> Layer1_Device3_LayerNorm1
	Layer1_Device3_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device3_LayerNorm1 -> Layer1_Device3_QKVProj
	Layer1_Device3_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device3_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage0_RecvKV -> Layer1_Device3_Stage0_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage0_Attention [label=Q_local]
	Layer1_Device3_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage0_Attention -> Layer1_Device3_Stage0_Accumulate
	Layer1_Device3_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage0_RecvKV -> Layer1_Device3_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage1_RecvKV -> Layer1_Device3_Stage1_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage1_Attention [label=Q_local]
	Layer1_Device3_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage1_Attention -> Layer1_Device3_Stage1_Accumulate
	Layer1_Device3_Stage0_Accumulate -> Layer1_Device3_Stage1_Accumulate
	Layer1_Device3_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage1_RecvKV -> Layer1_Device3_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage2_RecvKV -> Layer1_Device3_Stage2_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage2_Attention [label=Q_local]
	Layer1_Device3_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage2_Attention -> Layer1_Device3_Stage2_Accumulate
	Layer1_Device3_Stage1_Accumulate -> Layer1_Device3_Stage2_Accumulate
	Layer1_Device3_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage2_RecvKV -> Layer1_Device3_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage3_RecvKV -> Layer1_Device3_Stage3_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage3_Attention [label=Q_local]
	Layer1_Device3_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage3_Attention -> Layer1_Device3_Stage3_Accumulate
	Layer1_Device3_Stage2_Accumulate -> Layer1_Device3_Stage3_Accumulate
	Layer1_Device3_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage3_RecvKV -> Layer1_Device3_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage4_RecvKV -> Layer1_Device3_Stage4_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage4_Attention [label=Q_local]
	Layer1_Device3_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage4_Attention -> Layer1_Device3_Stage4_Accumulate
	Layer1_Device3_Stage3_Accumulate -> Layer1_Device3_Stage4_Accumulate
	Layer1_Device3_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage4_RecvKV -> Layer1_Device3_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage5_RecvKV -> Layer1_Device3_Stage5_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage5_Attention [label=Q_local]
	Layer1_Device3_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage5_Attention -> Layer1_Device3_Stage5_Accumulate
	Layer1_Device3_Stage4_Accumulate -> Layer1_Device3_Stage5_Accumulate
	Layer1_Device3_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage5_RecvKV -> Layer1_Device3_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage6_RecvKV -> Layer1_Device3_Stage6_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage6_Attention [label=Q_local]
	Layer1_Device3_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage6_Attention -> Layer1_Device3_Stage6_Accumulate
	Layer1_Device3_Stage5_Accumulate -> Layer1_Device3_Stage6_Accumulate
	Layer1_Device3_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage6_RecvKV -> Layer1_Device3_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage7_RecvKV -> Layer1_Device3_Stage7_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage7_Attention [label=Q_local]
	Layer1_Device3_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage7_Attention -> Layer1_Device3_Stage7_Accumulate
	Layer1_Device3_Stage6_Accumulate -> Layer1_Device3_Stage7_Accumulate
	Layer1_Device3_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage7_RecvKV -> Layer1_Device3_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage8_RecvKV -> Layer1_Device3_Stage8_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage8_Attention [label=Q_local]
	Layer1_Device3_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage8_Attention -> Layer1_Device3_Stage8_Accumulate
	Layer1_Device3_Stage7_Accumulate -> Layer1_Device3_Stage8_Accumulate
	Layer1_Device3_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage8_RecvKV -> Layer1_Device3_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage9_RecvKV -> Layer1_Device3_Stage9_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage9_Attention [label=Q_local]
	Layer1_Device3_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage9_Attention -> Layer1_Device3_Stage9_Accumulate
	Layer1_Device3_Stage8_Accumulate -> Layer1_Device3_Stage9_Accumulate
	Layer1_Device3_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage9_RecvKV -> Layer1_Device3_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage10_RecvKV -> Layer1_Device3_Stage10_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage10_Attention [label=Q_local]
	Layer1_Device3_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage10_Attention -> Layer1_Device3_Stage10_Accumulate
	Layer1_Device3_Stage9_Accumulate -> Layer1_Device3_Stage10_Accumulate
	Layer1_Device3_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage10_RecvKV -> Layer1_Device3_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage11_RecvKV -> Layer1_Device3_Stage11_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage11_Attention [label=Q_local]
	Layer1_Device3_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage11_Attention -> Layer1_Device3_Stage11_Accumulate
	Layer1_Device3_Stage10_Accumulate -> Layer1_Device3_Stage11_Accumulate
	Layer1_Device3_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage11_RecvKV -> Layer1_Device3_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage12_RecvKV -> Layer1_Device3_Stage12_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage12_Attention [label=Q_local]
	Layer1_Device3_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage12_Attention -> Layer1_Device3_Stage12_Accumulate
	Layer1_Device3_Stage11_Accumulate -> Layer1_Device3_Stage12_Accumulate
	Layer1_Device3_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage12_RecvKV -> Layer1_Device3_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage13_RecvKV -> Layer1_Device3_Stage13_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage13_Attention [label=Q_local]
	Layer1_Device3_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage13_Attention -> Layer1_Device3_Stage13_Accumulate
	Layer1_Device3_Stage12_Accumulate -> Layer1_Device3_Stage13_Accumulate
	Layer1_Device3_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage13_RecvKV -> Layer1_Device3_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage14_RecvKV -> Layer1_Device3_Stage14_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage14_Attention [label=Q_local]
	Layer1_Device3_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage14_Attention -> Layer1_Device3_Stage14_Accumulate
	Layer1_Device3_Stage13_Accumulate -> Layer1_Device3_Stage14_Accumulate
	Layer1_Device3_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device3_Stage14_RecvKV -> Layer1_Device3_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device3_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device3_Stage15_RecvKV -> Layer1_Device3_Stage15_Attention
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage15_Attention [label=Q_local]
	Layer1_Device3_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device3_Stage15_Attention -> Layer1_Device3_Stage15_Accumulate
	Layer1_Device3_Stage14_Accumulate -> Layer1_Device3_Stage15_Accumulate
	Layer1_Device3_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device3_Stage15_Accumulate -> Layer1_Device3_ConcatHeads
	Layer1_Device3_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device3_ConcatHeads -> Layer1_Device3_OutputProj
	Layer1_Device3_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device3_OutputProj -> Layer1_Device3_Residual1
	Layer1_Device3_Input -> Layer1_Device3_Residual1
	Layer1_Device3_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device3_Residual1 -> Layer1_Device3_LayerNorm2
	Layer1_Device3_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device3_LayerNorm2 -> Layer1_Device3_GateProj
	Layer1_Device3_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device3_LayerNorm2 -> Layer1_Device3_UpProj
	Layer1_Device3_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device3_GateProj -> Layer1_Device3_Activation
	Layer1_Device3_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device3_Activation -> Layer1_Device3_ElemMul
	Layer1_Device3_UpProj -> Layer1_Device3_ElemMul
	Layer1_Device3_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device3_ElemMul -> Layer1_Device3_DownProj
	Layer1_Device3_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device3_DownProj -> Layer1_Device3_Residual2
	Layer1_Device3_Residual1 -> Layer1_Device3_Residual2
	Layer1_Device3_Output [label="Layer 1 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device3_Residual2 -> Layer1_Device3_Output
	Layer1_Device4_Input [label="Layer 1 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device4_Output -> Layer1_Device4_Input
	Layer1_Device4_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device4_Input -> Layer1_Device4_LayerNorm1
	Layer1_Device4_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device4_LayerNorm1 -> Layer1_Device4_QKVProj
	Layer1_Device4_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device4_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage0_RecvKV -> Layer1_Device4_Stage0_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage0_Attention [label=Q_local]
	Layer1_Device4_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage0_Attention -> Layer1_Device4_Stage0_Accumulate
	Layer1_Device4_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage0_RecvKV -> Layer1_Device4_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage1_RecvKV -> Layer1_Device4_Stage1_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage1_Attention [label=Q_local]
	Layer1_Device4_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage1_Attention -> Layer1_Device4_Stage1_Accumulate
	Layer1_Device4_Stage0_Accumulate -> Layer1_Device4_Stage1_Accumulate
	Layer1_Device4_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage1_RecvKV -> Layer1_Device4_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage2_RecvKV -> Layer1_Device4_Stage2_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage2_Attention [label=Q_local]
	Layer1_Device4_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage2_Attention -> Layer1_Device4_Stage2_Accumulate
	Layer1_Device4_Stage1_Accumulate -> Layer1_Device4_Stage2_Accumulate
	Layer1_Device4_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage2_RecvKV -> Layer1_Device4_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage3_RecvKV -> Layer1_Device4_Stage3_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage3_Attention [label=Q_local]
	Layer1_Device4_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage3_Attention -> Layer1_Device4_Stage3_Accumulate
	Layer1_Device4_Stage2_Accumulate -> Layer1_Device4_Stage3_Accumulate
	Layer1_Device4_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage3_RecvKV -> Layer1_Device4_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage4_RecvKV -> Layer1_Device4_Stage4_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage4_Attention [label=Q_local]
	Layer1_Device4_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage4_Attention -> Layer1_Device4_Stage4_Accumulate
	Layer1_Device4_Stage3_Accumulate -> Layer1_Device4_Stage4_Accumulate
	Layer1_Device4_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage4_RecvKV -> Layer1_Device4_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage5_RecvKV -> Layer1_Device4_Stage5_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage5_Attention [label=Q_local]
	Layer1_Device4_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage5_Attention -> Layer1_Device4_Stage5_Accumulate
	Layer1_Device4_Stage4_Accumulate -> Layer1_Device4_Stage5_Accumulate
	Layer1_Device4_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage5_RecvKV -> Layer1_Device4_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage6_RecvKV -> Layer1_Device4_Stage6_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage6_Attention [label=Q_local]
	Layer1_Device4_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage6_Attention -> Layer1_Device4_Stage6_Accumulate
	Layer1_Device4_Stage5_Accumulate -> Layer1_Device4_Stage6_Accumulate
	Layer1_Device4_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage6_RecvKV -> Layer1_Device4_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage7_RecvKV -> Layer1_Device4_Stage7_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage7_Attention [label=Q_local]
	Layer1_Device4_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage7_Attention -> Layer1_Device4_Stage7_Accumulate
	Layer1_Device4_Stage6_Accumulate -> Layer1_Device4_Stage7_Accumulate
	Layer1_Device4_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage7_RecvKV -> Layer1_Device4_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage8_RecvKV -> Layer1_Device4_Stage8_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage8_Attention [label=Q_local]
	Layer1_Device4_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage8_Attention -> Layer1_Device4_Stage8_Accumulate
	Layer1_Device4_Stage7_Accumulate -> Layer1_Device4_Stage8_Accumulate
	Layer1_Device4_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage8_RecvKV -> Layer1_Device4_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage9_RecvKV -> Layer1_Device4_Stage9_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage9_Attention [label=Q_local]
	Layer1_Device4_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage9_Attention -> Layer1_Device4_Stage9_Accumulate
	Layer1_Device4_Stage8_Accumulate -> Layer1_Device4_Stage9_Accumulate
	Layer1_Device4_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage9_RecvKV -> Layer1_Device4_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage10_RecvKV -> Layer1_Device4_Stage10_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage10_Attention [label=Q_local]
	Layer1_Device4_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage10_Attention -> Layer1_Device4_Stage10_Accumulate
	Layer1_Device4_Stage9_Accumulate -> Layer1_Device4_Stage10_Accumulate
	Layer1_Device4_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage10_RecvKV -> Layer1_Device4_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage11_RecvKV -> Layer1_Device4_Stage11_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage11_Attention [label=Q_local]
	Layer1_Device4_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage11_Attention -> Layer1_Device4_Stage11_Accumulate
	Layer1_Device4_Stage10_Accumulate -> Layer1_Device4_Stage11_Accumulate
	Layer1_Device4_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage11_RecvKV -> Layer1_Device4_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage12_RecvKV -> Layer1_Device4_Stage12_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage12_Attention [label=Q_local]
	Layer1_Device4_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage12_Attention -> Layer1_Device4_Stage12_Accumulate
	Layer1_Device4_Stage11_Accumulate -> Layer1_Device4_Stage12_Accumulate
	Layer1_Device4_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage12_RecvKV -> Layer1_Device4_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage13_RecvKV -> Layer1_Device4_Stage13_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage13_Attention [label=Q_local]
	Layer1_Device4_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage13_Attention -> Layer1_Device4_Stage13_Accumulate
	Layer1_Device4_Stage12_Accumulate -> Layer1_Device4_Stage13_Accumulate
	Layer1_Device4_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage13_RecvKV -> Layer1_Device4_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage14_RecvKV -> Layer1_Device4_Stage14_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage14_Attention [label=Q_local]
	Layer1_Device4_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage14_Attention -> Layer1_Device4_Stage14_Accumulate
	Layer1_Device4_Stage13_Accumulate -> Layer1_Device4_Stage14_Accumulate
	Layer1_Device4_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device4_Stage14_RecvKV -> Layer1_Device4_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device4_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device4_Stage15_RecvKV -> Layer1_Device4_Stage15_Attention
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage15_Attention [label=Q_local]
	Layer1_Device4_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device4_Stage15_Attention -> Layer1_Device4_Stage15_Accumulate
	Layer1_Device4_Stage14_Accumulate -> Layer1_Device4_Stage15_Accumulate
	Layer1_Device4_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device4_Stage15_Accumulate -> Layer1_Device4_ConcatHeads
	Layer1_Device4_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device4_ConcatHeads -> Layer1_Device4_OutputProj
	Layer1_Device4_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device4_OutputProj -> Layer1_Device4_Residual1
	Layer1_Device4_Input -> Layer1_Device4_Residual1
	Layer1_Device4_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device4_Residual1 -> Layer1_Device4_LayerNorm2
	Layer1_Device4_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device4_LayerNorm2 -> Layer1_Device4_GateProj
	Layer1_Device4_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device4_LayerNorm2 -> Layer1_Device4_UpProj
	Layer1_Device4_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device4_GateProj -> Layer1_Device4_Activation
	Layer1_Device4_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device4_Activation -> Layer1_Device4_ElemMul
	Layer1_Device4_UpProj -> Layer1_Device4_ElemMul
	Layer1_Device4_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device4_ElemMul -> Layer1_Device4_DownProj
	Layer1_Device4_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device4_DownProj -> Layer1_Device4_Residual2
	Layer1_Device4_Residual1 -> Layer1_Device4_Residual2
	Layer1_Device4_Output [label="Layer 1 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device4_Residual2 -> Layer1_Device4_Output
	Layer1_Device5_Input [label="Layer 1 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device5_Output -> Layer1_Device5_Input
	Layer1_Device5_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device5_Input -> Layer1_Device5_LayerNorm1
	Layer1_Device5_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device5_LayerNorm1 -> Layer1_Device5_QKVProj
	Layer1_Device5_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device5_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage0_RecvKV -> Layer1_Device5_Stage0_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage0_Attention [label=Q_local]
	Layer1_Device5_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage0_Attention -> Layer1_Device5_Stage0_Accumulate
	Layer1_Device5_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage0_RecvKV -> Layer1_Device5_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage1_RecvKV -> Layer1_Device5_Stage1_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage1_Attention [label=Q_local]
	Layer1_Device5_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage1_Attention -> Layer1_Device5_Stage1_Accumulate
	Layer1_Device5_Stage0_Accumulate -> Layer1_Device5_Stage1_Accumulate
	Layer1_Device5_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage1_RecvKV -> Layer1_Device5_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage2_RecvKV -> Layer1_Device5_Stage2_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage2_Attention [label=Q_local]
	Layer1_Device5_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage2_Attention -> Layer1_Device5_Stage2_Accumulate
	Layer1_Device5_Stage1_Accumulate -> Layer1_Device5_Stage2_Accumulate
	Layer1_Device5_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage2_RecvKV -> Layer1_Device5_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage3_RecvKV -> Layer1_Device5_Stage3_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage3_Attention [label=Q_local]
	Layer1_Device5_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage3_Attention -> Layer1_Device5_Stage3_Accumulate
	Layer1_Device5_Stage2_Accumulate -> Layer1_Device5_Stage3_Accumulate
	Layer1_Device5_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage3_RecvKV -> Layer1_Device5_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage4_RecvKV -> Layer1_Device5_Stage4_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage4_Attention [label=Q_local]
	Layer1_Device5_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage4_Attention -> Layer1_Device5_Stage4_Accumulate
	Layer1_Device5_Stage3_Accumulate -> Layer1_Device5_Stage4_Accumulate
	Layer1_Device5_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage4_RecvKV -> Layer1_Device5_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage5_RecvKV -> Layer1_Device5_Stage5_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage5_Attention [label=Q_local]
	Layer1_Device5_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage5_Attention -> Layer1_Device5_Stage5_Accumulate
	Layer1_Device5_Stage4_Accumulate -> Layer1_Device5_Stage5_Accumulate
	Layer1_Device5_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage5_RecvKV -> Layer1_Device5_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage6_RecvKV -> Layer1_Device5_Stage6_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage6_Attention [label=Q_local]
	Layer1_Device5_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage6_Attention -> Layer1_Device5_Stage6_Accumulate
	Layer1_Device5_Stage5_Accumulate -> Layer1_Device5_Stage6_Accumulate
	Layer1_Device5_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage6_RecvKV -> Layer1_Device5_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage7_RecvKV -> Layer1_Device5_Stage7_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage7_Attention [label=Q_local]
	Layer1_Device5_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage7_Attention -> Layer1_Device5_Stage7_Accumulate
	Layer1_Device5_Stage6_Accumulate -> Layer1_Device5_Stage7_Accumulate
	Layer1_Device5_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage7_RecvKV -> Layer1_Device5_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage8_RecvKV -> Layer1_Device5_Stage8_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage8_Attention [label=Q_local]
	Layer1_Device5_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage8_Attention -> Layer1_Device5_Stage8_Accumulate
	Layer1_Device5_Stage7_Accumulate -> Layer1_Device5_Stage8_Accumulate
	Layer1_Device5_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage8_RecvKV -> Layer1_Device5_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage9_RecvKV -> Layer1_Device5_Stage9_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage9_Attention [label=Q_local]
	Layer1_Device5_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage9_Attention -> Layer1_Device5_Stage9_Accumulate
	Layer1_Device5_Stage8_Accumulate -> Layer1_Device5_Stage9_Accumulate
	Layer1_Device5_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage9_RecvKV -> Layer1_Device5_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage10_RecvKV -> Layer1_Device5_Stage10_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage10_Attention [label=Q_local]
	Layer1_Device5_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage10_Attention -> Layer1_Device5_Stage10_Accumulate
	Layer1_Device5_Stage9_Accumulate -> Layer1_Device5_Stage10_Accumulate
	Layer1_Device5_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage10_RecvKV -> Layer1_Device5_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage11_RecvKV -> Layer1_Device5_Stage11_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage11_Attention [label=Q_local]
	Layer1_Device5_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage11_Attention -> Layer1_Device5_Stage11_Accumulate
	Layer1_Device5_Stage10_Accumulate -> Layer1_Device5_Stage11_Accumulate
	Layer1_Device5_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage11_RecvKV -> Layer1_Device5_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage12_RecvKV -> Layer1_Device5_Stage12_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage12_Attention [label=Q_local]
	Layer1_Device5_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage12_Attention -> Layer1_Device5_Stage12_Accumulate
	Layer1_Device5_Stage11_Accumulate -> Layer1_Device5_Stage12_Accumulate
	Layer1_Device5_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage12_RecvKV -> Layer1_Device5_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage13_RecvKV -> Layer1_Device5_Stage13_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage13_Attention [label=Q_local]
	Layer1_Device5_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage13_Attention -> Layer1_Device5_Stage13_Accumulate
	Layer1_Device5_Stage12_Accumulate -> Layer1_Device5_Stage13_Accumulate
	Layer1_Device5_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage13_RecvKV -> Layer1_Device5_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage14_RecvKV -> Layer1_Device5_Stage14_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage14_Attention [label=Q_local]
	Layer1_Device5_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage14_Attention -> Layer1_Device5_Stage14_Accumulate
	Layer1_Device5_Stage13_Accumulate -> Layer1_Device5_Stage14_Accumulate
	Layer1_Device5_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device5_Stage14_RecvKV -> Layer1_Device5_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device5_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device5_Stage15_RecvKV -> Layer1_Device5_Stage15_Attention
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage15_Attention [label=Q_local]
	Layer1_Device5_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device5_Stage15_Attention -> Layer1_Device5_Stage15_Accumulate
	Layer1_Device5_Stage14_Accumulate -> Layer1_Device5_Stage15_Accumulate
	Layer1_Device5_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device5_Stage15_Accumulate -> Layer1_Device5_ConcatHeads
	Layer1_Device5_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device5_ConcatHeads -> Layer1_Device5_OutputProj
	Layer1_Device5_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device5_OutputProj -> Layer1_Device5_Residual1
	Layer1_Device5_Input -> Layer1_Device5_Residual1
	Layer1_Device5_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device5_Residual1 -> Layer1_Device5_LayerNorm2
	Layer1_Device5_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device5_LayerNorm2 -> Layer1_Device5_GateProj
	Layer1_Device5_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device5_LayerNorm2 -> Layer1_Device5_UpProj
	Layer1_Device5_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device5_GateProj -> Layer1_Device5_Activation
	Layer1_Device5_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device5_Activation -> Layer1_Device5_ElemMul
	Layer1_Device5_UpProj -> Layer1_Device5_ElemMul
	Layer1_Device5_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device5_ElemMul -> Layer1_Device5_DownProj
	Layer1_Device5_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device5_DownProj -> Layer1_Device5_Residual2
	Layer1_Device5_Residual1 -> Layer1_Device5_Residual2
	Layer1_Device5_Output [label="Layer 1 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device5_Residual2 -> Layer1_Device5_Output
	Layer1_Device6_Input [label="Layer 1 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device6_Output -> Layer1_Device6_Input
	Layer1_Device6_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device6_Input -> Layer1_Device6_LayerNorm1
	Layer1_Device6_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device6_LayerNorm1 -> Layer1_Device6_QKVProj
	Layer1_Device6_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device6_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage0_RecvKV -> Layer1_Device6_Stage0_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage0_Attention [label=Q_local]
	Layer1_Device6_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage0_Attention -> Layer1_Device6_Stage0_Accumulate
	Layer1_Device6_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage0_RecvKV -> Layer1_Device6_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage1_RecvKV -> Layer1_Device6_Stage1_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage1_Attention [label=Q_local]
	Layer1_Device6_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage1_Attention -> Layer1_Device6_Stage1_Accumulate
	Layer1_Device6_Stage0_Accumulate -> Layer1_Device6_Stage1_Accumulate
	Layer1_Device6_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage1_RecvKV -> Layer1_Device6_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage2_RecvKV -> Layer1_Device6_Stage2_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage2_Attention [label=Q_local]
	Layer1_Device6_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage2_Attention -> Layer1_Device6_Stage2_Accumulate
	Layer1_Device6_Stage1_Accumulate -> Layer1_Device6_Stage2_Accumulate
	Layer1_Device6_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage2_RecvKV -> Layer1_Device6_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage3_RecvKV -> Layer1_Device6_Stage3_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage3_Attention [label=Q_local]
	Layer1_Device6_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage3_Attention -> Layer1_Device6_Stage3_Accumulate
	Layer1_Device6_Stage2_Accumulate -> Layer1_Device6_Stage3_Accumulate
	Layer1_Device6_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage3_RecvKV -> Layer1_Device6_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage4_RecvKV -> Layer1_Device6_Stage4_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage4_Attention [label=Q_local]
	Layer1_Device6_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage4_Attention -> Layer1_Device6_Stage4_Accumulate
	Layer1_Device6_Stage3_Accumulate -> Layer1_Device6_Stage4_Accumulate
	Layer1_Device6_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage4_RecvKV -> Layer1_Device6_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage5_RecvKV -> Layer1_Device6_Stage5_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage5_Attention [label=Q_local]
	Layer1_Device6_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage5_Attention -> Layer1_Device6_Stage5_Accumulate
	Layer1_Device6_Stage4_Accumulate -> Layer1_Device6_Stage5_Accumulate
	Layer1_Device6_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage5_RecvKV -> Layer1_Device6_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage6_RecvKV -> Layer1_Device6_Stage6_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage6_Attention [label=Q_local]
	Layer1_Device6_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage6_Attention -> Layer1_Device6_Stage6_Accumulate
	Layer1_Device6_Stage5_Accumulate -> Layer1_Device6_Stage6_Accumulate
	Layer1_Device6_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage6_RecvKV -> Layer1_Device6_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage7_RecvKV -> Layer1_Device6_Stage7_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage7_Attention [label=Q_local]
	Layer1_Device6_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage7_Attention -> Layer1_Device6_Stage7_Accumulate
	Layer1_Device6_Stage6_Accumulate -> Layer1_Device6_Stage7_Accumulate
	Layer1_Device6_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage7_RecvKV -> Layer1_Device6_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage8_RecvKV -> Layer1_Device6_Stage8_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage8_Attention [label=Q_local]
	Layer1_Device6_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage8_Attention -> Layer1_Device6_Stage8_Accumulate
	Layer1_Device6_Stage7_Accumulate -> Layer1_Device6_Stage8_Accumulate
	Layer1_Device6_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage8_RecvKV -> Layer1_Device6_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage9_RecvKV -> Layer1_Device6_Stage9_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage9_Attention [label=Q_local]
	Layer1_Device6_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage9_Attention -> Layer1_Device6_Stage9_Accumulate
	Layer1_Device6_Stage8_Accumulate -> Layer1_Device6_Stage9_Accumulate
	Layer1_Device6_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage9_RecvKV -> Layer1_Device6_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage10_RecvKV -> Layer1_Device6_Stage10_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage10_Attention [label=Q_local]
	Layer1_Device6_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage10_Attention -> Layer1_Device6_Stage10_Accumulate
	Layer1_Device6_Stage9_Accumulate -> Layer1_Device6_Stage10_Accumulate
	Layer1_Device6_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage10_RecvKV -> Layer1_Device6_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage11_RecvKV -> Layer1_Device6_Stage11_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage11_Attention [label=Q_local]
	Layer1_Device6_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage11_Attention -> Layer1_Device6_Stage11_Accumulate
	Layer1_Device6_Stage10_Accumulate -> Layer1_Device6_Stage11_Accumulate
	Layer1_Device6_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage11_RecvKV -> Layer1_Device6_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage12_RecvKV -> Layer1_Device6_Stage12_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage12_Attention [label=Q_local]
	Layer1_Device6_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage12_Attention -> Layer1_Device6_Stage12_Accumulate
	Layer1_Device6_Stage11_Accumulate -> Layer1_Device6_Stage12_Accumulate
	Layer1_Device6_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage12_RecvKV -> Layer1_Device6_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage13_RecvKV -> Layer1_Device6_Stage13_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage13_Attention [label=Q_local]
	Layer1_Device6_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage13_Attention -> Layer1_Device6_Stage13_Accumulate
	Layer1_Device6_Stage12_Accumulate -> Layer1_Device6_Stage13_Accumulate
	Layer1_Device6_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage13_RecvKV -> Layer1_Device6_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage14_RecvKV -> Layer1_Device6_Stage14_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage14_Attention [label=Q_local]
	Layer1_Device6_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage14_Attention -> Layer1_Device6_Stage14_Accumulate
	Layer1_Device6_Stage13_Accumulate -> Layer1_Device6_Stage14_Accumulate
	Layer1_Device6_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device6_Stage14_RecvKV -> Layer1_Device6_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device6_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device6_Stage15_RecvKV -> Layer1_Device6_Stage15_Attention
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage15_Attention [label=Q_local]
	Layer1_Device6_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device6_Stage15_Attention -> Layer1_Device6_Stage15_Accumulate
	Layer1_Device6_Stage14_Accumulate -> Layer1_Device6_Stage15_Accumulate
	Layer1_Device6_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device6_Stage15_Accumulate -> Layer1_Device6_ConcatHeads
	Layer1_Device6_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device6_ConcatHeads -> Layer1_Device6_OutputProj
	Layer1_Device6_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device6_OutputProj -> Layer1_Device6_Residual1
	Layer1_Device6_Input -> Layer1_Device6_Residual1
	Layer1_Device6_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device6_Residual1 -> Layer1_Device6_LayerNorm2
	Layer1_Device6_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device6_LayerNorm2 -> Layer1_Device6_GateProj
	Layer1_Device6_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device6_LayerNorm2 -> Layer1_Device6_UpProj
	Layer1_Device6_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device6_GateProj -> Layer1_Device6_Activation
	Layer1_Device6_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device6_Activation -> Layer1_Device6_ElemMul
	Layer1_Device6_UpProj -> Layer1_Device6_ElemMul
	Layer1_Device6_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device6_ElemMul -> Layer1_Device6_DownProj
	Layer1_Device6_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device6_DownProj -> Layer1_Device6_Residual2
	Layer1_Device6_Residual1 -> Layer1_Device6_Residual2
	Layer1_Device6_Output [label="Layer 1 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device6_Residual2 -> Layer1_Device6_Output
	Layer1_Device7_Input [label="Layer 1 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device7_Output -> Layer1_Device7_Input
	Layer1_Device7_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device7_Input -> Layer1_Device7_LayerNorm1
	Layer1_Device7_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device7_LayerNorm1 -> Layer1_Device7_QKVProj
	Layer1_Device7_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device7_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage0_RecvKV -> Layer1_Device7_Stage0_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage0_Attention [label=Q_local]
	Layer1_Device7_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage0_Attention -> Layer1_Device7_Stage0_Accumulate
	Layer1_Device7_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage0_RecvKV -> Layer1_Device7_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage1_RecvKV -> Layer1_Device7_Stage1_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage1_Attention [label=Q_local]
	Layer1_Device7_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage1_Attention -> Layer1_Device7_Stage1_Accumulate
	Layer1_Device7_Stage0_Accumulate -> Layer1_Device7_Stage1_Accumulate
	Layer1_Device7_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage1_RecvKV -> Layer1_Device7_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage2_RecvKV -> Layer1_Device7_Stage2_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage2_Attention [label=Q_local]
	Layer1_Device7_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage2_Attention -> Layer1_Device7_Stage2_Accumulate
	Layer1_Device7_Stage1_Accumulate -> Layer1_Device7_Stage2_Accumulate
	Layer1_Device7_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage2_RecvKV -> Layer1_Device7_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage3_RecvKV -> Layer1_Device7_Stage3_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage3_Attention [label=Q_local]
	Layer1_Device7_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage3_Attention -> Layer1_Device7_Stage3_Accumulate
	Layer1_Device7_Stage2_Accumulate -> Layer1_Device7_Stage3_Accumulate
	Layer1_Device7_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage3_RecvKV -> Layer1_Device7_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage4_RecvKV -> Layer1_Device7_Stage4_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage4_Attention [label=Q_local]
	Layer1_Device7_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage4_Attention -> Layer1_Device7_Stage4_Accumulate
	Layer1_Device7_Stage3_Accumulate -> Layer1_Device7_Stage4_Accumulate
	Layer1_Device7_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage4_RecvKV -> Layer1_Device7_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage5_RecvKV -> Layer1_Device7_Stage5_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage5_Attention [label=Q_local]
	Layer1_Device7_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage5_Attention -> Layer1_Device7_Stage5_Accumulate
	Layer1_Device7_Stage4_Accumulate -> Layer1_Device7_Stage5_Accumulate
	Layer1_Device7_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage5_RecvKV -> Layer1_Device7_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage6_RecvKV -> Layer1_Device7_Stage6_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage6_Attention [label=Q_local]
	Layer1_Device7_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage6_Attention -> Layer1_Device7_Stage6_Accumulate
	Layer1_Device7_Stage5_Accumulate -> Layer1_Device7_Stage6_Accumulate
	Layer1_Device7_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage6_RecvKV -> Layer1_Device7_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage7_RecvKV -> Layer1_Device7_Stage7_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage7_Attention [label=Q_local]
	Layer1_Device7_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage7_Attention -> Layer1_Device7_Stage7_Accumulate
	Layer1_Device7_Stage6_Accumulate -> Layer1_Device7_Stage7_Accumulate
	Layer1_Device7_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage7_RecvKV -> Layer1_Device7_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage8_RecvKV -> Layer1_Device7_Stage8_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage8_Attention [label=Q_local]
	Layer1_Device7_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage8_Attention -> Layer1_Device7_Stage8_Accumulate
	Layer1_Device7_Stage7_Accumulate -> Layer1_Device7_Stage8_Accumulate
	Layer1_Device7_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage8_RecvKV -> Layer1_Device7_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage9_RecvKV -> Layer1_Device7_Stage9_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage9_Attention [label=Q_local]
	Layer1_Device7_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage9_Attention -> Layer1_Device7_Stage9_Accumulate
	Layer1_Device7_Stage8_Accumulate -> Layer1_Device7_Stage9_Accumulate
	Layer1_Device7_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage9_RecvKV -> Layer1_Device7_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage10_RecvKV -> Layer1_Device7_Stage10_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage10_Attention [label=Q_local]
	Layer1_Device7_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage10_Attention -> Layer1_Device7_Stage10_Accumulate
	Layer1_Device7_Stage9_Accumulate -> Layer1_Device7_Stage10_Accumulate
	Layer1_Device7_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage10_RecvKV -> Layer1_Device7_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage11_RecvKV -> Layer1_Device7_Stage11_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage11_Attention [label=Q_local]
	Layer1_Device7_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage11_Attention -> Layer1_Device7_Stage11_Accumulate
	Layer1_Device7_Stage10_Accumulate -> Layer1_Device7_Stage11_Accumulate
	Layer1_Device7_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage11_RecvKV -> Layer1_Device7_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage12_RecvKV -> Layer1_Device7_Stage12_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage12_Attention [label=Q_local]
	Layer1_Device7_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage12_Attention -> Layer1_Device7_Stage12_Accumulate
	Layer1_Device7_Stage11_Accumulate -> Layer1_Device7_Stage12_Accumulate
	Layer1_Device7_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage12_RecvKV -> Layer1_Device7_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage13_RecvKV -> Layer1_Device7_Stage13_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage13_Attention [label=Q_local]
	Layer1_Device7_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage13_Attention -> Layer1_Device7_Stage13_Accumulate
	Layer1_Device7_Stage12_Accumulate -> Layer1_Device7_Stage13_Accumulate
	Layer1_Device7_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage13_RecvKV -> Layer1_Device7_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage14_RecvKV -> Layer1_Device7_Stage14_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage14_Attention [label=Q_local]
	Layer1_Device7_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage14_Attention -> Layer1_Device7_Stage14_Accumulate
	Layer1_Device7_Stage13_Accumulate -> Layer1_Device7_Stage14_Accumulate
	Layer1_Device7_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device7_Stage14_RecvKV -> Layer1_Device7_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device7_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device7_Stage15_RecvKV -> Layer1_Device7_Stage15_Attention
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage15_Attention [label=Q_local]
	Layer1_Device7_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device7_Stage15_Attention -> Layer1_Device7_Stage15_Accumulate
	Layer1_Device7_Stage14_Accumulate -> Layer1_Device7_Stage15_Accumulate
	Layer1_Device7_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device7_Stage15_Accumulate -> Layer1_Device7_ConcatHeads
	Layer1_Device7_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device7_ConcatHeads -> Layer1_Device7_OutputProj
	Layer1_Device7_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device7_OutputProj -> Layer1_Device7_Residual1
	Layer1_Device7_Input -> Layer1_Device7_Residual1
	Layer1_Device7_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device7_Residual1 -> Layer1_Device7_LayerNorm2
	Layer1_Device7_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device7_LayerNorm2 -> Layer1_Device7_GateProj
	Layer1_Device7_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device7_LayerNorm2 -> Layer1_Device7_UpProj
	Layer1_Device7_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device7_GateProj -> Layer1_Device7_Activation
	Layer1_Device7_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device7_Activation -> Layer1_Device7_ElemMul
	Layer1_Device7_UpProj -> Layer1_Device7_ElemMul
	Layer1_Device7_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device7_ElemMul -> Layer1_Device7_DownProj
	Layer1_Device7_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device7_DownProj -> Layer1_Device7_Residual2
	Layer1_Device7_Residual1 -> Layer1_Device7_Residual2
	Layer1_Device7_Output [label="Layer 1 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device7_Residual2 -> Layer1_Device7_Output
	Layer1_Device8_Input [label="Layer 1 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device8_Output -> Layer1_Device8_Input
	Layer1_Device8_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device8_Input -> Layer1_Device8_LayerNorm1
	Layer1_Device8_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device8_LayerNorm1 -> Layer1_Device8_QKVProj
	Layer1_Device8_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device8_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage0_RecvKV -> Layer1_Device8_Stage0_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage0_Attention [label=Q_local]
	Layer1_Device8_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage0_Attention -> Layer1_Device8_Stage0_Accumulate
	Layer1_Device8_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage0_RecvKV -> Layer1_Device8_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage1_RecvKV -> Layer1_Device8_Stage1_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage1_Attention [label=Q_local]
	Layer1_Device8_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage1_Attention -> Layer1_Device8_Stage1_Accumulate
	Layer1_Device8_Stage0_Accumulate -> Layer1_Device8_Stage1_Accumulate
	Layer1_Device8_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage1_RecvKV -> Layer1_Device8_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage2_RecvKV -> Layer1_Device8_Stage2_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage2_Attention [label=Q_local]
	Layer1_Device8_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage2_Attention -> Layer1_Device8_Stage2_Accumulate
	Layer1_Device8_Stage1_Accumulate -> Layer1_Device8_Stage2_Accumulate
	Layer1_Device8_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage2_RecvKV -> Layer1_Device8_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage3_RecvKV -> Layer1_Device8_Stage3_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage3_Attention [label=Q_local]
	Layer1_Device8_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage3_Attention -> Layer1_Device8_Stage3_Accumulate
	Layer1_Device8_Stage2_Accumulate -> Layer1_Device8_Stage3_Accumulate
	Layer1_Device8_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage3_RecvKV -> Layer1_Device8_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage4_RecvKV -> Layer1_Device8_Stage4_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage4_Attention [label=Q_local]
	Layer1_Device8_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage4_Attention -> Layer1_Device8_Stage4_Accumulate
	Layer1_Device8_Stage3_Accumulate -> Layer1_Device8_Stage4_Accumulate
	Layer1_Device8_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage4_RecvKV -> Layer1_Device8_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage5_RecvKV -> Layer1_Device8_Stage5_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage5_Attention [label=Q_local]
	Layer1_Device8_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage5_Attention -> Layer1_Device8_Stage5_Accumulate
	Layer1_Device8_Stage4_Accumulate -> Layer1_Device8_Stage5_Accumulate
	Layer1_Device8_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage5_RecvKV -> Layer1_Device8_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage6_RecvKV -> Layer1_Device8_Stage6_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage6_Attention [label=Q_local]
	Layer1_Device8_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage6_Attention -> Layer1_Device8_Stage6_Accumulate
	Layer1_Device8_Stage5_Accumulate -> Layer1_Device8_Stage6_Accumulate
	Layer1_Device8_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage6_RecvKV -> Layer1_Device8_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage7_RecvKV -> Layer1_Device8_Stage7_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage7_Attention [label=Q_local]
	Layer1_Device8_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage7_Attention -> Layer1_Device8_Stage7_Accumulate
	Layer1_Device8_Stage6_Accumulate -> Layer1_Device8_Stage7_Accumulate
	Layer1_Device8_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage7_RecvKV -> Layer1_Device8_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage8_RecvKV -> Layer1_Device8_Stage8_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage8_Attention [label=Q_local]
	Layer1_Device8_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage8_Attention -> Layer1_Device8_Stage8_Accumulate
	Layer1_Device8_Stage7_Accumulate -> Layer1_Device8_Stage8_Accumulate
	Layer1_Device8_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage8_RecvKV -> Layer1_Device8_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage9_RecvKV -> Layer1_Device8_Stage9_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage9_Attention [label=Q_local]
	Layer1_Device8_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage9_Attention -> Layer1_Device8_Stage9_Accumulate
	Layer1_Device8_Stage8_Accumulate -> Layer1_Device8_Stage9_Accumulate
	Layer1_Device8_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage9_RecvKV -> Layer1_Device8_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage10_RecvKV -> Layer1_Device8_Stage10_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage10_Attention [label=Q_local]
	Layer1_Device8_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage10_Attention -> Layer1_Device8_Stage10_Accumulate
	Layer1_Device8_Stage9_Accumulate -> Layer1_Device8_Stage10_Accumulate
	Layer1_Device8_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage10_RecvKV -> Layer1_Device8_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage11_RecvKV -> Layer1_Device8_Stage11_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage11_Attention [label=Q_local]
	Layer1_Device8_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage11_Attention -> Layer1_Device8_Stage11_Accumulate
	Layer1_Device8_Stage10_Accumulate -> Layer1_Device8_Stage11_Accumulate
	Layer1_Device8_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage11_RecvKV -> Layer1_Device8_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage12_RecvKV -> Layer1_Device8_Stage12_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage12_Attention [label=Q_local]
	Layer1_Device8_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage12_Attention -> Layer1_Device8_Stage12_Accumulate
	Layer1_Device8_Stage11_Accumulate -> Layer1_Device8_Stage12_Accumulate
	Layer1_Device8_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage12_RecvKV -> Layer1_Device8_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage13_RecvKV -> Layer1_Device8_Stage13_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage13_Attention [label=Q_local]
	Layer1_Device8_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage13_Attention -> Layer1_Device8_Stage13_Accumulate
	Layer1_Device8_Stage12_Accumulate -> Layer1_Device8_Stage13_Accumulate
	Layer1_Device8_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage13_RecvKV -> Layer1_Device8_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage14_RecvKV -> Layer1_Device8_Stage14_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage14_Attention [label=Q_local]
	Layer1_Device8_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage14_Attention -> Layer1_Device8_Stage14_Accumulate
	Layer1_Device8_Stage13_Accumulate -> Layer1_Device8_Stage14_Accumulate
	Layer1_Device8_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device8_Stage14_RecvKV -> Layer1_Device8_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device8_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device8_Stage15_RecvKV -> Layer1_Device8_Stage15_Attention
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage15_Attention [label=Q_local]
	Layer1_Device8_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device8_Stage15_Attention -> Layer1_Device8_Stage15_Accumulate
	Layer1_Device8_Stage14_Accumulate -> Layer1_Device8_Stage15_Accumulate
	Layer1_Device8_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device8_Stage15_Accumulate -> Layer1_Device8_ConcatHeads
	Layer1_Device8_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device8_ConcatHeads -> Layer1_Device8_OutputProj
	Layer1_Device8_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device8_OutputProj -> Layer1_Device8_Residual1
	Layer1_Device8_Input -> Layer1_Device8_Residual1
	Layer1_Device8_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device8_Residual1 -> Layer1_Device8_LayerNorm2
	Layer1_Device8_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device8_LayerNorm2 -> Layer1_Device8_GateProj
	Layer1_Device8_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device8_LayerNorm2 -> Layer1_Device8_UpProj
	Layer1_Device8_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device8_GateProj -> Layer1_Device8_Activation
	Layer1_Device8_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device8_Activation -> Layer1_Device8_ElemMul
	Layer1_Device8_UpProj -> Layer1_Device8_ElemMul
	Layer1_Device8_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device8_ElemMul -> Layer1_Device8_DownProj
	Layer1_Device8_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device8_DownProj -> Layer1_Device8_Residual2
	Layer1_Device8_Residual1 -> Layer1_Device8_Residual2
	Layer1_Device8_Output [label="Layer 1 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device8_Residual2 -> Layer1_Device8_Output
	Layer1_Device9_Input [label="Layer 1 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device9_Output -> Layer1_Device9_Input
	Layer1_Device9_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device9_Input -> Layer1_Device9_LayerNorm1
	Layer1_Device9_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device9_LayerNorm1 -> Layer1_Device9_QKVProj
	Layer1_Device9_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device9_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage0_RecvKV -> Layer1_Device9_Stage0_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage0_Attention [label=Q_local]
	Layer1_Device9_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage0_Attention -> Layer1_Device9_Stage0_Accumulate
	Layer1_Device9_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage0_RecvKV -> Layer1_Device9_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage1_RecvKV -> Layer1_Device9_Stage1_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage1_Attention [label=Q_local]
	Layer1_Device9_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage1_Attention -> Layer1_Device9_Stage1_Accumulate
	Layer1_Device9_Stage0_Accumulate -> Layer1_Device9_Stage1_Accumulate
	Layer1_Device9_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage1_RecvKV -> Layer1_Device9_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage2_RecvKV -> Layer1_Device9_Stage2_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage2_Attention [label=Q_local]
	Layer1_Device9_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage2_Attention -> Layer1_Device9_Stage2_Accumulate
	Layer1_Device9_Stage1_Accumulate -> Layer1_Device9_Stage2_Accumulate
	Layer1_Device9_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage2_RecvKV -> Layer1_Device9_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage3_RecvKV -> Layer1_Device9_Stage3_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage3_Attention [label=Q_local]
	Layer1_Device9_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage3_Attention -> Layer1_Device9_Stage3_Accumulate
	Layer1_Device9_Stage2_Accumulate -> Layer1_Device9_Stage3_Accumulate
	Layer1_Device9_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage3_RecvKV -> Layer1_Device9_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage4_RecvKV -> Layer1_Device9_Stage4_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage4_Attention [label=Q_local]
	Layer1_Device9_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage4_Attention -> Layer1_Device9_Stage4_Accumulate
	Layer1_Device9_Stage3_Accumulate -> Layer1_Device9_Stage4_Accumulate
	Layer1_Device9_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage4_RecvKV -> Layer1_Device9_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage5_RecvKV -> Layer1_Device9_Stage5_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage5_Attention [label=Q_local]
	Layer1_Device9_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage5_Attention -> Layer1_Device9_Stage5_Accumulate
	Layer1_Device9_Stage4_Accumulate -> Layer1_Device9_Stage5_Accumulate
	Layer1_Device9_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage5_RecvKV -> Layer1_Device9_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage6_RecvKV -> Layer1_Device9_Stage6_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage6_Attention [label=Q_local]
	Layer1_Device9_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage6_Attention -> Layer1_Device9_Stage6_Accumulate
	Layer1_Device9_Stage5_Accumulate -> Layer1_Device9_Stage6_Accumulate
	Layer1_Device9_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage6_RecvKV -> Layer1_Device9_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage7_RecvKV -> Layer1_Device9_Stage7_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage7_Attention [label=Q_local]
	Layer1_Device9_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage7_Attention -> Layer1_Device9_Stage7_Accumulate
	Layer1_Device9_Stage6_Accumulate -> Layer1_Device9_Stage7_Accumulate
	Layer1_Device9_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage7_RecvKV -> Layer1_Device9_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage8_RecvKV -> Layer1_Device9_Stage8_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage8_Attention [label=Q_local]
	Layer1_Device9_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage8_Attention -> Layer1_Device9_Stage8_Accumulate
	Layer1_Device9_Stage7_Accumulate -> Layer1_Device9_Stage8_Accumulate
	Layer1_Device9_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage8_RecvKV -> Layer1_Device9_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage9_RecvKV -> Layer1_Device9_Stage9_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage9_Attention [label=Q_local]
	Layer1_Device9_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage9_Attention -> Layer1_Device9_Stage9_Accumulate
	Layer1_Device9_Stage8_Accumulate -> Layer1_Device9_Stage9_Accumulate
	Layer1_Device9_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage9_RecvKV -> Layer1_Device9_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage10_RecvKV -> Layer1_Device9_Stage10_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage10_Attention [label=Q_local]
	Layer1_Device9_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage10_Attention -> Layer1_Device9_Stage10_Accumulate
	Layer1_Device9_Stage9_Accumulate -> Layer1_Device9_Stage10_Accumulate
	Layer1_Device9_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage10_RecvKV -> Layer1_Device9_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage11_RecvKV -> Layer1_Device9_Stage11_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage11_Attention [label=Q_local]
	Layer1_Device9_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage11_Attention -> Layer1_Device9_Stage11_Accumulate
	Layer1_Device9_Stage10_Accumulate -> Layer1_Device9_Stage11_Accumulate
	Layer1_Device9_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage11_RecvKV -> Layer1_Device9_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage12_RecvKV -> Layer1_Device9_Stage12_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage12_Attention [label=Q_local]
	Layer1_Device9_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage12_Attention -> Layer1_Device9_Stage12_Accumulate
	Layer1_Device9_Stage11_Accumulate -> Layer1_Device9_Stage12_Accumulate
	Layer1_Device9_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage12_RecvKV -> Layer1_Device9_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage13_RecvKV -> Layer1_Device9_Stage13_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage13_Attention [label=Q_local]
	Layer1_Device9_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage13_Attention -> Layer1_Device9_Stage13_Accumulate
	Layer1_Device9_Stage12_Accumulate -> Layer1_Device9_Stage13_Accumulate
	Layer1_Device9_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage13_RecvKV -> Layer1_Device9_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage14_RecvKV -> Layer1_Device9_Stage14_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage14_Attention [label=Q_local]
	Layer1_Device9_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage14_Attention -> Layer1_Device9_Stage14_Accumulate
	Layer1_Device9_Stage13_Accumulate -> Layer1_Device9_Stage14_Accumulate
	Layer1_Device9_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device9_Stage14_RecvKV -> Layer1_Device9_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device9_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device9_Stage15_RecvKV -> Layer1_Device9_Stage15_Attention
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage15_Attention [label=Q_local]
	Layer1_Device9_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device9_Stage15_Attention -> Layer1_Device9_Stage15_Accumulate
	Layer1_Device9_Stage14_Accumulate -> Layer1_Device9_Stage15_Accumulate
	Layer1_Device9_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device9_Stage15_Accumulate -> Layer1_Device9_ConcatHeads
	Layer1_Device9_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device9_ConcatHeads -> Layer1_Device9_OutputProj
	Layer1_Device9_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device9_OutputProj -> Layer1_Device9_Residual1
	Layer1_Device9_Input -> Layer1_Device9_Residual1
	Layer1_Device9_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device9_Residual1 -> Layer1_Device9_LayerNorm2
	Layer1_Device9_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device9_LayerNorm2 -> Layer1_Device9_GateProj
	Layer1_Device9_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device9_LayerNorm2 -> Layer1_Device9_UpProj
	Layer1_Device9_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device9_GateProj -> Layer1_Device9_Activation
	Layer1_Device9_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device9_Activation -> Layer1_Device9_ElemMul
	Layer1_Device9_UpProj -> Layer1_Device9_ElemMul
	Layer1_Device9_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device9_ElemMul -> Layer1_Device9_DownProj
	Layer1_Device9_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device9_DownProj -> Layer1_Device9_Residual2
	Layer1_Device9_Residual1 -> Layer1_Device9_Residual2
	Layer1_Device9_Output [label="Layer 1 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device9_Residual2 -> Layer1_Device9_Output
	Layer1_Device10_Input [label="Layer 1 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device10_Output -> Layer1_Device10_Input
	Layer1_Device10_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device10_Input -> Layer1_Device10_LayerNorm1
	Layer1_Device10_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device10_LayerNorm1 -> Layer1_Device10_QKVProj
	Layer1_Device10_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device10_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage0_RecvKV -> Layer1_Device10_Stage0_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage0_Attention [label=Q_local]
	Layer1_Device10_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage0_Attention -> Layer1_Device10_Stage0_Accumulate
	Layer1_Device10_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage0_RecvKV -> Layer1_Device10_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage1_RecvKV -> Layer1_Device10_Stage1_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage1_Attention [label=Q_local]
	Layer1_Device10_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage1_Attention -> Layer1_Device10_Stage1_Accumulate
	Layer1_Device10_Stage0_Accumulate -> Layer1_Device10_Stage1_Accumulate
	Layer1_Device10_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage1_RecvKV -> Layer1_Device10_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage2_RecvKV -> Layer1_Device10_Stage2_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage2_Attention [label=Q_local]
	Layer1_Device10_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage2_Attention -> Layer1_Device10_Stage2_Accumulate
	Layer1_Device10_Stage1_Accumulate -> Layer1_Device10_Stage2_Accumulate
	Layer1_Device10_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage2_RecvKV -> Layer1_Device10_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage3_RecvKV -> Layer1_Device10_Stage3_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage3_Attention [label=Q_local]
	Layer1_Device10_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage3_Attention -> Layer1_Device10_Stage3_Accumulate
	Layer1_Device10_Stage2_Accumulate -> Layer1_Device10_Stage3_Accumulate
	Layer1_Device10_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage3_RecvKV -> Layer1_Device10_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage4_RecvKV -> Layer1_Device10_Stage4_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage4_Attention [label=Q_local]
	Layer1_Device10_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage4_Attention -> Layer1_Device10_Stage4_Accumulate
	Layer1_Device10_Stage3_Accumulate -> Layer1_Device10_Stage4_Accumulate
	Layer1_Device10_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage4_RecvKV -> Layer1_Device10_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage5_RecvKV -> Layer1_Device10_Stage5_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage5_Attention [label=Q_local]
	Layer1_Device10_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage5_Attention -> Layer1_Device10_Stage5_Accumulate
	Layer1_Device10_Stage4_Accumulate -> Layer1_Device10_Stage5_Accumulate
	Layer1_Device10_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage5_RecvKV -> Layer1_Device10_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage6_RecvKV -> Layer1_Device10_Stage6_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage6_Attention [label=Q_local]
	Layer1_Device10_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage6_Attention -> Layer1_Device10_Stage6_Accumulate
	Layer1_Device10_Stage5_Accumulate -> Layer1_Device10_Stage6_Accumulate
	Layer1_Device10_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage6_RecvKV -> Layer1_Device10_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage7_RecvKV -> Layer1_Device10_Stage7_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage7_Attention [label=Q_local]
	Layer1_Device10_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage7_Attention -> Layer1_Device10_Stage7_Accumulate
	Layer1_Device10_Stage6_Accumulate -> Layer1_Device10_Stage7_Accumulate
	Layer1_Device10_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage7_RecvKV -> Layer1_Device10_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage8_RecvKV -> Layer1_Device10_Stage8_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage8_Attention [label=Q_local]
	Layer1_Device10_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage8_Attention -> Layer1_Device10_Stage8_Accumulate
	Layer1_Device10_Stage7_Accumulate -> Layer1_Device10_Stage8_Accumulate
	Layer1_Device10_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage8_RecvKV -> Layer1_Device10_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage9_RecvKV -> Layer1_Device10_Stage9_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage9_Attention [label=Q_local]
	Layer1_Device10_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage9_Attention -> Layer1_Device10_Stage9_Accumulate
	Layer1_Device10_Stage8_Accumulate -> Layer1_Device10_Stage9_Accumulate
	Layer1_Device10_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage9_RecvKV -> Layer1_Device10_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage10_RecvKV -> Layer1_Device10_Stage10_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage10_Attention [label=Q_local]
	Layer1_Device10_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage10_Attention -> Layer1_Device10_Stage10_Accumulate
	Layer1_Device10_Stage9_Accumulate -> Layer1_Device10_Stage10_Accumulate
	Layer1_Device10_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage10_RecvKV -> Layer1_Device10_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage11_RecvKV -> Layer1_Device10_Stage11_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage11_Attention [label=Q_local]
	Layer1_Device10_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage11_Attention -> Layer1_Device10_Stage11_Accumulate
	Layer1_Device10_Stage10_Accumulate -> Layer1_Device10_Stage11_Accumulate
	Layer1_Device10_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage11_RecvKV -> Layer1_Device10_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage12_RecvKV -> Layer1_Device10_Stage12_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage12_Attention [label=Q_local]
	Layer1_Device10_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage12_Attention -> Layer1_Device10_Stage12_Accumulate
	Layer1_Device10_Stage11_Accumulate -> Layer1_Device10_Stage12_Accumulate
	Layer1_Device10_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage12_RecvKV -> Layer1_Device10_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage13_RecvKV -> Layer1_Device10_Stage13_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage13_Attention [label=Q_local]
	Layer1_Device10_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage13_Attention -> Layer1_Device10_Stage13_Accumulate
	Layer1_Device10_Stage12_Accumulate -> Layer1_Device10_Stage13_Accumulate
	Layer1_Device10_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage13_RecvKV -> Layer1_Device10_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage14_RecvKV -> Layer1_Device10_Stage14_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage14_Attention [label=Q_local]
	Layer1_Device10_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage14_Attention -> Layer1_Device10_Stage14_Accumulate
	Layer1_Device10_Stage13_Accumulate -> Layer1_Device10_Stage14_Accumulate
	Layer1_Device10_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device10_Stage14_RecvKV -> Layer1_Device10_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device10_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device10_Stage15_RecvKV -> Layer1_Device10_Stage15_Attention
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage15_Attention [label=Q_local]
	Layer1_Device10_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device10_Stage15_Attention -> Layer1_Device10_Stage15_Accumulate
	Layer1_Device10_Stage14_Accumulate -> Layer1_Device10_Stage15_Accumulate
	Layer1_Device10_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device10_Stage15_Accumulate -> Layer1_Device10_ConcatHeads
	Layer1_Device10_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device10_ConcatHeads -> Layer1_Device10_OutputProj
	Layer1_Device10_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device10_OutputProj -> Layer1_Device10_Residual1
	Layer1_Device10_Input -> Layer1_Device10_Residual1
	Layer1_Device10_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device10_Residual1 -> Layer1_Device10_LayerNorm2
	Layer1_Device10_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device10_LayerNorm2 -> Layer1_Device10_GateProj
	Layer1_Device10_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device10_LayerNorm2 -> Layer1_Device10_UpProj
	Layer1_Device10_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device10_GateProj -> Layer1_Device10_Activation
	Layer1_Device10_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device10_Activation -> Layer1_Device10_ElemMul
	Layer1_Device10_UpProj -> Layer1_Device10_ElemMul
	Layer1_Device10_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device10_ElemMul -> Layer1_Device10_DownProj
	Layer1_Device10_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device10_DownProj -> Layer1_Device10_Residual2
	Layer1_Device10_Residual1 -> Layer1_Device10_Residual2
	Layer1_Device10_Output [label="Layer 1 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device10_Residual2 -> Layer1_Device10_Output
	Layer1_Device11_Input [label="Layer 1 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device11_Output -> Layer1_Device11_Input
	Layer1_Device11_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device11_Input -> Layer1_Device11_LayerNorm1
	Layer1_Device11_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device11_LayerNorm1 -> Layer1_Device11_QKVProj
	Layer1_Device11_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device11_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage0_RecvKV -> Layer1_Device11_Stage0_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage0_Attention [label=Q_local]
	Layer1_Device11_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage0_Attention -> Layer1_Device11_Stage0_Accumulate
	Layer1_Device11_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage0_RecvKV -> Layer1_Device11_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage1_RecvKV -> Layer1_Device11_Stage1_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage1_Attention [label=Q_local]
	Layer1_Device11_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage1_Attention -> Layer1_Device11_Stage1_Accumulate
	Layer1_Device11_Stage0_Accumulate -> Layer1_Device11_Stage1_Accumulate
	Layer1_Device11_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage1_RecvKV -> Layer1_Device11_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage2_RecvKV -> Layer1_Device11_Stage2_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage2_Attention [label=Q_local]
	Layer1_Device11_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage2_Attention -> Layer1_Device11_Stage2_Accumulate
	Layer1_Device11_Stage1_Accumulate -> Layer1_Device11_Stage2_Accumulate
	Layer1_Device11_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage2_RecvKV -> Layer1_Device11_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage3_RecvKV -> Layer1_Device11_Stage3_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage3_Attention [label=Q_local]
	Layer1_Device11_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage3_Attention -> Layer1_Device11_Stage3_Accumulate
	Layer1_Device11_Stage2_Accumulate -> Layer1_Device11_Stage3_Accumulate
	Layer1_Device11_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage3_RecvKV -> Layer1_Device11_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage4_RecvKV -> Layer1_Device11_Stage4_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage4_Attention [label=Q_local]
	Layer1_Device11_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage4_Attention -> Layer1_Device11_Stage4_Accumulate
	Layer1_Device11_Stage3_Accumulate -> Layer1_Device11_Stage4_Accumulate
	Layer1_Device11_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage4_RecvKV -> Layer1_Device11_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage5_RecvKV -> Layer1_Device11_Stage5_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage5_Attention [label=Q_local]
	Layer1_Device11_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage5_Attention -> Layer1_Device11_Stage5_Accumulate
	Layer1_Device11_Stage4_Accumulate -> Layer1_Device11_Stage5_Accumulate
	Layer1_Device11_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage5_RecvKV -> Layer1_Device11_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage6_RecvKV -> Layer1_Device11_Stage6_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage6_Attention [label=Q_local]
	Layer1_Device11_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage6_Attention -> Layer1_Device11_Stage6_Accumulate
	Layer1_Device11_Stage5_Accumulate -> Layer1_Device11_Stage6_Accumulate
	Layer1_Device11_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage6_RecvKV -> Layer1_Device11_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage7_RecvKV -> Layer1_Device11_Stage7_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage7_Attention [label=Q_local]
	Layer1_Device11_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage7_Attention -> Layer1_Device11_Stage7_Accumulate
	Layer1_Device11_Stage6_Accumulate -> Layer1_Device11_Stage7_Accumulate
	Layer1_Device11_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage7_RecvKV -> Layer1_Device11_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage8_RecvKV -> Layer1_Device11_Stage8_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage8_Attention [label=Q_local]
	Layer1_Device11_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage8_Attention -> Layer1_Device11_Stage8_Accumulate
	Layer1_Device11_Stage7_Accumulate -> Layer1_Device11_Stage8_Accumulate
	Layer1_Device11_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage8_RecvKV -> Layer1_Device11_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage9_RecvKV -> Layer1_Device11_Stage9_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage9_Attention [label=Q_local]
	Layer1_Device11_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage9_Attention -> Layer1_Device11_Stage9_Accumulate
	Layer1_Device11_Stage8_Accumulate -> Layer1_Device11_Stage9_Accumulate
	Layer1_Device11_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage9_RecvKV -> Layer1_Device11_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage10_RecvKV -> Layer1_Device11_Stage10_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage10_Attention [label=Q_local]
	Layer1_Device11_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage10_Attention -> Layer1_Device11_Stage10_Accumulate
	Layer1_Device11_Stage9_Accumulate -> Layer1_Device11_Stage10_Accumulate
	Layer1_Device11_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage10_RecvKV -> Layer1_Device11_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage11_RecvKV -> Layer1_Device11_Stage11_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage11_Attention [label=Q_local]
	Layer1_Device11_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage11_Attention -> Layer1_Device11_Stage11_Accumulate
	Layer1_Device11_Stage10_Accumulate -> Layer1_Device11_Stage11_Accumulate
	Layer1_Device11_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage11_RecvKV -> Layer1_Device11_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage12_RecvKV -> Layer1_Device11_Stage12_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage12_Attention [label=Q_local]
	Layer1_Device11_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage12_Attention -> Layer1_Device11_Stage12_Accumulate
	Layer1_Device11_Stage11_Accumulate -> Layer1_Device11_Stage12_Accumulate
	Layer1_Device11_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage12_RecvKV -> Layer1_Device11_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage13_RecvKV -> Layer1_Device11_Stage13_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage13_Attention [label=Q_local]
	Layer1_Device11_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage13_Attention -> Layer1_Device11_Stage13_Accumulate
	Layer1_Device11_Stage12_Accumulate -> Layer1_Device11_Stage13_Accumulate
	Layer1_Device11_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage13_RecvKV -> Layer1_Device11_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage14_RecvKV -> Layer1_Device11_Stage14_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage14_Attention [label=Q_local]
	Layer1_Device11_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage14_Attention -> Layer1_Device11_Stage14_Accumulate
	Layer1_Device11_Stage13_Accumulate -> Layer1_Device11_Stage14_Accumulate
	Layer1_Device11_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device11_Stage14_RecvKV -> Layer1_Device11_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device11_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device11_Stage15_RecvKV -> Layer1_Device11_Stage15_Attention
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage15_Attention [label=Q_local]
	Layer1_Device11_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device11_Stage15_Attention -> Layer1_Device11_Stage15_Accumulate
	Layer1_Device11_Stage14_Accumulate -> Layer1_Device11_Stage15_Accumulate
	Layer1_Device11_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device11_Stage15_Accumulate -> Layer1_Device11_ConcatHeads
	Layer1_Device11_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device11_ConcatHeads -> Layer1_Device11_OutputProj
	Layer1_Device11_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device11_OutputProj -> Layer1_Device11_Residual1
	Layer1_Device11_Input -> Layer1_Device11_Residual1
	Layer1_Device11_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device11_Residual1 -> Layer1_Device11_LayerNorm2
	Layer1_Device11_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device11_LayerNorm2 -> Layer1_Device11_GateProj
	Layer1_Device11_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device11_LayerNorm2 -> Layer1_Device11_UpProj
	Layer1_Device11_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device11_GateProj -> Layer1_Device11_Activation
	Layer1_Device11_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device11_Activation -> Layer1_Device11_ElemMul
	Layer1_Device11_UpProj -> Layer1_Device11_ElemMul
	Layer1_Device11_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device11_ElemMul -> Layer1_Device11_DownProj
	Layer1_Device11_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device11_DownProj -> Layer1_Device11_Residual2
	Layer1_Device11_Residual1 -> Layer1_Device11_Residual2
	Layer1_Device11_Output [label="Layer 1 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device11_Residual2 -> Layer1_Device11_Output
	Layer1_Device12_Input [label="Layer 1 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device12_Output -> Layer1_Device12_Input
	Layer1_Device12_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device12_Input -> Layer1_Device12_LayerNorm1
	Layer1_Device12_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device12_LayerNorm1 -> Layer1_Device12_QKVProj
	Layer1_Device12_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device12_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage0_RecvKV -> Layer1_Device12_Stage0_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage0_Attention [label=Q_local]
	Layer1_Device12_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage0_Attention -> Layer1_Device12_Stage0_Accumulate
	Layer1_Device12_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage0_RecvKV -> Layer1_Device12_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage1_RecvKV -> Layer1_Device12_Stage1_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage1_Attention [label=Q_local]
	Layer1_Device12_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage1_Attention -> Layer1_Device12_Stage1_Accumulate
	Layer1_Device12_Stage0_Accumulate -> Layer1_Device12_Stage1_Accumulate
	Layer1_Device12_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage1_RecvKV -> Layer1_Device12_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage2_RecvKV -> Layer1_Device12_Stage2_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage2_Attention [label=Q_local]
	Layer1_Device12_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage2_Attention -> Layer1_Device12_Stage2_Accumulate
	Layer1_Device12_Stage1_Accumulate -> Layer1_Device12_Stage2_Accumulate
	Layer1_Device12_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage2_RecvKV -> Layer1_Device12_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage3_RecvKV -> Layer1_Device12_Stage3_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage3_Attention [label=Q_local]
	Layer1_Device12_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage3_Attention -> Layer1_Device12_Stage3_Accumulate
	Layer1_Device12_Stage2_Accumulate -> Layer1_Device12_Stage3_Accumulate
	Layer1_Device12_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage3_RecvKV -> Layer1_Device12_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage4_RecvKV -> Layer1_Device12_Stage4_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage4_Attention [label=Q_local]
	Layer1_Device12_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage4_Attention -> Layer1_Device12_Stage4_Accumulate
	Layer1_Device12_Stage3_Accumulate -> Layer1_Device12_Stage4_Accumulate
	Layer1_Device12_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage4_RecvKV -> Layer1_Device12_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage5_RecvKV -> Layer1_Device12_Stage5_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage5_Attention [label=Q_local]
	Layer1_Device12_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage5_Attention -> Layer1_Device12_Stage5_Accumulate
	Layer1_Device12_Stage4_Accumulate -> Layer1_Device12_Stage5_Accumulate
	Layer1_Device12_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage5_RecvKV -> Layer1_Device12_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage6_RecvKV -> Layer1_Device12_Stage6_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage6_Attention [label=Q_local]
	Layer1_Device12_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage6_Attention -> Layer1_Device12_Stage6_Accumulate
	Layer1_Device12_Stage5_Accumulate -> Layer1_Device12_Stage6_Accumulate
	Layer1_Device12_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage6_RecvKV -> Layer1_Device12_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage7_RecvKV -> Layer1_Device12_Stage7_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage7_Attention [label=Q_local]
	Layer1_Device12_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage7_Attention -> Layer1_Device12_Stage7_Accumulate
	Layer1_Device12_Stage6_Accumulate -> Layer1_Device12_Stage7_Accumulate
	Layer1_Device12_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage7_RecvKV -> Layer1_Device12_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage8_RecvKV -> Layer1_Device12_Stage8_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage8_Attention [label=Q_local]
	Layer1_Device12_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage8_Attention -> Layer1_Device12_Stage8_Accumulate
	Layer1_Device12_Stage7_Accumulate -> Layer1_Device12_Stage8_Accumulate
	Layer1_Device12_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage8_RecvKV -> Layer1_Device12_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage9_RecvKV -> Layer1_Device12_Stage9_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage9_Attention [label=Q_local]
	Layer1_Device12_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage9_Attention -> Layer1_Device12_Stage9_Accumulate
	Layer1_Device12_Stage8_Accumulate -> Layer1_Device12_Stage9_Accumulate
	Layer1_Device12_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage9_RecvKV -> Layer1_Device12_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage10_RecvKV -> Layer1_Device12_Stage10_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage10_Attention [label=Q_local]
	Layer1_Device12_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage10_Attention -> Layer1_Device12_Stage10_Accumulate
	Layer1_Device12_Stage9_Accumulate -> Layer1_Device12_Stage10_Accumulate
	Layer1_Device12_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage10_RecvKV -> Layer1_Device12_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage11_RecvKV -> Layer1_Device12_Stage11_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage11_Attention [label=Q_local]
	Layer1_Device12_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage11_Attention -> Layer1_Device12_Stage11_Accumulate
	Layer1_Device12_Stage10_Accumulate -> Layer1_Device12_Stage11_Accumulate
	Layer1_Device12_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage11_RecvKV -> Layer1_Device12_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage12_RecvKV -> Layer1_Device12_Stage12_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage12_Attention [label=Q_local]
	Layer1_Device12_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage12_Attention -> Layer1_Device12_Stage12_Accumulate
	Layer1_Device12_Stage11_Accumulate -> Layer1_Device12_Stage12_Accumulate
	Layer1_Device12_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage12_RecvKV -> Layer1_Device12_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage13_RecvKV -> Layer1_Device12_Stage13_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage13_Attention [label=Q_local]
	Layer1_Device12_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage13_Attention -> Layer1_Device12_Stage13_Accumulate
	Layer1_Device12_Stage12_Accumulate -> Layer1_Device12_Stage13_Accumulate
	Layer1_Device12_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage13_RecvKV -> Layer1_Device12_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage14_RecvKV -> Layer1_Device12_Stage14_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage14_Attention [label=Q_local]
	Layer1_Device12_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage14_Attention -> Layer1_Device12_Stage14_Accumulate
	Layer1_Device12_Stage13_Accumulate -> Layer1_Device12_Stage14_Accumulate
	Layer1_Device12_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device12_Stage14_RecvKV -> Layer1_Device12_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device12_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device12_Stage15_RecvKV -> Layer1_Device12_Stage15_Attention
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage15_Attention [label=Q_local]
	Layer1_Device12_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device12_Stage15_Attention -> Layer1_Device12_Stage15_Accumulate
	Layer1_Device12_Stage14_Accumulate -> Layer1_Device12_Stage15_Accumulate
	Layer1_Device12_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device12_Stage15_Accumulate -> Layer1_Device12_ConcatHeads
	Layer1_Device12_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device12_ConcatHeads -> Layer1_Device12_OutputProj
	Layer1_Device12_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device12_OutputProj -> Layer1_Device12_Residual1
	Layer1_Device12_Input -> Layer1_Device12_Residual1
	Layer1_Device12_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device12_Residual1 -> Layer1_Device12_LayerNorm2
	Layer1_Device12_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device12_LayerNorm2 -> Layer1_Device12_GateProj
	Layer1_Device12_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device12_LayerNorm2 -> Layer1_Device12_UpProj
	Layer1_Device12_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device12_GateProj -> Layer1_Device12_Activation
	Layer1_Device12_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device12_Activation -> Layer1_Device12_ElemMul
	Layer1_Device12_UpProj -> Layer1_Device12_ElemMul
	Layer1_Device12_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device12_ElemMul -> Layer1_Device12_DownProj
	Layer1_Device12_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device12_DownProj -> Layer1_Device12_Residual2
	Layer1_Device12_Residual1 -> Layer1_Device12_Residual2
	Layer1_Device12_Output [label="Layer 1 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device12_Residual2 -> Layer1_Device12_Output
	Layer1_Device13_Input [label="Layer 1 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device13_Output -> Layer1_Device13_Input
	Layer1_Device13_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device13_Input -> Layer1_Device13_LayerNorm1
	Layer1_Device13_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device13_LayerNorm1 -> Layer1_Device13_QKVProj
	Layer1_Device13_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device13_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage0_RecvKV -> Layer1_Device13_Stage0_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage0_Attention [label=Q_local]
	Layer1_Device13_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage0_Attention -> Layer1_Device13_Stage0_Accumulate
	Layer1_Device13_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage0_RecvKV -> Layer1_Device13_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage1_RecvKV -> Layer1_Device13_Stage1_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage1_Attention [label=Q_local]
	Layer1_Device13_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage1_Attention -> Layer1_Device13_Stage1_Accumulate
	Layer1_Device13_Stage0_Accumulate -> Layer1_Device13_Stage1_Accumulate
	Layer1_Device13_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage1_RecvKV -> Layer1_Device13_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage2_RecvKV -> Layer1_Device13_Stage2_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage2_Attention [label=Q_local]
	Layer1_Device13_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage2_Attention -> Layer1_Device13_Stage2_Accumulate
	Layer1_Device13_Stage1_Accumulate -> Layer1_Device13_Stage2_Accumulate
	Layer1_Device13_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage2_RecvKV -> Layer1_Device13_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage3_RecvKV -> Layer1_Device13_Stage3_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage3_Attention [label=Q_local]
	Layer1_Device13_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage3_Attention -> Layer1_Device13_Stage3_Accumulate
	Layer1_Device13_Stage2_Accumulate -> Layer1_Device13_Stage3_Accumulate
	Layer1_Device13_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage3_RecvKV -> Layer1_Device13_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage4_RecvKV -> Layer1_Device13_Stage4_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage4_Attention [label=Q_local]
	Layer1_Device13_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage4_Attention -> Layer1_Device13_Stage4_Accumulate
	Layer1_Device13_Stage3_Accumulate -> Layer1_Device13_Stage4_Accumulate
	Layer1_Device13_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage4_RecvKV -> Layer1_Device13_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage5_RecvKV -> Layer1_Device13_Stage5_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage5_Attention [label=Q_local]
	Layer1_Device13_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage5_Attention -> Layer1_Device13_Stage5_Accumulate
	Layer1_Device13_Stage4_Accumulate -> Layer1_Device13_Stage5_Accumulate
	Layer1_Device13_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage5_RecvKV -> Layer1_Device13_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage6_RecvKV -> Layer1_Device13_Stage6_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage6_Attention [label=Q_local]
	Layer1_Device13_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage6_Attention -> Layer1_Device13_Stage6_Accumulate
	Layer1_Device13_Stage5_Accumulate -> Layer1_Device13_Stage6_Accumulate
	Layer1_Device13_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage6_RecvKV -> Layer1_Device13_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage7_RecvKV -> Layer1_Device13_Stage7_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage7_Attention [label=Q_local]
	Layer1_Device13_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage7_Attention -> Layer1_Device13_Stage7_Accumulate
	Layer1_Device13_Stage6_Accumulate -> Layer1_Device13_Stage7_Accumulate
	Layer1_Device13_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage7_RecvKV -> Layer1_Device13_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage8_RecvKV -> Layer1_Device13_Stage8_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage8_Attention [label=Q_local]
	Layer1_Device13_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage8_Attention -> Layer1_Device13_Stage8_Accumulate
	Layer1_Device13_Stage7_Accumulate -> Layer1_Device13_Stage8_Accumulate
	Layer1_Device13_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage8_RecvKV -> Layer1_Device13_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage9_RecvKV -> Layer1_Device13_Stage9_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage9_Attention [label=Q_local]
	Layer1_Device13_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage9_Attention -> Layer1_Device13_Stage9_Accumulate
	Layer1_Device13_Stage8_Accumulate -> Layer1_Device13_Stage9_Accumulate
	Layer1_Device13_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage9_RecvKV -> Layer1_Device13_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage10_RecvKV -> Layer1_Device13_Stage10_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage10_Attention [label=Q_local]
	Layer1_Device13_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage10_Attention -> Layer1_Device13_Stage10_Accumulate
	Layer1_Device13_Stage9_Accumulate -> Layer1_Device13_Stage10_Accumulate
	Layer1_Device13_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage10_RecvKV -> Layer1_Device13_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage11_RecvKV -> Layer1_Device13_Stage11_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage11_Attention [label=Q_local]
	Layer1_Device13_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage11_Attention -> Layer1_Device13_Stage11_Accumulate
	Layer1_Device13_Stage10_Accumulate -> Layer1_Device13_Stage11_Accumulate
	Layer1_Device13_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage11_RecvKV -> Layer1_Device13_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage12_RecvKV -> Layer1_Device13_Stage12_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage12_Attention [label=Q_local]
	Layer1_Device13_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage12_Attention -> Layer1_Device13_Stage12_Accumulate
	Layer1_Device13_Stage11_Accumulate -> Layer1_Device13_Stage12_Accumulate
	Layer1_Device13_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage12_RecvKV -> Layer1_Device13_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage13_RecvKV -> Layer1_Device13_Stage13_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage13_Attention [label=Q_local]
	Layer1_Device13_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage13_Attention -> Layer1_Device13_Stage13_Accumulate
	Layer1_Device13_Stage12_Accumulate -> Layer1_Device13_Stage13_Accumulate
	Layer1_Device13_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage13_RecvKV -> Layer1_Device13_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage14_RecvKV -> Layer1_Device13_Stage14_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage14_Attention [label=Q_local]
	Layer1_Device13_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage14_Attention -> Layer1_Device13_Stage14_Accumulate
	Layer1_Device13_Stage13_Accumulate -> Layer1_Device13_Stage14_Accumulate
	Layer1_Device13_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device13_Stage14_RecvKV -> Layer1_Device13_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device13_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device13_Stage15_RecvKV -> Layer1_Device13_Stage15_Attention
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage15_Attention [label=Q_local]
	Layer1_Device13_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device13_Stage15_Attention -> Layer1_Device13_Stage15_Accumulate
	Layer1_Device13_Stage14_Accumulate -> Layer1_Device13_Stage15_Accumulate
	Layer1_Device13_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device13_Stage15_Accumulate -> Layer1_Device13_ConcatHeads
	Layer1_Device13_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device13_ConcatHeads -> Layer1_Device13_OutputProj
	Layer1_Device13_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device13_OutputProj -> Layer1_Device13_Residual1
	Layer1_Device13_Input -> Layer1_Device13_Residual1
	Layer1_Device13_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device13_Residual1 -> Layer1_Device13_LayerNorm2
	Layer1_Device13_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device13_LayerNorm2 -> Layer1_Device13_GateProj
	Layer1_Device13_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device13_LayerNorm2 -> Layer1_Device13_UpProj
	Layer1_Device13_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device13_GateProj -> Layer1_Device13_Activation
	Layer1_Device13_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device13_Activation -> Layer1_Device13_ElemMul
	Layer1_Device13_UpProj -> Layer1_Device13_ElemMul
	Layer1_Device13_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device13_ElemMul -> Layer1_Device13_DownProj
	Layer1_Device13_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device13_DownProj -> Layer1_Device13_Residual2
	Layer1_Device13_Residual1 -> Layer1_Device13_Residual2
	Layer1_Device13_Output [label="Layer 1 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device13_Residual2 -> Layer1_Device13_Output
	Layer1_Device14_Input [label="Layer 1 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device14_Output -> Layer1_Device14_Input
	Layer1_Device14_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device14_Input -> Layer1_Device14_LayerNorm1
	Layer1_Device14_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device14_LayerNorm1 -> Layer1_Device14_QKVProj
	Layer1_Device14_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device14_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage0_RecvKV -> Layer1_Device14_Stage0_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage0_Attention [label=Q_local]
	Layer1_Device14_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage0_Attention -> Layer1_Device14_Stage0_Accumulate
	Layer1_Device14_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage0_RecvKV -> Layer1_Device14_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage1_RecvKV -> Layer1_Device14_Stage1_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage1_Attention [label=Q_local]
	Layer1_Device14_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage1_Attention -> Layer1_Device14_Stage1_Accumulate
	Layer1_Device14_Stage0_Accumulate -> Layer1_Device14_Stage1_Accumulate
	Layer1_Device14_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage1_RecvKV -> Layer1_Device14_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage2_RecvKV -> Layer1_Device14_Stage2_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage2_Attention [label=Q_local]
	Layer1_Device14_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage2_Attention -> Layer1_Device14_Stage2_Accumulate
	Layer1_Device14_Stage1_Accumulate -> Layer1_Device14_Stage2_Accumulate
	Layer1_Device14_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage2_RecvKV -> Layer1_Device14_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage3_RecvKV -> Layer1_Device14_Stage3_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage3_Attention [label=Q_local]
	Layer1_Device14_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage3_Attention -> Layer1_Device14_Stage3_Accumulate
	Layer1_Device14_Stage2_Accumulate -> Layer1_Device14_Stage3_Accumulate
	Layer1_Device14_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage3_RecvKV -> Layer1_Device14_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage4_RecvKV -> Layer1_Device14_Stage4_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage4_Attention [label=Q_local]
	Layer1_Device14_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage4_Attention -> Layer1_Device14_Stage4_Accumulate
	Layer1_Device14_Stage3_Accumulate -> Layer1_Device14_Stage4_Accumulate
	Layer1_Device14_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage4_RecvKV -> Layer1_Device14_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage5_RecvKV -> Layer1_Device14_Stage5_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage5_Attention [label=Q_local]
	Layer1_Device14_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage5_Attention -> Layer1_Device14_Stage5_Accumulate
	Layer1_Device14_Stage4_Accumulate -> Layer1_Device14_Stage5_Accumulate
	Layer1_Device14_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage5_RecvKV -> Layer1_Device14_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage6_RecvKV -> Layer1_Device14_Stage6_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage6_Attention [label=Q_local]
	Layer1_Device14_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage6_Attention -> Layer1_Device14_Stage6_Accumulate
	Layer1_Device14_Stage5_Accumulate -> Layer1_Device14_Stage6_Accumulate
	Layer1_Device14_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage6_RecvKV -> Layer1_Device14_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage7_RecvKV -> Layer1_Device14_Stage7_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage7_Attention [label=Q_local]
	Layer1_Device14_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage7_Attention -> Layer1_Device14_Stage7_Accumulate
	Layer1_Device14_Stage6_Accumulate -> Layer1_Device14_Stage7_Accumulate
	Layer1_Device14_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage7_RecvKV -> Layer1_Device14_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage8_RecvKV -> Layer1_Device14_Stage8_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage8_Attention [label=Q_local]
	Layer1_Device14_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage8_Attention -> Layer1_Device14_Stage8_Accumulate
	Layer1_Device14_Stage7_Accumulate -> Layer1_Device14_Stage8_Accumulate
	Layer1_Device14_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage8_RecvKV -> Layer1_Device14_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage9_RecvKV -> Layer1_Device14_Stage9_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage9_Attention [label=Q_local]
	Layer1_Device14_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage9_Attention -> Layer1_Device14_Stage9_Accumulate
	Layer1_Device14_Stage8_Accumulate -> Layer1_Device14_Stage9_Accumulate
	Layer1_Device14_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage9_RecvKV -> Layer1_Device14_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage10_RecvKV -> Layer1_Device14_Stage10_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage10_Attention [label=Q_local]
	Layer1_Device14_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage10_Attention -> Layer1_Device14_Stage10_Accumulate
	Layer1_Device14_Stage9_Accumulate -> Layer1_Device14_Stage10_Accumulate
	Layer1_Device14_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage10_RecvKV -> Layer1_Device14_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage11_RecvKV -> Layer1_Device14_Stage11_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage11_Attention [label=Q_local]
	Layer1_Device14_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage11_Attention -> Layer1_Device14_Stage11_Accumulate
	Layer1_Device14_Stage10_Accumulate -> Layer1_Device14_Stage11_Accumulate
	Layer1_Device14_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage11_RecvKV -> Layer1_Device14_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage12_RecvKV -> Layer1_Device14_Stage12_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage12_Attention [label=Q_local]
	Layer1_Device14_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage12_Attention -> Layer1_Device14_Stage12_Accumulate
	Layer1_Device14_Stage11_Accumulate -> Layer1_Device14_Stage12_Accumulate
	Layer1_Device14_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage12_RecvKV -> Layer1_Device14_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage13_RecvKV -> Layer1_Device14_Stage13_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage13_Attention [label=Q_local]
	Layer1_Device14_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage13_Attention -> Layer1_Device14_Stage13_Accumulate
	Layer1_Device14_Stage12_Accumulate -> Layer1_Device14_Stage13_Accumulate
	Layer1_Device14_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage13_RecvKV -> Layer1_Device14_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage14_RecvKV -> Layer1_Device14_Stage14_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage14_Attention [label=Q_local]
	Layer1_Device14_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage14_Attention -> Layer1_Device14_Stage14_Accumulate
	Layer1_Device14_Stage13_Accumulate -> Layer1_Device14_Stage14_Accumulate
	Layer1_Device14_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device14_Stage14_RecvKV -> Layer1_Device14_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device14_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device14_Stage15_RecvKV -> Layer1_Device14_Stage15_Attention
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage15_Attention [label=Q_local]
	Layer1_Device14_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device14_Stage15_Attention -> Layer1_Device14_Stage15_Accumulate
	Layer1_Device14_Stage14_Accumulate -> Layer1_Device14_Stage15_Accumulate
	Layer1_Device14_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device14_Stage15_Accumulate -> Layer1_Device14_ConcatHeads
	Layer1_Device14_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device14_ConcatHeads -> Layer1_Device14_OutputProj
	Layer1_Device14_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device14_OutputProj -> Layer1_Device14_Residual1
	Layer1_Device14_Input -> Layer1_Device14_Residual1
	Layer1_Device14_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device14_Residual1 -> Layer1_Device14_LayerNorm2
	Layer1_Device14_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device14_LayerNorm2 -> Layer1_Device14_GateProj
	Layer1_Device14_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device14_LayerNorm2 -> Layer1_Device14_UpProj
	Layer1_Device14_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device14_GateProj -> Layer1_Device14_Activation
	Layer1_Device14_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device14_Activation -> Layer1_Device14_ElemMul
	Layer1_Device14_UpProj -> Layer1_Device14_ElemMul
	Layer1_Device14_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device14_ElemMul -> Layer1_Device14_DownProj
	Layer1_Device14_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device14_DownProj -> Layer1_Device14_Residual2
	Layer1_Device14_Residual1 -> Layer1_Device14_Residual2
	Layer1_Device14_Output [label="Layer 1 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device14_Residual2 -> Layer1_Device14_Output
	Layer1_Device15_Input [label="Layer 1 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer0_Device15_Output -> Layer1_Device15_Input
	Layer1_Device15_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device15_Input -> Layer1_Device15_LayerNorm1
	Layer1_Device15_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device15_LayerNorm1 -> Layer1_Device15_QKVProj
	Layer1_Device15_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage0_RecvKV [label="Local K,V"]
	Layer1_Device15_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage0_RecvKV -> Layer1_Device15_Stage0_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage0_Attention [label=Q_local]
	Layer1_Device15_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage0_Attention -> Layer1_Device15_Stage0_Accumulate
	Layer1_Device15_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage0_RecvKV -> Layer1_Device15_Stage1_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage1_RecvKV -> Layer1_Device15_Stage1_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage1_Attention [label=Q_local]
	Layer1_Device15_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage1_Attention -> Layer1_Device15_Stage1_Accumulate
	Layer1_Device15_Stage0_Accumulate -> Layer1_Device15_Stage1_Accumulate
	Layer1_Device15_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage1_RecvKV -> Layer1_Device15_Stage2_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage2_RecvKV -> Layer1_Device15_Stage2_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage2_Attention [label=Q_local]
	Layer1_Device15_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage2_Attention -> Layer1_Device15_Stage2_Accumulate
	Layer1_Device15_Stage1_Accumulate -> Layer1_Device15_Stage2_Accumulate
	Layer1_Device15_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage2_RecvKV -> Layer1_Device15_Stage3_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage3_RecvKV -> Layer1_Device15_Stage3_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage3_Attention [label=Q_local]
	Layer1_Device15_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage3_Attention -> Layer1_Device15_Stage3_Accumulate
	Layer1_Device15_Stage2_Accumulate -> Layer1_Device15_Stage3_Accumulate
	Layer1_Device15_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage3_RecvKV -> Layer1_Device15_Stage4_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage4_RecvKV -> Layer1_Device15_Stage4_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage4_Attention [label=Q_local]
	Layer1_Device15_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage4_Attention -> Layer1_Device15_Stage4_Accumulate
	Layer1_Device15_Stage3_Accumulate -> Layer1_Device15_Stage4_Accumulate
	Layer1_Device15_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage4_RecvKV -> Layer1_Device15_Stage5_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage5_RecvKV -> Layer1_Device15_Stage5_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage5_Attention [label=Q_local]
	Layer1_Device15_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage5_Attention -> Layer1_Device15_Stage5_Accumulate
	Layer1_Device15_Stage4_Accumulate -> Layer1_Device15_Stage5_Accumulate
	Layer1_Device15_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage5_RecvKV -> Layer1_Device15_Stage6_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage6_RecvKV -> Layer1_Device15_Stage6_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage6_Attention [label=Q_local]
	Layer1_Device15_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage6_Attention -> Layer1_Device15_Stage6_Accumulate
	Layer1_Device15_Stage5_Accumulate -> Layer1_Device15_Stage6_Accumulate
	Layer1_Device15_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage6_RecvKV -> Layer1_Device15_Stage7_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage7_RecvKV -> Layer1_Device15_Stage7_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage7_Attention [label=Q_local]
	Layer1_Device15_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage7_Attention -> Layer1_Device15_Stage7_Accumulate
	Layer1_Device15_Stage6_Accumulate -> Layer1_Device15_Stage7_Accumulate
	Layer1_Device15_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage7_RecvKV -> Layer1_Device15_Stage8_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage8_RecvKV -> Layer1_Device15_Stage8_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage8_Attention [label=Q_local]
	Layer1_Device15_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage8_Attention -> Layer1_Device15_Stage8_Accumulate
	Layer1_Device15_Stage7_Accumulate -> Layer1_Device15_Stage8_Accumulate
	Layer1_Device15_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage8_RecvKV -> Layer1_Device15_Stage9_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage9_RecvKV -> Layer1_Device15_Stage9_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage9_Attention [label=Q_local]
	Layer1_Device15_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage9_Attention -> Layer1_Device15_Stage9_Accumulate
	Layer1_Device15_Stage8_Accumulate -> Layer1_Device15_Stage9_Accumulate
	Layer1_Device15_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage9_RecvKV -> Layer1_Device15_Stage10_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage10_RecvKV -> Layer1_Device15_Stage10_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage10_Attention [label=Q_local]
	Layer1_Device15_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage10_Attention -> Layer1_Device15_Stage10_Accumulate
	Layer1_Device15_Stage9_Accumulate -> Layer1_Device15_Stage10_Accumulate
	Layer1_Device15_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage10_RecvKV -> Layer1_Device15_Stage11_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage11_RecvKV -> Layer1_Device15_Stage11_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage11_Attention [label=Q_local]
	Layer1_Device15_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage11_Attention -> Layer1_Device15_Stage11_Accumulate
	Layer1_Device15_Stage10_Accumulate -> Layer1_Device15_Stage11_Accumulate
	Layer1_Device15_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage11_RecvKV -> Layer1_Device15_Stage12_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage12_RecvKV -> Layer1_Device15_Stage12_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage12_Attention [label=Q_local]
	Layer1_Device15_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage12_Attention -> Layer1_Device15_Stage12_Accumulate
	Layer1_Device15_Stage11_Accumulate -> Layer1_Device15_Stage12_Accumulate
	Layer1_Device15_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage12_RecvKV -> Layer1_Device15_Stage13_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage13_RecvKV -> Layer1_Device15_Stage13_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage13_Attention [label=Q_local]
	Layer1_Device15_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage13_Attention -> Layer1_Device15_Stage13_Accumulate
	Layer1_Device15_Stage12_Accumulate -> Layer1_Device15_Stage13_Accumulate
	Layer1_Device15_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage13_RecvKV -> Layer1_Device15_Stage14_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage14_RecvKV -> Layer1_Device15_Stage14_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage14_Attention [label=Q_local]
	Layer1_Device15_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage14_Attention -> Layer1_Device15_Stage14_Accumulate
	Layer1_Device15_Stage13_Accumulate -> Layer1_Device15_Stage14_Accumulate
	Layer1_Device15_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer1_Device15_Stage14_RecvKV -> Layer1_Device15_Stage15_RecvKV [label="Ring transfer"]
	Layer1_Device15_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer1_Device15_Stage15_RecvKV -> Layer1_Device15_Stage15_Attention
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage15_Attention [label=Q_local]
	Layer1_Device15_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer1_Device15_Stage15_Attention -> Layer1_Device15_Stage15_Accumulate
	Layer1_Device15_Stage14_Accumulate -> Layer1_Device15_Stage15_Accumulate
	Layer1_Device15_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer1_Device15_Stage15_Accumulate -> Layer1_Device15_ConcatHeads
	Layer1_Device15_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device15_ConcatHeads -> Layer1_Device15_OutputProj
	Layer1_Device15_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device15_OutputProj -> Layer1_Device15_Residual1
	Layer1_Device15_Input -> Layer1_Device15_Residual1
	Layer1_Device15_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device15_Residual1 -> Layer1_Device15_LayerNorm2
	Layer1_Device15_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device15_LayerNorm2 -> Layer1_Device15_GateProj
	Layer1_Device15_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device15_LayerNorm2 -> Layer1_Device15_UpProj
	Layer1_Device15_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device15_GateProj -> Layer1_Device15_Activation
	Layer1_Device15_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer1_Device15_Activation -> Layer1_Device15_ElemMul
	Layer1_Device15_UpProj -> Layer1_Device15_ElemMul
	Layer1_Device15_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer1_Device15_ElemMul -> Layer1_Device15_DownProj
	Layer1_Device15_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer1_Device15_DownProj -> Layer1_Device15_Residual2
	Layer1_Device15_Residual1 -> Layer1_Device15_Residual2
	Layer1_Device15_Output [label="Layer 1 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device15_Residual2 -> Layer1_Device15_Output
	Layer2_Device0_Input [label="Layer 2 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device0_Output -> Layer2_Device0_Input
	Layer2_Device0_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device0_Input -> Layer2_Device0_LayerNorm1
	Layer2_Device0_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device0_LayerNorm1 -> Layer2_Device0_QKVProj
	Layer2_Device0_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device0_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage0_RecvKV -> Layer2_Device0_Stage0_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage0_Attention [label=Q_local]
	Layer2_Device0_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage0_Attention -> Layer2_Device0_Stage0_Accumulate
	Layer2_Device0_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage0_RecvKV -> Layer2_Device0_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage1_RecvKV -> Layer2_Device0_Stage1_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage1_Attention [label=Q_local]
	Layer2_Device0_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage1_Attention -> Layer2_Device0_Stage1_Accumulate
	Layer2_Device0_Stage0_Accumulate -> Layer2_Device0_Stage1_Accumulate
	Layer2_Device0_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage1_RecvKV -> Layer2_Device0_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage2_RecvKV -> Layer2_Device0_Stage2_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage2_Attention [label=Q_local]
	Layer2_Device0_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage2_Attention -> Layer2_Device0_Stage2_Accumulate
	Layer2_Device0_Stage1_Accumulate -> Layer2_Device0_Stage2_Accumulate
	Layer2_Device0_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage2_RecvKV -> Layer2_Device0_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage3_RecvKV -> Layer2_Device0_Stage3_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage3_Attention [label=Q_local]
	Layer2_Device0_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage3_Attention -> Layer2_Device0_Stage3_Accumulate
	Layer2_Device0_Stage2_Accumulate -> Layer2_Device0_Stage3_Accumulate
	Layer2_Device0_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage3_RecvKV -> Layer2_Device0_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage4_RecvKV -> Layer2_Device0_Stage4_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage4_Attention [label=Q_local]
	Layer2_Device0_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage4_Attention -> Layer2_Device0_Stage4_Accumulate
	Layer2_Device0_Stage3_Accumulate -> Layer2_Device0_Stage4_Accumulate
	Layer2_Device0_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage4_RecvKV -> Layer2_Device0_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage5_RecvKV -> Layer2_Device0_Stage5_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage5_Attention [label=Q_local]
	Layer2_Device0_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage5_Attention -> Layer2_Device0_Stage5_Accumulate
	Layer2_Device0_Stage4_Accumulate -> Layer2_Device0_Stage5_Accumulate
	Layer2_Device0_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage5_RecvKV -> Layer2_Device0_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage6_RecvKV -> Layer2_Device0_Stage6_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage6_Attention [label=Q_local]
	Layer2_Device0_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage6_Attention -> Layer2_Device0_Stage6_Accumulate
	Layer2_Device0_Stage5_Accumulate -> Layer2_Device0_Stage6_Accumulate
	Layer2_Device0_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage6_RecvKV -> Layer2_Device0_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage7_RecvKV -> Layer2_Device0_Stage7_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage7_Attention [label=Q_local]
	Layer2_Device0_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage7_Attention -> Layer2_Device0_Stage7_Accumulate
	Layer2_Device0_Stage6_Accumulate -> Layer2_Device0_Stage7_Accumulate
	Layer2_Device0_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage7_RecvKV -> Layer2_Device0_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage8_RecvKV -> Layer2_Device0_Stage8_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage8_Attention [label=Q_local]
	Layer2_Device0_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage8_Attention -> Layer2_Device0_Stage8_Accumulate
	Layer2_Device0_Stage7_Accumulate -> Layer2_Device0_Stage8_Accumulate
	Layer2_Device0_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage8_RecvKV -> Layer2_Device0_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage9_RecvKV -> Layer2_Device0_Stage9_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage9_Attention [label=Q_local]
	Layer2_Device0_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage9_Attention -> Layer2_Device0_Stage9_Accumulate
	Layer2_Device0_Stage8_Accumulate -> Layer2_Device0_Stage9_Accumulate
	Layer2_Device0_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage9_RecvKV -> Layer2_Device0_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage10_RecvKV -> Layer2_Device0_Stage10_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage10_Attention [label=Q_local]
	Layer2_Device0_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage10_Attention -> Layer2_Device0_Stage10_Accumulate
	Layer2_Device0_Stage9_Accumulate -> Layer2_Device0_Stage10_Accumulate
	Layer2_Device0_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage10_RecvKV -> Layer2_Device0_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage11_RecvKV -> Layer2_Device0_Stage11_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage11_Attention [label=Q_local]
	Layer2_Device0_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage11_Attention -> Layer2_Device0_Stage11_Accumulate
	Layer2_Device0_Stage10_Accumulate -> Layer2_Device0_Stage11_Accumulate
	Layer2_Device0_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage11_RecvKV -> Layer2_Device0_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage12_RecvKV -> Layer2_Device0_Stage12_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage12_Attention [label=Q_local]
	Layer2_Device0_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage12_Attention -> Layer2_Device0_Stage12_Accumulate
	Layer2_Device0_Stage11_Accumulate -> Layer2_Device0_Stage12_Accumulate
	Layer2_Device0_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage12_RecvKV -> Layer2_Device0_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage13_RecvKV -> Layer2_Device0_Stage13_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage13_Attention [label=Q_local]
	Layer2_Device0_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage13_Attention -> Layer2_Device0_Stage13_Accumulate
	Layer2_Device0_Stage12_Accumulate -> Layer2_Device0_Stage13_Accumulate
	Layer2_Device0_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage13_RecvKV -> Layer2_Device0_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage14_RecvKV -> Layer2_Device0_Stage14_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage14_Attention [label=Q_local]
	Layer2_Device0_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage14_Attention -> Layer2_Device0_Stage14_Accumulate
	Layer2_Device0_Stage13_Accumulate -> Layer2_Device0_Stage14_Accumulate
	Layer2_Device0_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device0_Stage14_RecvKV -> Layer2_Device0_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device0_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device0_Stage15_RecvKV -> Layer2_Device0_Stage15_Attention
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage15_Attention [label=Q_local]
	Layer2_Device0_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device0_Stage15_Attention -> Layer2_Device0_Stage15_Accumulate
	Layer2_Device0_Stage14_Accumulate -> Layer2_Device0_Stage15_Accumulate
	Layer2_Device0_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device0_Stage15_Accumulate -> Layer2_Device0_ConcatHeads
	Layer2_Device0_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device0_ConcatHeads -> Layer2_Device0_OutputProj
	Layer2_Device0_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device0_OutputProj -> Layer2_Device0_Residual1
	Layer2_Device0_Input -> Layer2_Device0_Residual1
	Layer2_Device0_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device0_Residual1 -> Layer2_Device0_LayerNorm2
	Layer2_Device0_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device0_LayerNorm2 -> Layer2_Device0_GateProj
	Layer2_Device0_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device0_LayerNorm2 -> Layer2_Device0_UpProj
	Layer2_Device0_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device0_GateProj -> Layer2_Device0_Activation
	Layer2_Device0_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device0_Activation -> Layer2_Device0_ElemMul
	Layer2_Device0_UpProj -> Layer2_Device0_ElemMul
	Layer2_Device0_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device0_ElemMul -> Layer2_Device0_DownProj
	Layer2_Device0_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device0_DownProj -> Layer2_Device0_Residual2
	Layer2_Device0_Residual1 -> Layer2_Device0_Residual2
	Layer2_Device0_Output [label="Layer 2 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device0_Residual2 -> Layer2_Device0_Output
	Layer2_Device1_Input [label="Layer 2 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device1_Output -> Layer2_Device1_Input
	Layer2_Device1_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device1_Input -> Layer2_Device1_LayerNorm1
	Layer2_Device1_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device1_LayerNorm1 -> Layer2_Device1_QKVProj
	Layer2_Device1_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device1_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage0_RecvKV -> Layer2_Device1_Stage0_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage0_Attention [label=Q_local]
	Layer2_Device1_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage0_Attention -> Layer2_Device1_Stage0_Accumulate
	Layer2_Device1_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage0_RecvKV -> Layer2_Device1_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage1_RecvKV -> Layer2_Device1_Stage1_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage1_Attention [label=Q_local]
	Layer2_Device1_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage1_Attention -> Layer2_Device1_Stage1_Accumulate
	Layer2_Device1_Stage0_Accumulate -> Layer2_Device1_Stage1_Accumulate
	Layer2_Device1_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage1_RecvKV -> Layer2_Device1_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage2_RecvKV -> Layer2_Device1_Stage2_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage2_Attention [label=Q_local]
	Layer2_Device1_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage2_Attention -> Layer2_Device1_Stage2_Accumulate
	Layer2_Device1_Stage1_Accumulate -> Layer2_Device1_Stage2_Accumulate
	Layer2_Device1_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage2_RecvKV -> Layer2_Device1_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage3_RecvKV -> Layer2_Device1_Stage3_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage3_Attention [label=Q_local]
	Layer2_Device1_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage3_Attention -> Layer2_Device1_Stage3_Accumulate
	Layer2_Device1_Stage2_Accumulate -> Layer2_Device1_Stage3_Accumulate
	Layer2_Device1_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage3_RecvKV -> Layer2_Device1_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage4_RecvKV -> Layer2_Device1_Stage4_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage4_Attention [label=Q_local]
	Layer2_Device1_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage4_Attention -> Layer2_Device1_Stage4_Accumulate
	Layer2_Device1_Stage3_Accumulate -> Layer2_Device1_Stage4_Accumulate
	Layer2_Device1_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage4_RecvKV -> Layer2_Device1_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage5_RecvKV -> Layer2_Device1_Stage5_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage5_Attention [label=Q_local]
	Layer2_Device1_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage5_Attention -> Layer2_Device1_Stage5_Accumulate
	Layer2_Device1_Stage4_Accumulate -> Layer2_Device1_Stage5_Accumulate
	Layer2_Device1_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage5_RecvKV -> Layer2_Device1_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage6_RecvKV -> Layer2_Device1_Stage6_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage6_Attention [label=Q_local]
	Layer2_Device1_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage6_Attention -> Layer2_Device1_Stage6_Accumulate
	Layer2_Device1_Stage5_Accumulate -> Layer2_Device1_Stage6_Accumulate
	Layer2_Device1_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage6_RecvKV -> Layer2_Device1_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage7_RecvKV -> Layer2_Device1_Stage7_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage7_Attention [label=Q_local]
	Layer2_Device1_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage7_Attention -> Layer2_Device1_Stage7_Accumulate
	Layer2_Device1_Stage6_Accumulate -> Layer2_Device1_Stage7_Accumulate
	Layer2_Device1_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage7_RecvKV -> Layer2_Device1_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage8_RecvKV -> Layer2_Device1_Stage8_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage8_Attention [label=Q_local]
	Layer2_Device1_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage8_Attention -> Layer2_Device1_Stage8_Accumulate
	Layer2_Device1_Stage7_Accumulate -> Layer2_Device1_Stage8_Accumulate
	Layer2_Device1_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage8_RecvKV -> Layer2_Device1_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage9_RecvKV -> Layer2_Device1_Stage9_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage9_Attention [label=Q_local]
	Layer2_Device1_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage9_Attention -> Layer2_Device1_Stage9_Accumulate
	Layer2_Device1_Stage8_Accumulate -> Layer2_Device1_Stage9_Accumulate
	Layer2_Device1_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage9_RecvKV -> Layer2_Device1_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage10_RecvKV -> Layer2_Device1_Stage10_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage10_Attention [label=Q_local]
	Layer2_Device1_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage10_Attention -> Layer2_Device1_Stage10_Accumulate
	Layer2_Device1_Stage9_Accumulate -> Layer2_Device1_Stage10_Accumulate
	Layer2_Device1_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage10_RecvKV -> Layer2_Device1_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage11_RecvKV -> Layer2_Device1_Stage11_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage11_Attention [label=Q_local]
	Layer2_Device1_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage11_Attention -> Layer2_Device1_Stage11_Accumulate
	Layer2_Device1_Stage10_Accumulate -> Layer2_Device1_Stage11_Accumulate
	Layer2_Device1_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage11_RecvKV -> Layer2_Device1_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage12_RecvKV -> Layer2_Device1_Stage12_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage12_Attention [label=Q_local]
	Layer2_Device1_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage12_Attention -> Layer2_Device1_Stage12_Accumulate
	Layer2_Device1_Stage11_Accumulate -> Layer2_Device1_Stage12_Accumulate
	Layer2_Device1_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage12_RecvKV -> Layer2_Device1_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage13_RecvKV -> Layer2_Device1_Stage13_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage13_Attention [label=Q_local]
	Layer2_Device1_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage13_Attention -> Layer2_Device1_Stage13_Accumulate
	Layer2_Device1_Stage12_Accumulate -> Layer2_Device1_Stage13_Accumulate
	Layer2_Device1_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage13_RecvKV -> Layer2_Device1_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage14_RecvKV -> Layer2_Device1_Stage14_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage14_Attention [label=Q_local]
	Layer2_Device1_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage14_Attention -> Layer2_Device1_Stage14_Accumulate
	Layer2_Device1_Stage13_Accumulate -> Layer2_Device1_Stage14_Accumulate
	Layer2_Device1_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device1_Stage14_RecvKV -> Layer2_Device1_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device1_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device1_Stage15_RecvKV -> Layer2_Device1_Stage15_Attention
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage15_Attention [label=Q_local]
	Layer2_Device1_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device1_Stage15_Attention -> Layer2_Device1_Stage15_Accumulate
	Layer2_Device1_Stage14_Accumulate -> Layer2_Device1_Stage15_Accumulate
	Layer2_Device1_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device1_Stage15_Accumulate -> Layer2_Device1_ConcatHeads
	Layer2_Device1_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device1_ConcatHeads -> Layer2_Device1_OutputProj
	Layer2_Device1_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device1_OutputProj -> Layer2_Device1_Residual1
	Layer2_Device1_Input -> Layer2_Device1_Residual1
	Layer2_Device1_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device1_Residual1 -> Layer2_Device1_LayerNorm2
	Layer2_Device1_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device1_LayerNorm2 -> Layer2_Device1_GateProj
	Layer2_Device1_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device1_LayerNorm2 -> Layer2_Device1_UpProj
	Layer2_Device1_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device1_GateProj -> Layer2_Device1_Activation
	Layer2_Device1_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device1_Activation -> Layer2_Device1_ElemMul
	Layer2_Device1_UpProj -> Layer2_Device1_ElemMul
	Layer2_Device1_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device1_ElemMul -> Layer2_Device1_DownProj
	Layer2_Device1_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device1_DownProj -> Layer2_Device1_Residual2
	Layer2_Device1_Residual1 -> Layer2_Device1_Residual2
	Layer2_Device1_Output [label="Layer 2 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device1_Residual2 -> Layer2_Device1_Output
	Layer2_Device2_Input [label="Layer 2 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device2_Output -> Layer2_Device2_Input
	Layer2_Device2_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device2_Input -> Layer2_Device2_LayerNorm1
	Layer2_Device2_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device2_LayerNorm1 -> Layer2_Device2_QKVProj
	Layer2_Device2_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device2_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage0_RecvKV -> Layer2_Device2_Stage0_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage0_Attention [label=Q_local]
	Layer2_Device2_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage0_Attention -> Layer2_Device2_Stage0_Accumulate
	Layer2_Device2_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage0_RecvKV -> Layer2_Device2_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage1_RecvKV -> Layer2_Device2_Stage1_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage1_Attention [label=Q_local]
	Layer2_Device2_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage1_Attention -> Layer2_Device2_Stage1_Accumulate
	Layer2_Device2_Stage0_Accumulate -> Layer2_Device2_Stage1_Accumulate
	Layer2_Device2_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage1_RecvKV -> Layer2_Device2_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage2_RecvKV -> Layer2_Device2_Stage2_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage2_Attention [label=Q_local]
	Layer2_Device2_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage2_Attention -> Layer2_Device2_Stage2_Accumulate
	Layer2_Device2_Stage1_Accumulate -> Layer2_Device2_Stage2_Accumulate
	Layer2_Device2_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage2_RecvKV -> Layer2_Device2_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage3_RecvKV -> Layer2_Device2_Stage3_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage3_Attention [label=Q_local]
	Layer2_Device2_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage3_Attention -> Layer2_Device2_Stage3_Accumulate
	Layer2_Device2_Stage2_Accumulate -> Layer2_Device2_Stage3_Accumulate
	Layer2_Device2_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage3_RecvKV -> Layer2_Device2_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage4_RecvKV -> Layer2_Device2_Stage4_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage4_Attention [label=Q_local]
	Layer2_Device2_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage4_Attention -> Layer2_Device2_Stage4_Accumulate
	Layer2_Device2_Stage3_Accumulate -> Layer2_Device2_Stage4_Accumulate
	Layer2_Device2_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage4_RecvKV -> Layer2_Device2_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage5_RecvKV -> Layer2_Device2_Stage5_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage5_Attention [label=Q_local]
	Layer2_Device2_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage5_Attention -> Layer2_Device2_Stage5_Accumulate
	Layer2_Device2_Stage4_Accumulate -> Layer2_Device2_Stage5_Accumulate
	Layer2_Device2_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage5_RecvKV -> Layer2_Device2_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage6_RecvKV -> Layer2_Device2_Stage6_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage6_Attention [label=Q_local]
	Layer2_Device2_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage6_Attention -> Layer2_Device2_Stage6_Accumulate
	Layer2_Device2_Stage5_Accumulate -> Layer2_Device2_Stage6_Accumulate
	Layer2_Device2_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage6_RecvKV -> Layer2_Device2_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage7_RecvKV -> Layer2_Device2_Stage7_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage7_Attention [label=Q_local]
	Layer2_Device2_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage7_Attention -> Layer2_Device2_Stage7_Accumulate
	Layer2_Device2_Stage6_Accumulate -> Layer2_Device2_Stage7_Accumulate
	Layer2_Device2_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage7_RecvKV -> Layer2_Device2_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage8_RecvKV -> Layer2_Device2_Stage8_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage8_Attention [label=Q_local]
	Layer2_Device2_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage8_Attention -> Layer2_Device2_Stage8_Accumulate
	Layer2_Device2_Stage7_Accumulate -> Layer2_Device2_Stage8_Accumulate
	Layer2_Device2_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage8_RecvKV -> Layer2_Device2_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage9_RecvKV -> Layer2_Device2_Stage9_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage9_Attention [label=Q_local]
	Layer2_Device2_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage9_Attention -> Layer2_Device2_Stage9_Accumulate
	Layer2_Device2_Stage8_Accumulate -> Layer2_Device2_Stage9_Accumulate
	Layer2_Device2_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage9_RecvKV -> Layer2_Device2_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage10_RecvKV -> Layer2_Device2_Stage10_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage10_Attention [label=Q_local]
	Layer2_Device2_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage10_Attention -> Layer2_Device2_Stage10_Accumulate
	Layer2_Device2_Stage9_Accumulate -> Layer2_Device2_Stage10_Accumulate
	Layer2_Device2_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage10_RecvKV -> Layer2_Device2_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage11_RecvKV -> Layer2_Device2_Stage11_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage11_Attention [label=Q_local]
	Layer2_Device2_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage11_Attention -> Layer2_Device2_Stage11_Accumulate
	Layer2_Device2_Stage10_Accumulate -> Layer2_Device2_Stage11_Accumulate
	Layer2_Device2_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage11_RecvKV -> Layer2_Device2_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage12_RecvKV -> Layer2_Device2_Stage12_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage12_Attention [label=Q_local]
	Layer2_Device2_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage12_Attention -> Layer2_Device2_Stage12_Accumulate
	Layer2_Device2_Stage11_Accumulate -> Layer2_Device2_Stage12_Accumulate
	Layer2_Device2_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage12_RecvKV -> Layer2_Device2_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage13_RecvKV -> Layer2_Device2_Stage13_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage13_Attention [label=Q_local]
	Layer2_Device2_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage13_Attention -> Layer2_Device2_Stage13_Accumulate
	Layer2_Device2_Stage12_Accumulate -> Layer2_Device2_Stage13_Accumulate
	Layer2_Device2_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage13_RecvKV -> Layer2_Device2_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage14_RecvKV -> Layer2_Device2_Stage14_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage14_Attention [label=Q_local]
	Layer2_Device2_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage14_Attention -> Layer2_Device2_Stage14_Accumulate
	Layer2_Device2_Stage13_Accumulate -> Layer2_Device2_Stage14_Accumulate
	Layer2_Device2_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device2_Stage14_RecvKV -> Layer2_Device2_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device2_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device2_Stage15_RecvKV -> Layer2_Device2_Stage15_Attention
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage15_Attention [label=Q_local]
	Layer2_Device2_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device2_Stage15_Attention -> Layer2_Device2_Stage15_Accumulate
	Layer2_Device2_Stage14_Accumulate -> Layer2_Device2_Stage15_Accumulate
	Layer2_Device2_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device2_Stage15_Accumulate -> Layer2_Device2_ConcatHeads
	Layer2_Device2_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device2_ConcatHeads -> Layer2_Device2_OutputProj
	Layer2_Device2_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device2_OutputProj -> Layer2_Device2_Residual1
	Layer2_Device2_Input -> Layer2_Device2_Residual1
	Layer2_Device2_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device2_Residual1 -> Layer2_Device2_LayerNorm2
	Layer2_Device2_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device2_LayerNorm2 -> Layer2_Device2_GateProj
	Layer2_Device2_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device2_LayerNorm2 -> Layer2_Device2_UpProj
	Layer2_Device2_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device2_GateProj -> Layer2_Device2_Activation
	Layer2_Device2_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device2_Activation -> Layer2_Device2_ElemMul
	Layer2_Device2_UpProj -> Layer2_Device2_ElemMul
	Layer2_Device2_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device2_ElemMul -> Layer2_Device2_DownProj
	Layer2_Device2_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device2_DownProj -> Layer2_Device2_Residual2
	Layer2_Device2_Residual1 -> Layer2_Device2_Residual2
	Layer2_Device2_Output [label="Layer 2 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device2_Residual2 -> Layer2_Device2_Output
	Layer2_Device3_Input [label="Layer 2 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device3_Output -> Layer2_Device3_Input
	Layer2_Device3_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device3_Input -> Layer2_Device3_LayerNorm1
	Layer2_Device3_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device3_LayerNorm1 -> Layer2_Device3_QKVProj
	Layer2_Device3_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device3_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage0_RecvKV -> Layer2_Device3_Stage0_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage0_Attention [label=Q_local]
	Layer2_Device3_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage0_Attention -> Layer2_Device3_Stage0_Accumulate
	Layer2_Device3_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage0_RecvKV -> Layer2_Device3_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage1_RecvKV -> Layer2_Device3_Stage1_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage1_Attention [label=Q_local]
	Layer2_Device3_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage1_Attention -> Layer2_Device3_Stage1_Accumulate
	Layer2_Device3_Stage0_Accumulate -> Layer2_Device3_Stage1_Accumulate
	Layer2_Device3_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage1_RecvKV -> Layer2_Device3_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage2_RecvKV -> Layer2_Device3_Stage2_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage2_Attention [label=Q_local]
	Layer2_Device3_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage2_Attention -> Layer2_Device3_Stage2_Accumulate
	Layer2_Device3_Stage1_Accumulate -> Layer2_Device3_Stage2_Accumulate
	Layer2_Device3_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage2_RecvKV -> Layer2_Device3_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage3_RecvKV -> Layer2_Device3_Stage3_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage3_Attention [label=Q_local]
	Layer2_Device3_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage3_Attention -> Layer2_Device3_Stage3_Accumulate
	Layer2_Device3_Stage2_Accumulate -> Layer2_Device3_Stage3_Accumulate
	Layer2_Device3_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage3_RecvKV -> Layer2_Device3_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage4_RecvKV -> Layer2_Device3_Stage4_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage4_Attention [label=Q_local]
	Layer2_Device3_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage4_Attention -> Layer2_Device3_Stage4_Accumulate
	Layer2_Device3_Stage3_Accumulate -> Layer2_Device3_Stage4_Accumulate
	Layer2_Device3_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage4_RecvKV -> Layer2_Device3_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage5_RecvKV -> Layer2_Device3_Stage5_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage5_Attention [label=Q_local]
	Layer2_Device3_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage5_Attention -> Layer2_Device3_Stage5_Accumulate
	Layer2_Device3_Stage4_Accumulate -> Layer2_Device3_Stage5_Accumulate
	Layer2_Device3_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage5_RecvKV -> Layer2_Device3_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage6_RecvKV -> Layer2_Device3_Stage6_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage6_Attention [label=Q_local]
	Layer2_Device3_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage6_Attention -> Layer2_Device3_Stage6_Accumulate
	Layer2_Device3_Stage5_Accumulate -> Layer2_Device3_Stage6_Accumulate
	Layer2_Device3_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage6_RecvKV -> Layer2_Device3_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage7_RecvKV -> Layer2_Device3_Stage7_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage7_Attention [label=Q_local]
	Layer2_Device3_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage7_Attention -> Layer2_Device3_Stage7_Accumulate
	Layer2_Device3_Stage6_Accumulate -> Layer2_Device3_Stage7_Accumulate
	Layer2_Device3_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage7_RecvKV -> Layer2_Device3_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage8_RecvKV -> Layer2_Device3_Stage8_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage8_Attention [label=Q_local]
	Layer2_Device3_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage8_Attention -> Layer2_Device3_Stage8_Accumulate
	Layer2_Device3_Stage7_Accumulate -> Layer2_Device3_Stage8_Accumulate
	Layer2_Device3_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage8_RecvKV -> Layer2_Device3_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage9_RecvKV -> Layer2_Device3_Stage9_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage9_Attention [label=Q_local]
	Layer2_Device3_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage9_Attention -> Layer2_Device3_Stage9_Accumulate
	Layer2_Device3_Stage8_Accumulate -> Layer2_Device3_Stage9_Accumulate
	Layer2_Device3_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage9_RecvKV -> Layer2_Device3_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage10_RecvKV -> Layer2_Device3_Stage10_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage10_Attention [label=Q_local]
	Layer2_Device3_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage10_Attention -> Layer2_Device3_Stage10_Accumulate
	Layer2_Device3_Stage9_Accumulate -> Layer2_Device3_Stage10_Accumulate
	Layer2_Device3_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage10_RecvKV -> Layer2_Device3_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage11_RecvKV -> Layer2_Device3_Stage11_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage11_Attention [label=Q_local]
	Layer2_Device3_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage11_Attention -> Layer2_Device3_Stage11_Accumulate
	Layer2_Device3_Stage10_Accumulate -> Layer2_Device3_Stage11_Accumulate
	Layer2_Device3_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage11_RecvKV -> Layer2_Device3_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage12_RecvKV -> Layer2_Device3_Stage12_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage12_Attention [label=Q_local]
	Layer2_Device3_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage12_Attention -> Layer2_Device3_Stage12_Accumulate
	Layer2_Device3_Stage11_Accumulate -> Layer2_Device3_Stage12_Accumulate
	Layer2_Device3_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage12_RecvKV -> Layer2_Device3_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage13_RecvKV -> Layer2_Device3_Stage13_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage13_Attention [label=Q_local]
	Layer2_Device3_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage13_Attention -> Layer2_Device3_Stage13_Accumulate
	Layer2_Device3_Stage12_Accumulate -> Layer2_Device3_Stage13_Accumulate
	Layer2_Device3_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage13_RecvKV -> Layer2_Device3_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage14_RecvKV -> Layer2_Device3_Stage14_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage14_Attention [label=Q_local]
	Layer2_Device3_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage14_Attention -> Layer2_Device3_Stage14_Accumulate
	Layer2_Device3_Stage13_Accumulate -> Layer2_Device3_Stage14_Accumulate
	Layer2_Device3_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device3_Stage14_RecvKV -> Layer2_Device3_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device3_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device3_Stage15_RecvKV -> Layer2_Device3_Stage15_Attention
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage15_Attention [label=Q_local]
	Layer2_Device3_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device3_Stage15_Attention -> Layer2_Device3_Stage15_Accumulate
	Layer2_Device3_Stage14_Accumulate -> Layer2_Device3_Stage15_Accumulate
	Layer2_Device3_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device3_Stage15_Accumulate -> Layer2_Device3_ConcatHeads
	Layer2_Device3_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device3_ConcatHeads -> Layer2_Device3_OutputProj
	Layer2_Device3_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device3_OutputProj -> Layer2_Device3_Residual1
	Layer2_Device3_Input -> Layer2_Device3_Residual1
	Layer2_Device3_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device3_Residual1 -> Layer2_Device3_LayerNorm2
	Layer2_Device3_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device3_LayerNorm2 -> Layer2_Device3_GateProj
	Layer2_Device3_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device3_LayerNorm2 -> Layer2_Device3_UpProj
	Layer2_Device3_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device3_GateProj -> Layer2_Device3_Activation
	Layer2_Device3_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device3_Activation -> Layer2_Device3_ElemMul
	Layer2_Device3_UpProj -> Layer2_Device3_ElemMul
	Layer2_Device3_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device3_ElemMul -> Layer2_Device3_DownProj
	Layer2_Device3_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device3_DownProj -> Layer2_Device3_Residual2
	Layer2_Device3_Residual1 -> Layer2_Device3_Residual2
	Layer2_Device3_Output [label="Layer 2 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device3_Residual2 -> Layer2_Device3_Output
	Layer2_Device4_Input [label="Layer 2 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device4_Output -> Layer2_Device4_Input
	Layer2_Device4_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device4_Input -> Layer2_Device4_LayerNorm1
	Layer2_Device4_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device4_LayerNorm1 -> Layer2_Device4_QKVProj
	Layer2_Device4_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device4_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage0_RecvKV -> Layer2_Device4_Stage0_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage0_Attention [label=Q_local]
	Layer2_Device4_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage0_Attention -> Layer2_Device4_Stage0_Accumulate
	Layer2_Device4_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage0_RecvKV -> Layer2_Device4_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage1_RecvKV -> Layer2_Device4_Stage1_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage1_Attention [label=Q_local]
	Layer2_Device4_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage1_Attention -> Layer2_Device4_Stage1_Accumulate
	Layer2_Device4_Stage0_Accumulate -> Layer2_Device4_Stage1_Accumulate
	Layer2_Device4_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage1_RecvKV -> Layer2_Device4_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage2_RecvKV -> Layer2_Device4_Stage2_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage2_Attention [label=Q_local]
	Layer2_Device4_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage2_Attention -> Layer2_Device4_Stage2_Accumulate
	Layer2_Device4_Stage1_Accumulate -> Layer2_Device4_Stage2_Accumulate
	Layer2_Device4_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage2_RecvKV -> Layer2_Device4_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage3_RecvKV -> Layer2_Device4_Stage3_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage3_Attention [label=Q_local]
	Layer2_Device4_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage3_Attention -> Layer2_Device4_Stage3_Accumulate
	Layer2_Device4_Stage2_Accumulate -> Layer2_Device4_Stage3_Accumulate
	Layer2_Device4_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage3_RecvKV -> Layer2_Device4_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage4_RecvKV -> Layer2_Device4_Stage4_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage4_Attention [label=Q_local]
	Layer2_Device4_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage4_Attention -> Layer2_Device4_Stage4_Accumulate
	Layer2_Device4_Stage3_Accumulate -> Layer2_Device4_Stage4_Accumulate
	Layer2_Device4_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage4_RecvKV -> Layer2_Device4_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage5_RecvKV -> Layer2_Device4_Stage5_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage5_Attention [label=Q_local]
	Layer2_Device4_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage5_Attention -> Layer2_Device4_Stage5_Accumulate
	Layer2_Device4_Stage4_Accumulate -> Layer2_Device4_Stage5_Accumulate
	Layer2_Device4_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage5_RecvKV -> Layer2_Device4_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage6_RecvKV -> Layer2_Device4_Stage6_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage6_Attention [label=Q_local]
	Layer2_Device4_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage6_Attention -> Layer2_Device4_Stage6_Accumulate
	Layer2_Device4_Stage5_Accumulate -> Layer2_Device4_Stage6_Accumulate
	Layer2_Device4_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage6_RecvKV -> Layer2_Device4_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage7_RecvKV -> Layer2_Device4_Stage7_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage7_Attention [label=Q_local]
	Layer2_Device4_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage7_Attention -> Layer2_Device4_Stage7_Accumulate
	Layer2_Device4_Stage6_Accumulate -> Layer2_Device4_Stage7_Accumulate
	Layer2_Device4_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage7_RecvKV -> Layer2_Device4_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage8_RecvKV -> Layer2_Device4_Stage8_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage8_Attention [label=Q_local]
	Layer2_Device4_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage8_Attention -> Layer2_Device4_Stage8_Accumulate
	Layer2_Device4_Stage7_Accumulate -> Layer2_Device4_Stage8_Accumulate
	Layer2_Device4_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage8_RecvKV -> Layer2_Device4_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage9_RecvKV -> Layer2_Device4_Stage9_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage9_Attention [label=Q_local]
	Layer2_Device4_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage9_Attention -> Layer2_Device4_Stage9_Accumulate
	Layer2_Device4_Stage8_Accumulate -> Layer2_Device4_Stage9_Accumulate
	Layer2_Device4_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage9_RecvKV -> Layer2_Device4_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage10_RecvKV -> Layer2_Device4_Stage10_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage10_Attention [label=Q_local]
	Layer2_Device4_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage10_Attention -> Layer2_Device4_Stage10_Accumulate
	Layer2_Device4_Stage9_Accumulate -> Layer2_Device4_Stage10_Accumulate
	Layer2_Device4_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage10_RecvKV -> Layer2_Device4_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage11_RecvKV -> Layer2_Device4_Stage11_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage11_Attention [label=Q_local]
	Layer2_Device4_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage11_Attention -> Layer2_Device4_Stage11_Accumulate
	Layer2_Device4_Stage10_Accumulate -> Layer2_Device4_Stage11_Accumulate
	Layer2_Device4_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage11_RecvKV -> Layer2_Device4_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage12_RecvKV -> Layer2_Device4_Stage12_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage12_Attention [label=Q_local]
	Layer2_Device4_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage12_Attention -> Layer2_Device4_Stage12_Accumulate
	Layer2_Device4_Stage11_Accumulate -> Layer2_Device4_Stage12_Accumulate
	Layer2_Device4_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage12_RecvKV -> Layer2_Device4_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage13_RecvKV -> Layer2_Device4_Stage13_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage13_Attention [label=Q_local]
	Layer2_Device4_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage13_Attention -> Layer2_Device4_Stage13_Accumulate
	Layer2_Device4_Stage12_Accumulate -> Layer2_Device4_Stage13_Accumulate
	Layer2_Device4_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage13_RecvKV -> Layer2_Device4_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage14_RecvKV -> Layer2_Device4_Stage14_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage14_Attention [label=Q_local]
	Layer2_Device4_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage14_Attention -> Layer2_Device4_Stage14_Accumulate
	Layer2_Device4_Stage13_Accumulate -> Layer2_Device4_Stage14_Accumulate
	Layer2_Device4_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device4_Stage14_RecvKV -> Layer2_Device4_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device4_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device4_Stage15_RecvKV -> Layer2_Device4_Stage15_Attention
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage15_Attention [label=Q_local]
	Layer2_Device4_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device4_Stage15_Attention -> Layer2_Device4_Stage15_Accumulate
	Layer2_Device4_Stage14_Accumulate -> Layer2_Device4_Stage15_Accumulate
	Layer2_Device4_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device4_Stage15_Accumulate -> Layer2_Device4_ConcatHeads
	Layer2_Device4_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device4_ConcatHeads -> Layer2_Device4_OutputProj
	Layer2_Device4_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device4_OutputProj -> Layer2_Device4_Residual1
	Layer2_Device4_Input -> Layer2_Device4_Residual1
	Layer2_Device4_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device4_Residual1 -> Layer2_Device4_LayerNorm2
	Layer2_Device4_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device4_LayerNorm2 -> Layer2_Device4_GateProj
	Layer2_Device4_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device4_LayerNorm2 -> Layer2_Device4_UpProj
	Layer2_Device4_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device4_GateProj -> Layer2_Device4_Activation
	Layer2_Device4_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device4_Activation -> Layer2_Device4_ElemMul
	Layer2_Device4_UpProj -> Layer2_Device4_ElemMul
	Layer2_Device4_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device4_ElemMul -> Layer2_Device4_DownProj
	Layer2_Device4_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device4_DownProj -> Layer2_Device4_Residual2
	Layer2_Device4_Residual1 -> Layer2_Device4_Residual2
	Layer2_Device4_Output [label="Layer 2 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device4_Residual2 -> Layer2_Device4_Output
	Layer2_Device5_Input [label="Layer 2 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device5_Output -> Layer2_Device5_Input
	Layer2_Device5_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device5_Input -> Layer2_Device5_LayerNorm1
	Layer2_Device5_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device5_LayerNorm1 -> Layer2_Device5_QKVProj
	Layer2_Device5_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device5_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage0_RecvKV -> Layer2_Device5_Stage0_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage0_Attention [label=Q_local]
	Layer2_Device5_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage0_Attention -> Layer2_Device5_Stage0_Accumulate
	Layer2_Device5_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage0_RecvKV -> Layer2_Device5_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage1_RecvKV -> Layer2_Device5_Stage1_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage1_Attention [label=Q_local]
	Layer2_Device5_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage1_Attention -> Layer2_Device5_Stage1_Accumulate
	Layer2_Device5_Stage0_Accumulate -> Layer2_Device5_Stage1_Accumulate
	Layer2_Device5_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage1_RecvKV -> Layer2_Device5_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage2_RecvKV -> Layer2_Device5_Stage2_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage2_Attention [label=Q_local]
	Layer2_Device5_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage2_Attention -> Layer2_Device5_Stage2_Accumulate
	Layer2_Device5_Stage1_Accumulate -> Layer2_Device5_Stage2_Accumulate
	Layer2_Device5_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage2_RecvKV -> Layer2_Device5_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage3_RecvKV -> Layer2_Device5_Stage3_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage3_Attention [label=Q_local]
	Layer2_Device5_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage3_Attention -> Layer2_Device5_Stage3_Accumulate
	Layer2_Device5_Stage2_Accumulate -> Layer2_Device5_Stage3_Accumulate
	Layer2_Device5_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage3_RecvKV -> Layer2_Device5_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage4_RecvKV -> Layer2_Device5_Stage4_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage4_Attention [label=Q_local]
	Layer2_Device5_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage4_Attention -> Layer2_Device5_Stage4_Accumulate
	Layer2_Device5_Stage3_Accumulate -> Layer2_Device5_Stage4_Accumulate
	Layer2_Device5_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage4_RecvKV -> Layer2_Device5_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage5_RecvKV -> Layer2_Device5_Stage5_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage5_Attention [label=Q_local]
	Layer2_Device5_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage5_Attention -> Layer2_Device5_Stage5_Accumulate
	Layer2_Device5_Stage4_Accumulate -> Layer2_Device5_Stage5_Accumulate
	Layer2_Device5_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage5_RecvKV -> Layer2_Device5_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage6_RecvKV -> Layer2_Device5_Stage6_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage6_Attention [label=Q_local]
	Layer2_Device5_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage6_Attention -> Layer2_Device5_Stage6_Accumulate
	Layer2_Device5_Stage5_Accumulate -> Layer2_Device5_Stage6_Accumulate
	Layer2_Device5_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage6_RecvKV -> Layer2_Device5_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage7_RecvKV -> Layer2_Device5_Stage7_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage7_Attention [label=Q_local]
	Layer2_Device5_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage7_Attention -> Layer2_Device5_Stage7_Accumulate
	Layer2_Device5_Stage6_Accumulate -> Layer2_Device5_Stage7_Accumulate
	Layer2_Device5_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage7_RecvKV -> Layer2_Device5_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage8_RecvKV -> Layer2_Device5_Stage8_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage8_Attention [label=Q_local]
	Layer2_Device5_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage8_Attention -> Layer2_Device5_Stage8_Accumulate
	Layer2_Device5_Stage7_Accumulate -> Layer2_Device5_Stage8_Accumulate
	Layer2_Device5_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage8_RecvKV -> Layer2_Device5_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage9_RecvKV -> Layer2_Device5_Stage9_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage9_Attention [label=Q_local]
	Layer2_Device5_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage9_Attention -> Layer2_Device5_Stage9_Accumulate
	Layer2_Device5_Stage8_Accumulate -> Layer2_Device5_Stage9_Accumulate
	Layer2_Device5_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage9_RecvKV -> Layer2_Device5_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage10_RecvKV -> Layer2_Device5_Stage10_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage10_Attention [label=Q_local]
	Layer2_Device5_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage10_Attention -> Layer2_Device5_Stage10_Accumulate
	Layer2_Device5_Stage9_Accumulate -> Layer2_Device5_Stage10_Accumulate
	Layer2_Device5_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage10_RecvKV -> Layer2_Device5_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage11_RecvKV -> Layer2_Device5_Stage11_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage11_Attention [label=Q_local]
	Layer2_Device5_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage11_Attention -> Layer2_Device5_Stage11_Accumulate
	Layer2_Device5_Stage10_Accumulate -> Layer2_Device5_Stage11_Accumulate
	Layer2_Device5_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage11_RecvKV -> Layer2_Device5_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage12_RecvKV -> Layer2_Device5_Stage12_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage12_Attention [label=Q_local]
	Layer2_Device5_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage12_Attention -> Layer2_Device5_Stage12_Accumulate
	Layer2_Device5_Stage11_Accumulate -> Layer2_Device5_Stage12_Accumulate
	Layer2_Device5_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage12_RecvKV -> Layer2_Device5_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage13_RecvKV -> Layer2_Device5_Stage13_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage13_Attention [label=Q_local]
	Layer2_Device5_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage13_Attention -> Layer2_Device5_Stage13_Accumulate
	Layer2_Device5_Stage12_Accumulate -> Layer2_Device5_Stage13_Accumulate
	Layer2_Device5_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage13_RecvKV -> Layer2_Device5_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage14_RecvKV -> Layer2_Device5_Stage14_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage14_Attention [label=Q_local]
	Layer2_Device5_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage14_Attention -> Layer2_Device5_Stage14_Accumulate
	Layer2_Device5_Stage13_Accumulate -> Layer2_Device5_Stage14_Accumulate
	Layer2_Device5_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device5_Stage14_RecvKV -> Layer2_Device5_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device5_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device5_Stage15_RecvKV -> Layer2_Device5_Stage15_Attention
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage15_Attention [label=Q_local]
	Layer2_Device5_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device5_Stage15_Attention -> Layer2_Device5_Stage15_Accumulate
	Layer2_Device5_Stage14_Accumulate -> Layer2_Device5_Stage15_Accumulate
	Layer2_Device5_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device5_Stage15_Accumulate -> Layer2_Device5_ConcatHeads
	Layer2_Device5_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device5_ConcatHeads -> Layer2_Device5_OutputProj
	Layer2_Device5_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device5_OutputProj -> Layer2_Device5_Residual1
	Layer2_Device5_Input -> Layer2_Device5_Residual1
	Layer2_Device5_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device5_Residual1 -> Layer2_Device5_LayerNorm2
	Layer2_Device5_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device5_LayerNorm2 -> Layer2_Device5_GateProj
	Layer2_Device5_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device5_LayerNorm2 -> Layer2_Device5_UpProj
	Layer2_Device5_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device5_GateProj -> Layer2_Device5_Activation
	Layer2_Device5_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device5_Activation -> Layer2_Device5_ElemMul
	Layer2_Device5_UpProj -> Layer2_Device5_ElemMul
	Layer2_Device5_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device5_ElemMul -> Layer2_Device5_DownProj
	Layer2_Device5_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device5_DownProj -> Layer2_Device5_Residual2
	Layer2_Device5_Residual1 -> Layer2_Device5_Residual2
	Layer2_Device5_Output [label="Layer 2 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device5_Residual2 -> Layer2_Device5_Output
	Layer2_Device6_Input [label="Layer 2 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device6_Output -> Layer2_Device6_Input
	Layer2_Device6_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device6_Input -> Layer2_Device6_LayerNorm1
	Layer2_Device6_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device6_LayerNorm1 -> Layer2_Device6_QKVProj
	Layer2_Device6_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device6_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage0_RecvKV -> Layer2_Device6_Stage0_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage0_Attention [label=Q_local]
	Layer2_Device6_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage0_Attention -> Layer2_Device6_Stage0_Accumulate
	Layer2_Device6_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage0_RecvKV -> Layer2_Device6_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage1_RecvKV -> Layer2_Device6_Stage1_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage1_Attention [label=Q_local]
	Layer2_Device6_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage1_Attention -> Layer2_Device6_Stage1_Accumulate
	Layer2_Device6_Stage0_Accumulate -> Layer2_Device6_Stage1_Accumulate
	Layer2_Device6_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage1_RecvKV -> Layer2_Device6_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage2_RecvKV -> Layer2_Device6_Stage2_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage2_Attention [label=Q_local]
	Layer2_Device6_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage2_Attention -> Layer2_Device6_Stage2_Accumulate
	Layer2_Device6_Stage1_Accumulate -> Layer2_Device6_Stage2_Accumulate
	Layer2_Device6_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage2_RecvKV -> Layer2_Device6_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage3_RecvKV -> Layer2_Device6_Stage3_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage3_Attention [label=Q_local]
	Layer2_Device6_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage3_Attention -> Layer2_Device6_Stage3_Accumulate
	Layer2_Device6_Stage2_Accumulate -> Layer2_Device6_Stage3_Accumulate
	Layer2_Device6_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage3_RecvKV -> Layer2_Device6_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage4_RecvKV -> Layer2_Device6_Stage4_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage4_Attention [label=Q_local]
	Layer2_Device6_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage4_Attention -> Layer2_Device6_Stage4_Accumulate
	Layer2_Device6_Stage3_Accumulate -> Layer2_Device6_Stage4_Accumulate
	Layer2_Device6_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage4_RecvKV -> Layer2_Device6_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage5_RecvKV -> Layer2_Device6_Stage5_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage5_Attention [label=Q_local]
	Layer2_Device6_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage5_Attention -> Layer2_Device6_Stage5_Accumulate
	Layer2_Device6_Stage4_Accumulate -> Layer2_Device6_Stage5_Accumulate
	Layer2_Device6_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage5_RecvKV -> Layer2_Device6_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage6_RecvKV -> Layer2_Device6_Stage6_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage6_Attention [label=Q_local]
	Layer2_Device6_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage6_Attention -> Layer2_Device6_Stage6_Accumulate
	Layer2_Device6_Stage5_Accumulate -> Layer2_Device6_Stage6_Accumulate
	Layer2_Device6_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage6_RecvKV -> Layer2_Device6_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage7_RecvKV -> Layer2_Device6_Stage7_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage7_Attention [label=Q_local]
	Layer2_Device6_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage7_Attention -> Layer2_Device6_Stage7_Accumulate
	Layer2_Device6_Stage6_Accumulate -> Layer2_Device6_Stage7_Accumulate
	Layer2_Device6_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage7_RecvKV -> Layer2_Device6_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage8_RecvKV -> Layer2_Device6_Stage8_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage8_Attention [label=Q_local]
	Layer2_Device6_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage8_Attention -> Layer2_Device6_Stage8_Accumulate
	Layer2_Device6_Stage7_Accumulate -> Layer2_Device6_Stage8_Accumulate
	Layer2_Device6_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage8_RecvKV -> Layer2_Device6_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage9_RecvKV -> Layer2_Device6_Stage9_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage9_Attention [label=Q_local]
	Layer2_Device6_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage9_Attention -> Layer2_Device6_Stage9_Accumulate
	Layer2_Device6_Stage8_Accumulate -> Layer2_Device6_Stage9_Accumulate
	Layer2_Device6_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage9_RecvKV -> Layer2_Device6_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage10_RecvKV -> Layer2_Device6_Stage10_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage10_Attention [label=Q_local]
	Layer2_Device6_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage10_Attention -> Layer2_Device6_Stage10_Accumulate
	Layer2_Device6_Stage9_Accumulate -> Layer2_Device6_Stage10_Accumulate
	Layer2_Device6_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage10_RecvKV -> Layer2_Device6_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage11_RecvKV -> Layer2_Device6_Stage11_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage11_Attention [label=Q_local]
	Layer2_Device6_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage11_Attention -> Layer2_Device6_Stage11_Accumulate
	Layer2_Device6_Stage10_Accumulate -> Layer2_Device6_Stage11_Accumulate
	Layer2_Device6_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage11_RecvKV -> Layer2_Device6_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage12_RecvKV -> Layer2_Device6_Stage12_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage12_Attention [label=Q_local]
	Layer2_Device6_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage12_Attention -> Layer2_Device6_Stage12_Accumulate
	Layer2_Device6_Stage11_Accumulate -> Layer2_Device6_Stage12_Accumulate
	Layer2_Device6_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage12_RecvKV -> Layer2_Device6_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage13_RecvKV -> Layer2_Device6_Stage13_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage13_Attention [label=Q_local]
	Layer2_Device6_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage13_Attention -> Layer2_Device6_Stage13_Accumulate
	Layer2_Device6_Stage12_Accumulate -> Layer2_Device6_Stage13_Accumulate
	Layer2_Device6_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage13_RecvKV -> Layer2_Device6_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage14_RecvKV -> Layer2_Device6_Stage14_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage14_Attention [label=Q_local]
	Layer2_Device6_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage14_Attention -> Layer2_Device6_Stage14_Accumulate
	Layer2_Device6_Stage13_Accumulate -> Layer2_Device6_Stage14_Accumulate
	Layer2_Device6_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device6_Stage14_RecvKV -> Layer2_Device6_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device6_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device6_Stage15_RecvKV -> Layer2_Device6_Stage15_Attention
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage15_Attention [label=Q_local]
	Layer2_Device6_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device6_Stage15_Attention -> Layer2_Device6_Stage15_Accumulate
	Layer2_Device6_Stage14_Accumulate -> Layer2_Device6_Stage15_Accumulate
	Layer2_Device6_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device6_Stage15_Accumulate -> Layer2_Device6_ConcatHeads
	Layer2_Device6_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device6_ConcatHeads -> Layer2_Device6_OutputProj
	Layer2_Device6_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device6_OutputProj -> Layer2_Device6_Residual1
	Layer2_Device6_Input -> Layer2_Device6_Residual1
	Layer2_Device6_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device6_Residual1 -> Layer2_Device6_LayerNorm2
	Layer2_Device6_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device6_LayerNorm2 -> Layer2_Device6_GateProj
	Layer2_Device6_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device6_LayerNorm2 -> Layer2_Device6_UpProj
	Layer2_Device6_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device6_GateProj -> Layer2_Device6_Activation
	Layer2_Device6_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device6_Activation -> Layer2_Device6_ElemMul
	Layer2_Device6_UpProj -> Layer2_Device6_ElemMul
	Layer2_Device6_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device6_ElemMul -> Layer2_Device6_DownProj
	Layer2_Device6_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device6_DownProj -> Layer2_Device6_Residual2
	Layer2_Device6_Residual1 -> Layer2_Device6_Residual2
	Layer2_Device6_Output [label="Layer 2 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device6_Residual2 -> Layer2_Device6_Output
	Layer2_Device7_Input [label="Layer 2 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device7_Output -> Layer2_Device7_Input
	Layer2_Device7_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device7_Input -> Layer2_Device7_LayerNorm1
	Layer2_Device7_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device7_LayerNorm1 -> Layer2_Device7_QKVProj
	Layer2_Device7_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device7_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage0_RecvKV -> Layer2_Device7_Stage0_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage0_Attention [label=Q_local]
	Layer2_Device7_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage0_Attention -> Layer2_Device7_Stage0_Accumulate
	Layer2_Device7_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage0_RecvKV -> Layer2_Device7_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage1_RecvKV -> Layer2_Device7_Stage1_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage1_Attention [label=Q_local]
	Layer2_Device7_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage1_Attention -> Layer2_Device7_Stage1_Accumulate
	Layer2_Device7_Stage0_Accumulate -> Layer2_Device7_Stage1_Accumulate
	Layer2_Device7_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage1_RecvKV -> Layer2_Device7_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage2_RecvKV -> Layer2_Device7_Stage2_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage2_Attention [label=Q_local]
	Layer2_Device7_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage2_Attention -> Layer2_Device7_Stage2_Accumulate
	Layer2_Device7_Stage1_Accumulate -> Layer2_Device7_Stage2_Accumulate
	Layer2_Device7_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage2_RecvKV -> Layer2_Device7_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage3_RecvKV -> Layer2_Device7_Stage3_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage3_Attention [label=Q_local]
	Layer2_Device7_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage3_Attention -> Layer2_Device7_Stage3_Accumulate
	Layer2_Device7_Stage2_Accumulate -> Layer2_Device7_Stage3_Accumulate
	Layer2_Device7_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage3_RecvKV -> Layer2_Device7_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage4_RecvKV -> Layer2_Device7_Stage4_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage4_Attention [label=Q_local]
	Layer2_Device7_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage4_Attention -> Layer2_Device7_Stage4_Accumulate
	Layer2_Device7_Stage3_Accumulate -> Layer2_Device7_Stage4_Accumulate
	Layer2_Device7_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage4_RecvKV -> Layer2_Device7_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage5_RecvKV -> Layer2_Device7_Stage5_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage5_Attention [label=Q_local]
	Layer2_Device7_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage5_Attention -> Layer2_Device7_Stage5_Accumulate
	Layer2_Device7_Stage4_Accumulate -> Layer2_Device7_Stage5_Accumulate
	Layer2_Device7_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage5_RecvKV -> Layer2_Device7_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage6_RecvKV -> Layer2_Device7_Stage6_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage6_Attention [label=Q_local]
	Layer2_Device7_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage6_Attention -> Layer2_Device7_Stage6_Accumulate
	Layer2_Device7_Stage5_Accumulate -> Layer2_Device7_Stage6_Accumulate
	Layer2_Device7_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage6_RecvKV -> Layer2_Device7_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage7_RecvKV -> Layer2_Device7_Stage7_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage7_Attention [label=Q_local]
	Layer2_Device7_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage7_Attention -> Layer2_Device7_Stage7_Accumulate
	Layer2_Device7_Stage6_Accumulate -> Layer2_Device7_Stage7_Accumulate
	Layer2_Device7_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage7_RecvKV -> Layer2_Device7_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage8_RecvKV -> Layer2_Device7_Stage8_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage8_Attention [label=Q_local]
	Layer2_Device7_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage8_Attention -> Layer2_Device7_Stage8_Accumulate
	Layer2_Device7_Stage7_Accumulate -> Layer2_Device7_Stage8_Accumulate
	Layer2_Device7_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage8_RecvKV -> Layer2_Device7_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage9_RecvKV -> Layer2_Device7_Stage9_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage9_Attention [label=Q_local]
	Layer2_Device7_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage9_Attention -> Layer2_Device7_Stage9_Accumulate
	Layer2_Device7_Stage8_Accumulate -> Layer2_Device7_Stage9_Accumulate
	Layer2_Device7_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage9_RecvKV -> Layer2_Device7_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage10_RecvKV -> Layer2_Device7_Stage10_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage10_Attention [label=Q_local]
	Layer2_Device7_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage10_Attention -> Layer2_Device7_Stage10_Accumulate
	Layer2_Device7_Stage9_Accumulate -> Layer2_Device7_Stage10_Accumulate
	Layer2_Device7_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage10_RecvKV -> Layer2_Device7_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage11_RecvKV -> Layer2_Device7_Stage11_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage11_Attention [label=Q_local]
	Layer2_Device7_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage11_Attention -> Layer2_Device7_Stage11_Accumulate
	Layer2_Device7_Stage10_Accumulate -> Layer2_Device7_Stage11_Accumulate
	Layer2_Device7_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage11_RecvKV -> Layer2_Device7_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage12_RecvKV -> Layer2_Device7_Stage12_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage12_Attention [label=Q_local]
	Layer2_Device7_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage12_Attention -> Layer2_Device7_Stage12_Accumulate
	Layer2_Device7_Stage11_Accumulate -> Layer2_Device7_Stage12_Accumulate
	Layer2_Device7_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage12_RecvKV -> Layer2_Device7_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage13_RecvKV -> Layer2_Device7_Stage13_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage13_Attention [label=Q_local]
	Layer2_Device7_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage13_Attention -> Layer2_Device7_Stage13_Accumulate
	Layer2_Device7_Stage12_Accumulate -> Layer2_Device7_Stage13_Accumulate
	Layer2_Device7_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage13_RecvKV -> Layer2_Device7_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage14_RecvKV -> Layer2_Device7_Stage14_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage14_Attention [label=Q_local]
	Layer2_Device7_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage14_Attention -> Layer2_Device7_Stage14_Accumulate
	Layer2_Device7_Stage13_Accumulate -> Layer2_Device7_Stage14_Accumulate
	Layer2_Device7_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device7_Stage14_RecvKV -> Layer2_Device7_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device7_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device7_Stage15_RecvKV -> Layer2_Device7_Stage15_Attention
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage15_Attention [label=Q_local]
	Layer2_Device7_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device7_Stage15_Attention -> Layer2_Device7_Stage15_Accumulate
	Layer2_Device7_Stage14_Accumulate -> Layer2_Device7_Stage15_Accumulate
	Layer2_Device7_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device7_Stage15_Accumulate -> Layer2_Device7_ConcatHeads
	Layer2_Device7_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device7_ConcatHeads -> Layer2_Device7_OutputProj
	Layer2_Device7_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device7_OutputProj -> Layer2_Device7_Residual1
	Layer2_Device7_Input -> Layer2_Device7_Residual1
	Layer2_Device7_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device7_Residual1 -> Layer2_Device7_LayerNorm2
	Layer2_Device7_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device7_LayerNorm2 -> Layer2_Device7_GateProj
	Layer2_Device7_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device7_LayerNorm2 -> Layer2_Device7_UpProj
	Layer2_Device7_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device7_GateProj -> Layer2_Device7_Activation
	Layer2_Device7_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device7_Activation -> Layer2_Device7_ElemMul
	Layer2_Device7_UpProj -> Layer2_Device7_ElemMul
	Layer2_Device7_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device7_ElemMul -> Layer2_Device7_DownProj
	Layer2_Device7_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device7_DownProj -> Layer2_Device7_Residual2
	Layer2_Device7_Residual1 -> Layer2_Device7_Residual2
	Layer2_Device7_Output [label="Layer 2 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device7_Residual2 -> Layer2_Device7_Output
	Layer2_Device8_Input [label="Layer 2 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device8_Output -> Layer2_Device8_Input
	Layer2_Device8_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device8_Input -> Layer2_Device8_LayerNorm1
	Layer2_Device8_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device8_LayerNorm1 -> Layer2_Device8_QKVProj
	Layer2_Device8_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device8_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage0_RecvKV -> Layer2_Device8_Stage0_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage0_Attention [label=Q_local]
	Layer2_Device8_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage0_Attention -> Layer2_Device8_Stage0_Accumulate
	Layer2_Device8_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage0_RecvKV -> Layer2_Device8_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage1_RecvKV -> Layer2_Device8_Stage1_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage1_Attention [label=Q_local]
	Layer2_Device8_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage1_Attention -> Layer2_Device8_Stage1_Accumulate
	Layer2_Device8_Stage0_Accumulate -> Layer2_Device8_Stage1_Accumulate
	Layer2_Device8_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage1_RecvKV -> Layer2_Device8_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage2_RecvKV -> Layer2_Device8_Stage2_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage2_Attention [label=Q_local]
	Layer2_Device8_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage2_Attention -> Layer2_Device8_Stage2_Accumulate
	Layer2_Device8_Stage1_Accumulate -> Layer2_Device8_Stage2_Accumulate
	Layer2_Device8_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage2_RecvKV -> Layer2_Device8_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage3_RecvKV -> Layer2_Device8_Stage3_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage3_Attention [label=Q_local]
	Layer2_Device8_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage3_Attention -> Layer2_Device8_Stage3_Accumulate
	Layer2_Device8_Stage2_Accumulate -> Layer2_Device8_Stage3_Accumulate
	Layer2_Device8_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage3_RecvKV -> Layer2_Device8_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage4_RecvKV -> Layer2_Device8_Stage4_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage4_Attention [label=Q_local]
	Layer2_Device8_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage4_Attention -> Layer2_Device8_Stage4_Accumulate
	Layer2_Device8_Stage3_Accumulate -> Layer2_Device8_Stage4_Accumulate
	Layer2_Device8_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage4_RecvKV -> Layer2_Device8_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage5_RecvKV -> Layer2_Device8_Stage5_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage5_Attention [label=Q_local]
	Layer2_Device8_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage5_Attention -> Layer2_Device8_Stage5_Accumulate
	Layer2_Device8_Stage4_Accumulate -> Layer2_Device8_Stage5_Accumulate
	Layer2_Device8_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage5_RecvKV -> Layer2_Device8_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage6_RecvKV -> Layer2_Device8_Stage6_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage6_Attention [label=Q_local]
	Layer2_Device8_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage6_Attention -> Layer2_Device8_Stage6_Accumulate
	Layer2_Device8_Stage5_Accumulate -> Layer2_Device8_Stage6_Accumulate
	Layer2_Device8_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage6_RecvKV -> Layer2_Device8_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage7_RecvKV -> Layer2_Device8_Stage7_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage7_Attention [label=Q_local]
	Layer2_Device8_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage7_Attention -> Layer2_Device8_Stage7_Accumulate
	Layer2_Device8_Stage6_Accumulate -> Layer2_Device8_Stage7_Accumulate
	Layer2_Device8_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage7_RecvKV -> Layer2_Device8_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage8_RecvKV -> Layer2_Device8_Stage8_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage8_Attention [label=Q_local]
	Layer2_Device8_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage8_Attention -> Layer2_Device8_Stage8_Accumulate
	Layer2_Device8_Stage7_Accumulate -> Layer2_Device8_Stage8_Accumulate
	Layer2_Device8_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage8_RecvKV -> Layer2_Device8_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage9_RecvKV -> Layer2_Device8_Stage9_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage9_Attention [label=Q_local]
	Layer2_Device8_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage9_Attention -> Layer2_Device8_Stage9_Accumulate
	Layer2_Device8_Stage8_Accumulate -> Layer2_Device8_Stage9_Accumulate
	Layer2_Device8_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage9_RecvKV -> Layer2_Device8_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage10_RecvKV -> Layer2_Device8_Stage10_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage10_Attention [label=Q_local]
	Layer2_Device8_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage10_Attention -> Layer2_Device8_Stage10_Accumulate
	Layer2_Device8_Stage9_Accumulate -> Layer2_Device8_Stage10_Accumulate
	Layer2_Device8_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage10_RecvKV -> Layer2_Device8_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage11_RecvKV -> Layer2_Device8_Stage11_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage11_Attention [label=Q_local]
	Layer2_Device8_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage11_Attention -> Layer2_Device8_Stage11_Accumulate
	Layer2_Device8_Stage10_Accumulate -> Layer2_Device8_Stage11_Accumulate
	Layer2_Device8_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage11_RecvKV -> Layer2_Device8_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage12_RecvKV -> Layer2_Device8_Stage12_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage12_Attention [label=Q_local]
	Layer2_Device8_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage12_Attention -> Layer2_Device8_Stage12_Accumulate
	Layer2_Device8_Stage11_Accumulate -> Layer2_Device8_Stage12_Accumulate
	Layer2_Device8_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage12_RecvKV -> Layer2_Device8_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage13_RecvKV -> Layer2_Device8_Stage13_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage13_Attention [label=Q_local]
	Layer2_Device8_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage13_Attention -> Layer2_Device8_Stage13_Accumulate
	Layer2_Device8_Stage12_Accumulate -> Layer2_Device8_Stage13_Accumulate
	Layer2_Device8_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage13_RecvKV -> Layer2_Device8_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage14_RecvKV -> Layer2_Device8_Stage14_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage14_Attention [label=Q_local]
	Layer2_Device8_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage14_Attention -> Layer2_Device8_Stage14_Accumulate
	Layer2_Device8_Stage13_Accumulate -> Layer2_Device8_Stage14_Accumulate
	Layer2_Device8_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device8_Stage14_RecvKV -> Layer2_Device8_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device8_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device8_Stage15_RecvKV -> Layer2_Device8_Stage15_Attention
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage15_Attention [label=Q_local]
	Layer2_Device8_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device8_Stage15_Attention -> Layer2_Device8_Stage15_Accumulate
	Layer2_Device8_Stage14_Accumulate -> Layer2_Device8_Stage15_Accumulate
	Layer2_Device8_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device8_Stage15_Accumulate -> Layer2_Device8_ConcatHeads
	Layer2_Device8_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device8_ConcatHeads -> Layer2_Device8_OutputProj
	Layer2_Device8_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device8_OutputProj -> Layer2_Device8_Residual1
	Layer2_Device8_Input -> Layer2_Device8_Residual1
	Layer2_Device8_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device8_Residual1 -> Layer2_Device8_LayerNorm2
	Layer2_Device8_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device8_LayerNorm2 -> Layer2_Device8_GateProj
	Layer2_Device8_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device8_LayerNorm2 -> Layer2_Device8_UpProj
	Layer2_Device8_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device8_GateProj -> Layer2_Device8_Activation
	Layer2_Device8_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device8_Activation -> Layer2_Device8_ElemMul
	Layer2_Device8_UpProj -> Layer2_Device8_ElemMul
	Layer2_Device8_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device8_ElemMul -> Layer2_Device8_DownProj
	Layer2_Device8_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device8_DownProj -> Layer2_Device8_Residual2
	Layer2_Device8_Residual1 -> Layer2_Device8_Residual2
	Layer2_Device8_Output [label="Layer 2 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device8_Residual2 -> Layer2_Device8_Output
	Layer2_Device9_Input [label="Layer 2 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device9_Output -> Layer2_Device9_Input
	Layer2_Device9_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device9_Input -> Layer2_Device9_LayerNorm1
	Layer2_Device9_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device9_LayerNorm1 -> Layer2_Device9_QKVProj
	Layer2_Device9_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device9_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage0_RecvKV -> Layer2_Device9_Stage0_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage0_Attention [label=Q_local]
	Layer2_Device9_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage0_Attention -> Layer2_Device9_Stage0_Accumulate
	Layer2_Device9_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage0_RecvKV -> Layer2_Device9_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage1_RecvKV -> Layer2_Device9_Stage1_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage1_Attention [label=Q_local]
	Layer2_Device9_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage1_Attention -> Layer2_Device9_Stage1_Accumulate
	Layer2_Device9_Stage0_Accumulate -> Layer2_Device9_Stage1_Accumulate
	Layer2_Device9_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage1_RecvKV -> Layer2_Device9_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage2_RecvKV -> Layer2_Device9_Stage2_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage2_Attention [label=Q_local]
	Layer2_Device9_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage2_Attention -> Layer2_Device9_Stage2_Accumulate
	Layer2_Device9_Stage1_Accumulate -> Layer2_Device9_Stage2_Accumulate
	Layer2_Device9_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage2_RecvKV -> Layer2_Device9_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage3_RecvKV -> Layer2_Device9_Stage3_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage3_Attention [label=Q_local]
	Layer2_Device9_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage3_Attention -> Layer2_Device9_Stage3_Accumulate
	Layer2_Device9_Stage2_Accumulate -> Layer2_Device9_Stage3_Accumulate
	Layer2_Device9_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage3_RecvKV -> Layer2_Device9_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage4_RecvKV -> Layer2_Device9_Stage4_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage4_Attention [label=Q_local]
	Layer2_Device9_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage4_Attention -> Layer2_Device9_Stage4_Accumulate
	Layer2_Device9_Stage3_Accumulate -> Layer2_Device9_Stage4_Accumulate
	Layer2_Device9_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage4_RecvKV -> Layer2_Device9_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage5_RecvKV -> Layer2_Device9_Stage5_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage5_Attention [label=Q_local]
	Layer2_Device9_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage5_Attention -> Layer2_Device9_Stage5_Accumulate
	Layer2_Device9_Stage4_Accumulate -> Layer2_Device9_Stage5_Accumulate
	Layer2_Device9_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage5_RecvKV -> Layer2_Device9_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage6_RecvKV -> Layer2_Device9_Stage6_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage6_Attention [label=Q_local]
	Layer2_Device9_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage6_Attention -> Layer2_Device9_Stage6_Accumulate
	Layer2_Device9_Stage5_Accumulate -> Layer2_Device9_Stage6_Accumulate
	Layer2_Device9_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage6_RecvKV -> Layer2_Device9_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage7_RecvKV -> Layer2_Device9_Stage7_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage7_Attention [label=Q_local]
	Layer2_Device9_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage7_Attention -> Layer2_Device9_Stage7_Accumulate
	Layer2_Device9_Stage6_Accumulate -> Layer2_Device9_Stage7_Accumulate
	Layer2_Device9_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage7_RecvKV -> Layer2_Device9_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage8_RecvKV -> Layer2_Device9_Stage8_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage8_Attention [label=Q_local]
	Layer2_Device9_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage8_Attention -> Layer2_Device9_Stage8_Accumulate
	Layer2_Device9_Stage7_Accumulate -> Layer2_Device9_Stage8_Accumulate
	Layer2_Device9_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage8_RecvKV -> Layer2_Device9_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage9_RecvKV -> Layer2_Device9_Stage9_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage9_Attention [label=Q_local]
	Layer2_Device9_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage9_Attention -> Layer2_Device9_Stage9_Accumulate
	Layer2_Device9_Stage8_Accumulate -> Layer2_Device9_Stage9_Accumulate
	Layer2_Device9_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage9_RecvKV -> Layer2_Device9_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage10_RecvKV -> Layer2_Device9_Stage10_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage10_Attention [label=Q_local]
	Layer2_Device9_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage10_Attention -> Layer2_Device9_Stage10_Accumulate
	Layer2_Device9_Stage9_Accumulate -> Layer2_Device9_Stage10_Accumulate
	Layer2_Device9_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage10_RecvKV -> Layer2_Device9_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage11_RecvKV -> Layer2_Device9_Stage11_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage11_Attention [label=Q_local]
	Layer2_Device9_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage11_Attention -> Layer2_Device9_Stage11_Accumulate
	Layer2_Device9_Stage10_Accumulate -> Layer2_Device9_Stage11_Accumulate
	Layer2_Device9_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage11_RecvKV -> Layer2_Device9_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage12_RecvKV -> Layer2_Device9_Stage12_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage12_Attention [label=Q_local]
	Layer2_Device9_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage12_Attention -> Layer2_Device9_Stage12_Accumulate
	Layer2_Device9_Stage11_Accumulate -> Layer2_Device9_Stage12_Accumulate
	Layer2_Device9_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage12_RecvKV -> Layer2_Device9_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage13_RecvKV -> Layer2_Device9_Stage13_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage13_Attention [label=Q_local]
	Layer2_Device9_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage13_Attention -> Layer2_Device9_Stage13_Accumulate
	Layer2_Device9_Stage12_Accumulate -> Layer2_Device9_Stage13_Accumulate
	Layer2_Device9_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage13_RecvKV -> Layer2_Device9_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage14_RecvKV -> Layer2_Device9_Stage14_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage14_Attention [label=Q_local]
	Layer2_Device9_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage14_Attention -> Layer2_Device9_Stage14_Accumulate
	Layer2_Device9_Stage13_Accumulate -> Layer2_Device9_Stage14_Accumulate
	Layer2_Device9_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device9_Stage14_RecvKV -> Layer2_Device9_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device9_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device9_Stage15_RecvKV -> Layer2_Device9_Stage15_Attention
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage15_Attention [label=Q_local]
	Layer2_Device9_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device9_Stage15_Attention -> Layer2_Device9_Stage15_Accumulate
	Layer2_Device9_Stage14_Accumulate -> Layer2_Device9_Stage15_Accumulate
	Layer2_Device9_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device9_Stage15_Accumulate -> Layer2_Device9_ConcatHeads
	Layer2_Device9_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device9_ConcatHeads -> Layer2_Device9_OutputProj
	Layer2_Device9_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device9_OutputProj -> Layer2_Device9_Residual1
	Layer2_Device9_Input -> Layer2_Device9_Residual1
	Layer2_Device9_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device9_Residual1 -> Layer2_Device9_LayerNorm2
	Layer2_Device9_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device9_LayerNorm2 -> Layer2_Device9_GateProj
	Layer2_Device9_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device9_LayerNorm2 -> Layer2_Device9_UpProj
	Layer2_Device9_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device9_GateProj -> Layer2_Device9_Activation
	Layer2_Device9_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device9_Activation -> Layer2_Device9_ElemMul
	Layer2_Device9_UpProj -> Layer2_Device9_ElemMul
	Layer2_Device9_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device9_ElemMul -> Layer2_Device9_DownProj
	Layer2_Device9_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device9_DownProj -> Layer2_Device9_Residual2
	Layer2_Device9_Residual1 -> Layer2_Device9_Residual2
	Layer2_Device9_Output [label="Layer 2 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device9_Residual2 -> Layer2_Device9_Output
	Layer2_Device10_Input [label="Layer 2 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device10_Output -> Layer2_Device10_Input
	Layer2_Device10_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device10_Input -> Layer2_Device10_LayerNorm1
	Layer2_Device10_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device10_LayerNorm1 -> Layer2_Device10_QKVProj
	Layer2_Device10_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device10_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage0_RecvKV -> Layer2_Device10_Stage0_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage0_Attention [label=Q_local]
	Layer2_Device10_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage0_Attention -> Layer2_Device10_Stage0_Accumulate
	Layer2_Device10_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage0_RecvKV -> Layer2_Device10_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage1_RecvKV -> Layer2_Device10_Stage1_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage1_Attention [label=Q_local]
	Layer2_Device10_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage1_Attention -> Layer2_Device10_Stage1_Accumulate
	Layer2_Device10_Stage0_Accumulate -> Layer2_Device10_Stage1_Accumulate
	Layer2_Device10_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage1_RecvKV -> Layer2_Device10_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage2_RecvKV -> Layer2_Device10_Stage2_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage2_Attention [label=Q_local]
	Layer2_Device10_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage2_Attention -> Layer2_Device10_Stage2_Accumulate
	Layer2_Device10_Stage1_Accumulate -> Layer2_Device10_Stage2_Accumulate
	Layer2_Device10_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage2_RecvKV -> Layer2_Device10_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage3_RecvKV -> Layer2_Device10_Stage3_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage3_Attention [label=Q_local]
	Layer2_Device10_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage3_Attention -> Layer2_Device10_Stage3_Accumulate
	Layer2_Device10_Stage2_Accumulate -> Layer2_Device10_Stage3_Accumulate
	Layer2_Device10_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage3_RecvKV -> Layer2_Device10_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage4_RecvKV -> Layer2_Device10_Stage4_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage4_Attention [label=Q_local]
	Layer2_Device10_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage4_Attention -> Layer2_Device10_Stage4_Accumulate
	Layer2_Device10_Stage3_Accumulate -> Layer2_Device10_Stage4_Accumulate
	Layer2_Device10_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage4_RecvKV -> Layer2_Device10_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage5_RecvKV -> Layer2_Device10_Stage5_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage5_Attention [label=Q_local]
	Layer2_Device10_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage5_Attention -> Layer2_Device10_Stage5_Accumulate
	Layer2_Device10_Stage4_Accumulate -> Layer2_Device10_Stage5_Accumulate
	Layer2_Device10_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage5_RecvKV -> Layer2_Device10_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage6_RecvKV -> Layer2_Device10_Stage6_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage6_Attention [label=Q_local]
	Layer2_Device10_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage6_Attention -> Layer2_Device10_Stage6_Accumulate
	Layer2_Device10_Stage5_Accumulate -> Layer2_Device10_Stage6_Accumulate
	Layer2_Device10_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage6_RecvKV -> Layer2_Device10_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage7_RecvKV -> Layer2_Device10_Stage7_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage7_Attention [label=Q_local]
	Layer2_Device10_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage7_Attention -> Layer2_Device10_Stage7_Accumulate
	Layer2_Device10_Stage6_Accumulate -> Layer2_Device10_Stage7_Accumulate
	Layer2_Device10_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage7_RecvKV -> Layer2_Device10_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage8_RecvKV -> Layer2_Device10_Stage8_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage8_Attention [label=Q_local]
	Layer2_Device10_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage8_Attention -> Layer2_Device10_Stage8_Accumulate
	Layer2_Device10_Stage7_Accumulate -> Layer2_Device10_Stage8_Accumulate
	Layer2_Device10_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage8_RecvKV -> Layer2_Device10_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage9_RecvKV -> Layer2_Device10_Stage9_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage9_Attention [label=Q_local]
	Layer2_Device10_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage9_Attention -> Layer2_Device10_Stage9_Accumulate
	Layer2_Device10_Stage8_Accumulate -> Layer2_Device10_Stage9_Accumulate
	Layer2_Device10_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage9_RecvKV -> Layer2_Device10_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage10_RecvKV -> Layer2_Device10_Stage10_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage10_Attention [label=Q_local]
	Layer2_Device10_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage10_Attention -> Layer2_Device10_Stage10_Accumulate
	Layer2_Device10_Stage9_Accumulate -> Layer2_Device10_Stage10_Accumulate
	Layer2_Device10_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage10_RecvKV -> Layer2_Device10_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage11_RecvKV -> Layer2_Device10_Stage11_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage11_Attention [label=Q_local]
	Layer2_Device10_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage11_Attention -> Layer2_Device10_Stage11_Accumulate
	Layer2_Device10_Stage10_Accumulate -> Layer2_Device10_Stage11_Accumulate
	Layer2_Device10_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage11_RecvKV -> Layer2_Device10_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage12_RecvKV -> Layer2_Device10_Stage12_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage12_Attention [label=Q_local]
	Layer2_Device10_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage12_Attention -> Layer2_Device10_Stage12_Accumulate
	Layer2_Device10_Stage11_Accumulate -> Layer2_Device10_Stage12_Accumulate
	Layer2_Device10_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage12_RecvKV -> Layer2_Device10_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage13_RecvKV -> Layer2_Device10_Stage13_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage13_Attention [label=Q_local]
	Layer2_Device10_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage13_Attention -> Layer2_Device10_Stage13_Accumulate
	Layer2_Device10_Stage12_Accumulate -> Layer2_Device10_Stage13_Accumulate
	Layer2_Device10_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage13_RecvKV -> Layer2_Device10_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage14_RecvKV -> Layer2_Device10_Stage14_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage14_Attention [label=Q_local]
	Layer2_Device10_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage14_Attention -> Layer2_Device10_Stage14_Accumulate
	Layer2_Device10_Stage13_Accumulate -> Layer2_Device10_Stage14_Accumulate
	Layer2_Device10_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device10_Stage14_RecvKV -> Layer2_Device10_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device10_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device10_Stage15_RecvKV -> Layer2_Device10_Stage15_Attention
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage15_Attention [label=Q_local]
	Layer2_Device10_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device10_Stage15_Attention -> Layer2_Device10_Stage15_Accumulate
	Layer2_Device10_Stage14_Accumulate -> Layer2_Device10_Stage15_Accumulate
	Layer2_Device10_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device10_Stage15_Accumulate -> Layer2_Device10_ConcatHeads
	Layer2_Device10_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device10_ConcatHeads -> Layer2_Device10_OutputProj
	Layer2_Device10_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device10_OutputProj -> Layer2_Device10_Residual1
	Layer2_Device10_Input -> Layer2_Device10_Residual1
	Layer2_Device10_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device10_Residual1 -> Layer2_Device10_LayerNorm2
	Layer2_Device10_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device10_LayerNorm2 -> Layer2_Device10_GateProj
	Layer2_Device10_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device10_LayerNorm2 -> Layer2_Device10_UpProj
	Layer2_Device10_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device10_GateProj -> Layer2_Device10_Activation
	Layer2_Device10_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device10_Activation -> Layer2_Device10_ElemMul
	Layer2_Device10_UpProj -> Layer2_Device10_ElemMul
	Layer2_Device10_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device10_ElemMul -> Layer2_Device10_DownProj
	Layer2_Device10_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device10_DownProj -> Layer2_Device10_Residual2
	Layer2_Device10_Residual1 -> Layer2_Device10_Residual2
	Layer2_Device10_Output [label="Layer 2 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device10_Residual2 -> Layer2_Device10_Output
	Layer2_Device11_Input [label="Layer 2 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device11_Output -> Layer2_Device11_Input
	Layer2_Device11_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device11_Input -> Layer2_Device11_LayerNorm1
	Layer2_Device11_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device11_LayerNorm1 -> Layer2_Device11_QKVProj
	Layer2_Device11_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device11_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage0_RecvKV -> Layer2_Device11_Stage0_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage0_Attention [label=Q_local]
	Layer2_Device11_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage0_Attention -> Layer2_Device11_Stage0_Accumulate
	Layer2_Device11_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage0_RecvKV -> Layer2_Device11_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage1_RecvKV -> Layer2_Device11_Stage1_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage1_Attention [label=Q_local]
	Layer2_Device11_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage1_Attention -> Layer2_Device11_Stage1_Accumulate
	Layer2_Device11_Stage0_Accumulate -> Layer2_Device11_Stage1_Accumulate
	Layer2_Device11_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage1_RecvKV -> Layer2_Device11_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage2_RecvKV -> Layer2_Device11_Stage2_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage2_Attention [label=Q_local]
	Layer2_Device11_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage2_Attention -> Layer2_Device11_Stage2_Accumulate
	Layer2_Device11_Stage1_Accumulate -> Layer2_Device11_Stage2_Accumulate
	Layer2_Device11_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage2_RecvKV -> Layer2_Device11_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage3_RecvKV -> Layer2_Device11_Stage3_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage3_Attention [label=Q_local]
	Layer2_Device11_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage3_Attention -> Layer2_Device11_Stage3_Accumulate
	Layer2_Device11_Stage2_Accumulate -> Layer2_Device11_Stage3_Accumulate
	Layer2_Device11_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage3_RecvKV -> Layer2_Device11_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage4_RecvKV -> Layer2_Device11_Stage4_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage4_Attention [label=Q_local]
	Layer2_Device11_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage4_Attention -> Layer2_Device11_Stage4_Accumulate
	Layer2_Device11_Stage3_Accumulate -> Layer2_Device11_Stage4_Accumulate
	Layer2_Device11_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage4_RecvKV -> Layer2_Device11_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage5_RecvKV -> Layer2_Device11_Stage5_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage5_Attention [label=Q_local]
	Layer2_Device11_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage5_Attention -> Layer2_Device11_Stage5_Accumulate
	Layer2_Device11_Stage4_Accumulate -> Layer2_Device11_Stage5_Accumulate
	Layer2_Device11_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage5_RecvKV -> Layer2_Device11_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage6_RecvKV -> Layer2_Device11_Stage6_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage6_Attention [label=Q_local]
	Layer2_Device11_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage6_Attention -> Layer2_Device11_Stage6_Accumulate
	Layer2_Device11_Stage5_Accumulate -> Layer2_Device11_Stage6_Accumulate
	Layer2_Device11_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage6_RecvKV -> Layer2_Device11_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage7_RecvKV -> Layer2_Device11_Stage7_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage7_Attention [label=Q_local]
	Layer2_Device11_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage7_Attention -> Layer2_Device11_Stage7_Accumulate
	Layer2_Device11_Stage6_Accumulate -> Layer2_Device11_Stage7_Accumulate
	Layer2_Device11_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage7_RecvKV -> Layer2_Device11_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage8_RecvKV -> Layer2_Device11_Stage8_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage8_Attention [label=Q_local]
	Layer2_Device11_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage8_Attention -> Layer2_Device11_Stage8_Accumulate
	Layer2_Device11_Stage7_Accumulate -> Layer2_Device11_Stage8_Accumulate
	Layer2_Device11_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage8_RecvKV -> Layer2_Device11_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage9_RecvKV -> Layer2_Device11_Stage9_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage9_Attention [label=Q_local]
	Layer2_Device11_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage9_Attention -> Layer2_Device11_Stage9_Accumulate
	Layer2_Device11_Stage8_Accumulate -> Layer2_Device11_Stage9_Accumulate
	Layer2_Device11_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage9_RecvKV -> Layer2_Device11_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage10_RecvKV -> Layer2_Device11_Stage10_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage10_Attention [label=Q_local]
	Layer2_Device11_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage10_Attention -> Layer2_Device11_Stage10_Accumulate
	Layer2_Device11_Stage9_Accumulate -> Layer2_Device11_Stage10_Accumulate
	Layer2_Device11_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage10_RecvKV -> Layer2_Device11_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage11_RecvKV -> Layer2_Device11_Stage11_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage11_Attention [label=Q_local]
	Layer2_Device11_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage11_Attention -> Layer2_Device11_Stage11_Accumulate
	Layer2_Device11_Stage10_Accumulate -> Layer2_Device11_Stage11_Accumulate
	Layer2_Device11_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage11_RecvKV -> Layer2_Device11_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage12_RecvKV -> Layer2_Device11_Stage12_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage12_Attention [label=Q_local]
	Layer2_Device11_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage12_Attention -> Layer2_Device11_Stage12_Accumulate
	Layer2_Device11_Stage11_Accumulate -> Layer2_Device11_Stage12_Accumulate
	Layer2_Device11_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage12_RecvKV -> Layer2_Device11_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage13_RecvKV -> Layer2_Device11_Stage13_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage13_Attention [label=Q_local]
	Layer2_Device11_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage13_Attention -> Layer2_Device11_Stage13_Accumulate
	Layer2_Device11_Stage12_Accumulate -> Layer2_Device11_Stage13_Accumulate
	Layer2_Device11_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage13_RecvKV -> Layer2_Device11_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage14_RecvKV -> Layer2_Device11_Stage14_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage14_Attention [label=Q_local]
	Layer2_Device11_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage14_Attention -> Layer2_Device11_Stage14_Accumulate
	Layer2_Device11_Stage13_Accumulate -> Layer2_Device11_Stage14_Accumulate
	Layer2_Device11_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device11_Stage14_RecvKV -> Layer2_Device11_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device11_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device11_Stage15_RecvKV -> Layer2_Device11_Stage15_Attention
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage15_Attention [label=Q_local]
	Layer2_Device11_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device11_Stage15_Attention -> Layer2_Device11_Stage15_Accumulate
	Layer2_Device11_Stage14_Accumulate -> Layer2_Device11_Stage15_Accumulate
	Layer2_Device11_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device11_Stage15_Accumulate -> Layer2_Device11_ConcatHeads
	Layer2_Device11_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device11_ConcatHeads -> Layer2_Device11_OutputProj
	Layer2_Device11_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device11_OutputProj -> Layer2_Device11_Residual1
	Layer2_Device11_Input -> Layer2_Device11_Residual1
	Layer2_Device11_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device11_Residual1 -> Layer2_Device11_LayerNorm2
	Layer2_Device11_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device11_LayerNorm2 -> Layer2_Device11_GateProj
	Layer2_Device11_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device11_LayerNorm2 -> Layer2_Device11_UpProj
	Layer2_Device11_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device11_GateProj -> Layer2_Device11_Activation
	Layer2_Device11_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device11_Activation -> Layer2_Device11_ElemMul
	Layer2_Device11_UpProj -> Layer2_Device11_ElemMul
	Layer2_Device11_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device11_ElemMul -> Layer2_Device11_DownProj
	Layer2_Device11_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device11_DownProj -> Layer2_Device11_Residual2
	Layer2_Device11_Residual1 -> Layer2_Device11_Residual2
	Layer2_Device11_Output [label="Layer 2 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device11_Residual2 -> Layer2_Device11_Output
	Layer2_Device12_Input [label="Layer 2 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device12_Output -> Layer2_Device12_Input
	Layer2_Device12_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device12_Input -> Layer2_Device12_LayerNorm1
	Layer2_Device12_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device12_LayerNorm1 -> Layer2_Device12_QKVProj
	Layer2_Device12_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device12_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage0_RecvKV -> Layer2_Device12_Stage0_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage0_Attention [label=Q_local]
	Layer2_Device12_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage0_Attention -> Layer2_Device12_Stage0_Accumulate
	Layer2_Device12_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage0_RecvKV -> Layer2_Device12_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage1_RecvKV -> Layer2_Device12_Stage1_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage1_Attention [label=Q_local]
	Layer2_Device12_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage1_Attention -> Layer2_Device12_Stage1_Accumulate
	Layer2_Device12_Stage0_Accumulate -> Layer2_Device12_Stage1_Accumulate
	Layer2_Device12_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage1_RecvKV -> Layer2_Device12_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage2_RecvKV -> Layer2_Device12_Stage2_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage2_Attention [label=Q_local]
	Layer2_Device12_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage2_Attention -> Layer2_Device12_Stage2_Accumulate
	Layer2_Device12_Stage1_Accumulate -> Layer2_Device12_Stage2_Accumulate
	Layer2_Device12_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage2_RecvKV -> Layer2_Device12_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage3_RecvKV -> Layer2_Device12_Stage3_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage3_Attention [label=Q_local]
	Layer2_Device12_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage3_Attention -> Layer2_Device12_Stage3_Accumulate
	Layer2_Device12_Stage2_Accumulate -> Layer2_Device12_Stage3_Accumulate
	Layer2_Device12_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage3_RecvKV -> Layer2_Device12_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage4_RecvKV -> Layer2_Device12_Stage4_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage4_Attention [label=Q_local]
	Layer2_Device12_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage4_Attention -> Layer2_Device12_Stage4_Accumulate
	Layer2_Device12_Stage3_Accumulate -> Layer2_Device12_Stage4_Accumulate
	Layer2_Device12_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage4_RecvKV -> Layer2_Device12_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage5_RecvKV -> Layer2_Device12_Stage5_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage5_Attention [label=Q_local]
	Layer2_Device12_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage5_Attention -> Layer2_Device12_Stage5_Accumulate
	Layer2_Device12_Stage4_Accumulate -> Layer2_Device12_Stage5_Accumulate
	Layer2_Device12_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage5_RecvKV -> Layer2_Device12_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage6_RecvKV -> Layer2_Device12_Stage6_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage6_Attention [label=Q_local]
	Layer2_Device12_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage6_Attention -> Layer2_Device12_Stage6_Accumulate
	Layer2_Device12_Stage5_Accumulate -> Layer2_Device12_Stage6_Accumulate
	Layer2_Device12_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage6_RecvKV -> Layer2_Device12_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage7_RecvKV -> Layer2_Device12_Stage7_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage7_Attention [label=Q_local]
	Layer2_Device12_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage7_Attention -> Layer2_Device12_Stage7_Accumulate
	Layer2_Device12_Stage6_Accumulate -> Layer2_Device12_Stage7_Accumulate
	Layer2_Device12_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage7_RecvKV -> Layer2_Device12_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage8_RecvKV -> Layer2_Device12_Stage8_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage8_Attention [label=Q_local]
	Layer2_Device12_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage8_Attention -> Layer2_Device12_Stage8_Accumulate
	Layer2_Device12_Stage7_Accumulate -> Layer2_Device12_Stage8_Accumulate
	Layer2_Device12_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage8_RecvKV -> Layer2_Device12_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage9_RecvKV -> Layer2_Device12_Stage9_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage9_Attention [label=Q_local]
	Layer2_Device12_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage9_Attention -> Layer2_Device12_Stage9_Accumulate
	Layer2_Device12_Stage8_Accumulate -> Layer2_Device12_Stage9_Accumulate
	Layer2_Device12_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage9_RecvKV -> Layer2_Device12_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage10_RecvKV -> Layer2_Device12_Stage10_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage10_Attention [label=Q_local]
	Layer2_Device12_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage10_Attention -> Layer2_Device12_Stage10_Accumulate
	Layer2_Device12_Stage9_Accumulate -> Layer2_Device12_Stage10_Accumulate
	Layer2_Device12_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage10_RecvKV -> Layer2_Device12_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage11_RecvKV -> Layer2_Device12_Stage11_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage11_Attention [label=Q_local]
	Layer2_Device12_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage11_Attention -> Layer2_Device12_Stage11_Accumulate
	Layer2_Device12_Stage10_Accumulate -> Layer2_Device12_Stage11_Accumulate
	Layer2_Device12_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage11_RecvKV -> Layer2_Device12_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage12_RecvKV -> Layer2_Device12_Stage12_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage12_Attention [label=Q_local]
	Layer2_Device12_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage12_Attention -> Layer2_Device12_Stage12_Accumulate
	Layer2_Device12_Stage11_Accumulate -> Layer2_Device12_Stage12_Accumulate
	Layer2_Device12_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage12_RecvKV -> Layer2_Device12_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage13_RecvKV -> Layer2_Device12_Stage13_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage13_Attention [label=Q_local]
	Layer2_Device12_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage13_Attention -> Layer2_Device12_Stage13_Accumulate
	Layer2_Device12_Stage12_Accumulate -> Layer2_Device12_Stage13_Accumulate
	Layer2_Device12_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage13_RecvKV -> Layer2_Device12_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage14_RecvKV -> Layer2_Device12_Stage14_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage14_Attention [label=Q_local]
	Layer2_Device12_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage14_Attention -> Layer2_Device12_Stage14_Accumulate
	Layer2_Device12_Stage13_Accumulate -> Layer2_Device12_Stage14_Accumulate
	Layer2_Device12_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device12_Stage14_RecvKV -> Layer2_Device12_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device12_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device12_Stage15_RecvKV -> Layer2_Device12_Stage15_Attention
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage15_Attention [label=Q_local]
	Layer2_Device12_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device12_Stage15_Attention -> Layer2_Device12_Stage15_Accumulate
	Layer2_Device12_Stage14_Accumulate -> Layer2_Device12_Stage15_Accumulate
	Layer2_Device12_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device12_Stage15_Accumulate -> Layer2_Device12_ConcatHeads
	Layer2_Device12_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device12_ConcatHeads -> Layer2_Device12_OutputProj
	Layer2_Device12_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device12_OutputProj -> Layer2_Device12_Residual1
	Layer2_Device12_Input -> Layer2_Device12_Residual1
	Layer2_Device12_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device12_Residual1 -> Layer2_Device12_LayerNorm2
	Layer2_Device12_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device12_LayerNorm2 -> Layer2_Device12_GateProj
	Layer2_Device12_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device12_LayerNorm2 -> Layer2_Device12_UpProj
	Layer2_Device12_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device12_GateProj -> Layer2_Device12_Activation
	Layer2_Device12_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device12_Activation -> Layer2_Device12_ElemMul
	Layer2_Device12_UpProj -> Layer2_Device12_ElemMul
	Layer2_Device12_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device12_ElemMul -> Layer2_Device12_DownProj
	Layer2_Device12_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device12_DownProj -> Layer2_Device12_Residual2
	Layer2_Device12_Residual1 -> Layer2_Device12_Residual2
	Layer2_Device12_Output [label="Layer 2 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device12_Residual2 -> Layer2_Device12_Output
	Layer2_Device13_Input [label="Layer 2 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device13_Output -> Layer2_Device13_Input
	Layer2_Device13_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device13_Input -> Layer2_Device13_LayerNorm1
	Layer2_Device13_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device13_LayerNorm1 -> Layer2_Device13_QKVProj
	Layer2_Device13_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device13_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage0_RecvKV -> Layer2_Device13_Stage0_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage0_Attention [label=Q_local]
	Layer2_Device13_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage0_Attention -> Layer2_Device13_Stage0_Accumulate
	Layer2_Device13_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage0_RecvKV -> Layer2_Device13_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage1_RecvKV -> Layer2_Device13_Stage1_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage1_Attention [label=Q_local]
	Layer2_Device13_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage1_Attention -> Layer2_Device13_Stage1_Accumulate
	Layer2_Device13_Stage0_Accumulate -> Layer2_Device13_Stage1_Accumulate
	Layer2_Device13_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage1_RecvKV -> Layer2_Device13_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage2_RecvKV -> Layer2_Device13_Stage2_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage2_Attention [label=Q_local]
	Layer2_Device13_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage2_Attention -> Layer2_Device13_Stage2_Accumulate
	Layer2_Device13_Stage1_Accumulate -> Layer2_Device13_Stage2_Accumulate
	Layer2_Device13_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage2_RecvKV -> Layer2_Device13_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage3_RecvKV -> Layer2_Device13_Stage3_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage3_Attention [label=Q_local]
	Layer2_Device13_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage3_Attention -> Layer2_Device13_Stage3_Accumulate
	Layer2_Device13_Stage2_Accumulate -> Layer2_Device13_Stage3_Accumulate
	Layer2_Device13_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage3_RecvKV -> Layer2_Device13_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage4_RecvKV -> Layer2_Device13_Stage4_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage4_Attention [label=Q_local]
	Layer2_Device13_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage4_Attention -> Layer2_Device13_Stage4_Accumulate
	Layer2_Device13_Stage3_Accumulate -> Layer2_Device13_Stage4_Accumulate
	Layer2_Device13_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage4_RecvKV -> Layer2_Device13_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage5_RecvKV -> Layer2_Device13_Stage5_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage5_Attention [label=Q_local]
	Layer2_Device13_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage5_Attention -> Layer2_Device13_Stage5_Accumulate
	Layer2_Device13_Stage4_Accumulate -> Layer2_Device13_Stage5_Accumulate
	Layer2_Device13_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage5_RecvKV -> Layer2_Device13_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage6_RecvKV -> Layer2_Device13_Stage6_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage6_Attention [label=Q_local]
	Layer2_Device13_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage6_Attention -> Layer2_Device13_Stage6_Accumulate
	Layer2_Device13_Stage5_Accumulate -> Layer2_Device13_Stage6_Accumulate
	Layer2_Device13_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage6_RecvKV -> Layer2_Device13_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage7_RecvKV -> Layer2_Device13_Stage7_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage7_Attention [label=Q_local]
	Layer2_Device13_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage7_Attention -> Layer2_Device13_Stage7_Accumulate
	Layer2_Device13_Stage6_Accumulate -> Layer2_Device13_Stage7_Accumulate
	Layer2_Device13_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage7_RecvKV -> Layer2_Device13_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage8_RecvKV -> Layer2_Device13_Stage8_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage8_Attention [label=Q_local]
	Layer2_Device13_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage8_Attention -> Layer2_Device13_Stage8_Accumulate
	Layer2_Device13_Stage7_Accumulate -> Layer2_Device13_Stage8_Accumulate
	Layer2_Device13_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage8_RecvKV -> Layer2_Device13_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage9_RecvKV -> Layer2_Device13_Stage9_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage9_Attention [label=Q_local]
	Layer2_Device13_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage9_Attention -> Layer2_Device13_Stage9_Accumulate
	Layer2_Device13_Stage8_Accumulate -> Layer2_Device13_Stage9_Accumulate
	Layer2_Device13_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage9_RecvKV -> Layer2_Device13_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage10_RecvKV -> Layer2_Device13_Stage10_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage10_Attention [label=Q_local]
	Layer2_Device13_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage10_Attention -> Layer2_Device13_Stage10_Accumulate
	Layer2_Device13_Stage9_Accumulate -> Layer2_Device13_Stage10_Accumulate
	Layer2_Device13_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage10_RecvKV -> Layer2_Device13_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage11_RecvKV -> Layer2_Device13_Stage11_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage11_Attention [label=Q_local]
	Layer2_Device13_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage11_Attention -> Layer2_Device13_Stage11_Accumulate
	Layer2_Device13_Stage10_Accumulate -> Layer2_Device13_Stage11_Accumulate
	Layer2_Device13_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage11_RecvKV -> Layer2_Device13_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage12_RecvKV -> Layer2_Device13_Stage12_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage12_Attention [label=Q_local]
	Layer2_Device13_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage12_Attention -> Layer2_Device13_Stage12_Accumulate
	Layer2_Device13_Stage11_Accumulate -> Layer2_Device13_Stage12_Accumulate
	Layer2_Device13_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage12_RecvKV -> Layer2_Device13_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage13_RecvKV -> Layer2_Device13_Stage13_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage13_Attention [label=Q_local]
	Layer2_Device13_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage13_Attention -> Layer2_Device13_Stage13_Accumulate
	Layer2_Device13_Stage12_Accumulate -> Layer2_Device13_Stage13_Accumulate
	Layer2_Device13_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage13_RecvKV -> Layer2_Device13_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage14_RecvKV -> Layer2_Device13_Stage14_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage14_Attention [label=Q_local]
	Layer2_Device13_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage14_Attention -> Layer2_Device13_Stage14_Accumulate
	Layer2_Device13_Stage13_Accumulate -> Layer2_Device13_Stage14_Accumulate
	Layer2_Device13_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device13_Stage14_RecvKV -> Layer2_Device13_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device13_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device13_Stage15_RecvKV -> Layer2_Device13_Stage15_Attention
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage15_Attention [label=Q_local]
	Layer2_Device13_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device13_Stage15_Attention -> Layer2_Device13_Stage15_Accumulate
	Layer2_Device13_Stage14_Accumulate -> Layer2_Device13_Stage15_Accumulate
	Layer2_Device13_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device13_Stage15_Accumulate -> Layer2_Device13_ConcatHeads
	Layer2_Device13_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device13_ConcatHeads -> Layer2_Device13_OutputProj
	Layer2_Device13_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device13_OutputProj -> Layer2_Device13_Residual1
	Layer2_Device13_Input -> Layer2_Device13_Residual1
	Layer2_Device13_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device13_Residual1 -> Layer2_Device13_LayerNorm2
	Layer2_Device13_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device13_LayerNorm2 -> Layer2_Device13_GateProj
	Layer2_Device13_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device13_LayerNorm2 -> Layer2_Device13_UpProj
	Layer2_Device13_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device13_GateProj -> Layer2_Device13_Activation
	Layer2_Device13_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device13_Activation -> Layer2_Device13_ElemMul
	Layer2_Device13_UpProj -> Layer2_Device13_ElemMul
	Layer2_Device13_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device13_ElemMul -> Layer2_Device13_DownProj
	Layer2_Device13_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device13_DownProj -> Layer2_Device13_Residual2
	Layer2_Device13_Residual1 -> Layer2_Device13_Residual2
	Layer2_Device13_Output [label="Layer 2 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device13_Residual2 -> Layer2_Device13_Output
	Layer2_Device14_Input [label="Layer 2 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device14_Output -> Layer2_Device14_Input
	Layer2_Device14_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device14_Input -> Layer2_Device14_LayerNorm1
	Layer2_Device14_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device14_LayerNorm1 -> Layer2_Device14_QKVProj
	Layer2_Device14_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device14_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage0_RecvKV -> Layer2_Device14_Stage0_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage0_Attention [label=Q_local]
	Layer2_Device14_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage0_Attention -> Layer2_Device14_Stage0_Accumulate
	Layer2_Device14_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage0_RecvKV -> Layer2_Device14_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage1_RecvKV -> Layer2_Device14_Stage1_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage1_Attention [label=Q_local]
	Layer2_Device14_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage1_Attention -> Layer2_Device14_Stage1_Accumulate
	Layer2_Device14_Stage0_Accumulate -> Layer2_Device14_Stage1_Accumulate
	Layer2_Device14_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage1_RecvKV -> Layer2_Device14_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage2_RecvKV -> Layer2_Device14_Stage2_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage2_Attention [label=Q_local]
	Layer2_Device14_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage2_Attention -> Layer2_Device14_Stage2_Accumulate
	Layer2_Device14_Stage1_Accumulate -> Layer2_Device14_Stage2_Accumulate
	Layer2_Device14_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage2_RecvKV -> Layer2_Device14_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage3_RecvKV -> Layer2_Device14_Stage3_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage3_Attention [label=Q_local]
	Layer2_Device14_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage3_Attention -> Layer2_Device14_Stage3_Accumulate
	Layer2_Device14_Stage2_Accumulate -> Layer2_Device14_Stage3_Accumulate
	Layer2_Device14_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage3_RecvKV -> Layer2_Device14_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage4_RecvKV -> Layer2_Device14_Stage4_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage4_Attention [label=Q_local]
	Layer2_Device14_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage4_Attention -> Layer2_Device14_Stage4_Accumulate
	Layer2_Device14_Stage3_Accumulate -> Layer2_Device14_Stage4_Accumulate
	Layer2_Device14_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage4_RecvKV -> Layer2_Device14_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage5_RecvKV -> Layer2_Device14_Stage5_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage5_Attention [label=Q_local]
	Layer2_Device14_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage5_Attention -> Layer2_Device14_Stage5_Accumulate
	Layer2_Device14_Stage4_Accumulate -> Layer2_Device14_Stage5_Accumulate
	Layer2_Device14_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage5_RecvKV -> Layer2_Device14_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage6_RecvKV -> Layer2_Device14_Stage6_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage6_Attention [label=Q_local]
	Layer2_Device14_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage6_Attention -> Layer2_Device14_Stage6_Accumulate
	Layer2_Device14_Stage5_Accumulate -> Layer2_Device14_Stage6_Accumulate
	Layer2_Device14_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage6_RecvKV -> Layer2_Device14_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage7_RecvKV -> Layer2_Device14_Stage7_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage7_Attention [label=Q_local]
	Layer2_Device14_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage7_Attention -> Layer2_Device14_Stage7_Accumulate
	Layer2_Device14_Stage6_Accumulate -> Layer2_Device14_Stage7_Accumulate
	Layer2_Device14_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage7_RecvKV -> Layer2_Device14_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage8_RecvKV -> Layer2_Device14_Stage8_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage8_Attention [label=Q_local]
	Layer2_Device14_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage8_Attention -> Layer2_Device14_Stage8_Accumulate
	Layer2_Device14_Stage7_Accumulate -> Layer2_Device14_Stage8_Accumulate
	Layer2_Device14_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage8_RecvKV -> Layer2_Device14_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage9_RecvKV -> Layer2_Device14_Stage9_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage9_Attention [label=Q_local]
	Layer2_Device14_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage9_Attention -> Layer2_Device14_Stage9_Accumulate
	Layer2_Device14_Stage8_Accumulate -> Layer2_Device14_Stage9_Accumulate
	Layer2_Device14_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage9_RecvKV -> Layer2_Device14_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage10_RecvKV -> Layer2_Device14_Stage10_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage10_Attention [label=Q_local]
	Layer2_Device14_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage10_Attention -> Layer2_Device14_Stage10_Accumulate
	Layer2_Device14_Stage9_Accumulate -> Layer2_Device14_Stage10_Accumulate
	Layer2_Device14_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage10_RecvKV -> Layer2_Device14_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage11_RecvKV -> Layer2_Device14_Stage11_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage11_Attention [label=Q_local]
	Layer2_Device14_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage11_Attention -> Layer2_Device14_Stage11_Accumulate
	Layer2_Device14_Stage10_Accumulate -> Layer2_Device14_Stage11_Accumulate
	Layer2_Device14_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage11_RecvKV -> Layer2_Device14_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage12_RecvKV -> Layer2_Device14_Stage12_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage12_Attention [label=Q_local]
	Layer2_Device14_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage12_Attention -> Layer2_Device14_Stage12_Accumulate
	Layer2_Device14_Stage11_Accumulate -> Layer2_Device14_Stage12_Accumulate
	Layer2_Device14_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage12_RecvKV -> Layer2_Device14_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage13_RecvKV -> Layer2_Device14_Stage13_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage13_Attention [label=Q_local]
	Layer2_Device14_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage13_Attention -> Layer2_Device14_Stage13_Accumulate
	Layer2_Device14_Stage12_Accumulate -> Layer2_Device14_Stage13_Accumulate
	Layer2_Device14_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage13_RecvKV -> Layer2_Device14_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage14_RecvKV -> Layer2_Device14_Stage14_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage14_Attention [label=Q_local]
	Layer2_Device14_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage14_Attention -> Layer2_Device14_Stage14_Accumulate
	Layer2_Device14_Stage13_Accumulate -> Layer2_Device14_Stage14_Accumulate
	Layer2_Device14_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device14_Stage14_RecvKV -> Layer2_Device14_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device14_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device14_Stage15_RecvKV -> Layer2_Device14_Stage15_Attention
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage15_Attention [label=Q_local]
	Layer2_Device14_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device14_Stage15_Attention -> Layer2_Device14_Stage15_Accumulate
	Layer2_Device14_Stage14_Accumulate -> Layer2_Device14_Stage15_Accumulate
	Layer2_Device14_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device14_Stage15_Accumulate -> Layer2_Device14_ConcatHeads
	Layer2_Device14_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device14_ConcatHeads -> Layer2_Device14_OutputProj
	Layer2_Device14_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device14_OutputProj -> Layer2_Device14_Residual1
	Layer2_Device14_Input -> Layer2_Device14_Residual1
	Layer2_Device14_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device14_Residual1 -> Layer2_Device14_LayerNorm2
	Layer2_Device14_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device14_LayerNorm2 -> Layer2_Device14_GateProj
	Layer2_Device14_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device14_LayerNorm2 -> Layer2_Device14_UpProj
	Layer2_Device14_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device14_GateProj -> Layer2_Device14_Activation
	Layer2_Device14_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device14_Activation -> Layer2_Device14_ElemMul
	Layer2_Device14_UpProj -> Layer2_Device14_ElemMul
	Layer2_Device14_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device14_ElemMul -> Layer2_Device14_DownProj
	Layer2_Device14_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device14_DownProj -> Layer2_Device14_Residual2
	Layer2_Device14_Residual1 -> Layer2_Device14_Residual2
	Layer2_Device14_Output [label="Layer 2 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device14_Residual2 -> Layer2_Device14_Output
	Layer2_Device15_Input [label="Layer 2 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer1_Device15_Output -> Layer2_Device15_Input
	Layer2_Device15_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device15_Input -> Layer2_Device15_LayerNorm1
	Layer2_Device15_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device15_LayerNorm1 -> Layer2_Device15_QKVProj
	Layer2_Device15_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage0_RecvKV [label="Local K,V"]
	Layer2_Device15_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage0_RecvKV -> Layer2_Device15_Stage0_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage0_Attention [label=Q_local]
	Layer2_Device15_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage0_Attention -> Layer2_Device15_Stage0_Accumulate
	Layer2_Device15_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage0_RecvKV -> Layer2_Device15_Stage1_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage1_RecvKV -> Layer2_Device15_Stage1_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage1_Attention [label=Q_local]
	Layer2_Device15_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage1_Attention -> Layer2_Device15_Stage1_Accumulate
	Layer2_Device15_Stage0_Accumulate -> Layer2_Device15_Stage1_Accumulate
	Layer2_Device15_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage1_RecvKV -> Layer2_Device15_Stage2_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage2_RecvKV -> Layer2_Device15_Stage2_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage2_Attention [label=Q_local]
	Layer2_Device15_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage2_Attention -> Layer2_Device15_Stage2_Accumulate
	Layer2_Device15_Stage1_Accumulate -> Layer2_Device15_Stage2_Accumulate
	Layer2_Device15_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage2_RecvKV -> Layer2_Device15_Stage3_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage3_RecvKV -> Layer2_Device15_Stage3_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage3_Attention [label=Q_local]
	Layer2_Device15_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage3_Attention -> Layer2_Device15_Stage3_Accumulate
	Layer2_Device15_Stage2_Accumulate -> Layer2_Device15_Stage3_Accumulate
	Layer2_Device15_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage3_RecvKV -> Layer2_Device15_Stage4_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage4_RecvKV -> Layer2_Device15_Stage4_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage4_Attention [label=Q_local]
	Layer2_Device15_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage4_Attention -> Layer2_Device15_Stage4_Accumulate
	Layer2_Device15_Stage3_Accumulate -> Layer2_Device15_Stage4_Accumulate
	Layer2_Device15_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage4_RecvKV -> Layer2_Device15_Stage5_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage5_RecvKV -> Layer2_Device15_Stage5_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage5_Attention [label=Q_local]
	Layer2_Device15_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage5_Attention -> Layer2_Device15_Stage5_Accumulate
	Layer2_Device15_Stage4_Accumulate -> Layer2_Device15_Stage5_Accumulate
	Layer2_Device15_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage5_RecvKV -> Layer2_Device15_Stage6_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage6_RecvKV -> Layer2_Device15_Stage6_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage6_Attention [label=Q_local]
	Layer2_Device15_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage6_Attention -> Layer2_Device15_Stage6_Accumulate
	Layer2_Device15_Stage5_Accumulate -> Layer2_Device15_Stage6_Accumulate
	Layer2_Device15_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage6_RecvKV -> Layer2_Device15_Stage7_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage7_RecvKV -> Layer2_Device15_Stage7_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage7_Attention [label=Q_local]
	Layer2_Device15_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage7_Attention -> Layer2_Device15_Stage7_Accumulate
	Layer2_Device15_Stage6_Accumulate -> Layer2_Device15_Stage7_Accumulate
	Layer2_Device15_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage7_RecvKV -> Layer2_Device15_Stage8_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage8_RecvKV -> Layer2_Device15_Stage8_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage8_Attention [label=Q_local]
	Layer2_Device15_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage8_Attention -> Layer2_Device15_Stage8_Accumulate
	Layer2_Device15_Stage7_Accumulate -> Layer2_Device15_Stage8_Accumulate
	Layer2_Device15_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage8_RecvKV -> Layer2_Device15_Stage9_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage9_RecvKV -> Layer2_Device15_Stage9_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage9_Attention [label=Q_local]
	Layer2_Device15_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage9_Attention -> Layer2_Device15_Stage9_Accumulate
	Layer2_Device15_Stage8_Accumulate -> Layer2_Device15_Stage9_Accumulate
	Layer2_Device15_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage9_RecvKV -> Layer2_Device15_Stage10_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage10_RecvKV -> Layer2_Device15_Stage10_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage10_Attention [label=Q_local]
	Layer2_Device15_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage10_Attention -> Layer2_Device15_Stage10_Accumulate
	Layer2_Device15_Stage9_Accumulate -> Layer2_Device15_Stage10_Accumulate
	Layer2_Device15_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage10_RecvKV -> Layer2_Device15_Stage11_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage11_RecvKV -> Layer2_Device15_Stage11_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage11_Attention [label=Q_local]
	Layer2_Device15_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage11_Attention -> Layer2_Device15_Stage11_Accumulate
	Layer2_Device15_Stage10_Accumulate -> Layer2_Device15_Stage11_Accumulate
	Layer2_Device15_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage11_RecvKV -> Layer2_Device15_Stage12_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage12_RecvKV -> Layer2_Device15_Stage12_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage12_Attention [label=Q_local]
	Layer2_Device15_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage12_Attention -> Layer2_Device15_Stage12_Accumulate
	Layer2_Device15_Stage11_Accumulate -> Layer2_Device15_Stage12_Accumulate
	Layer2_Device15_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage12_RecvKV -> Layer2_Device15_Stage13_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage13_RecvKV -> Layer2_Device15_Stage13_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage13_Attention [label=Q_local]
	Layer2_Device15_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage13_Attention -> Layer2_Device15_Stage13_Accumulate
	Layer2_Device15_Stage12_Accumulate -> Layer2_Device15_Stage13_Accumulate
	Layer2_Device15_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage13_RecvKV -> Layer2_Device15_Stage14_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage14_RecvKV -> Layer2_Device15_Stage14_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage14_Attention [label=Q_local]
	Layer2_Device15_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage14_Attention -> Layer2_Device15_Stage14_Accumulate
	Layer2_Device15_Stage13_Accumulate -> Layer2_Device15_Stage14_Accumulate
	Layer2_Device15_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer2_Device15_Stage14_RecvKV -> Layer2_Device15_Stage15_RecvKV [label="Ring transfer"]
	Layer2_Device15_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer2_Device15_Stage15_RecvKV -> Layer2_Device15_Stage15_Attention
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage15_Attention [label=Q_local]
	Layer2_Device15_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer2_Device15_Stage15_Attention -> Layer2_Device15_Stage15_Accumulate
	Layer2_Device15_Stage14_Accumulate -> Layer2_Device15_Stage15_Accumulate
	Layer2_Device15_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer2_Device15_Stage15_Accumulate -> Layer2_Device15_ConcatHeads
	Layer2_Device15_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device15_ConcatHeads -> Layer2_Device15_OutputProj
	Layer2_Device15_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device15_OutputProj -> Layer2_Device15_Residual1
	Layer2_Device15_Input -> Layer2_Device15_Residual1
	Layer2_Device15_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device15_Residual1 -> Layer2_Device15_LayerNorm2
	Layer2_Device15_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device15_LayerNorm2 -> Layer2_Device15_GateProj
	Layer2_Device15_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device15_LayerNorm2 -> Layer2_Device15_UpProj
	Layer2_Device15_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device15_GateProj -> Layer2_Device15_Activation
	Layer2_Device15_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer2_Device15_Activation -> Layer2_Device15_ElemMul
	Layer2_Device15_UpProj -> Layer2_Device15_ElemMul
	Layer2_Device15_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer2_Device15_ElemMul -> Layer2_Device15_DownProj
	Layer2_Device15_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer2_Device15_DownProj -> Layer2_Device15_Residual2
	Layer2_Device15_Residual1 -> Layer2_Device15_Residual2
	Layer2_Device15_Output [label="Layer 2 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device15_Residual2 -> Layer2_Device15_Output
	Layer3_Device0_Input [label="Layer 3 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device0_Output -> Layer3_Device0_Input
	Layer3_Device0_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device0_Input -> Layer3_Device0_LayerNorm1
	Layer3_Device0_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device0_LayerNorm1 -> Layer3_Device0_QKVProj
	Layer3_Device0_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device0_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage0_RecvKV -> Layer3_Device0_Stage0_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage0_Attention [label=Q_local]
	Layer3_Device0_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage0_Attention -> Layer3_Device0_Stage0_Accumulate
	Layer3_Device0_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage0_RecvKV -> Layer3_Device0_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage1_RecvKV -> Layer3_Device0_Stage1_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage1_Attention [label=Q_local]
	Layer3_Device0_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage1_Attention -> Layer3_Device0_Stage1_Accumulate
	Layer3_Device0_Stage0_Accumulate -> Layer3_Device0_Stage1_Accumulate
	Layer3_Device0_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage1_RecvKV -> Layer3_Device0_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage2_RecvKV -> Layer3_Device0_Stage2_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage2_Attention [label=Q_local]
	Layer3_Device0_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage2_Attention -> Layer3_Device0_Stage2_Accumulate
	Layer3_Device0_Stage1_Accumulate -> Layer3_Device0_Stage2_Accumulate
	Layer3_Device0_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage2_RecvKV -> Layer3_Device0_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage3_RecvKV -> Layer3_Device0_Stage3_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage3_Attention [label=Q_local]
	Layer3_Device0_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage3_Attention -> Layer3_Device0_Stage3_Accumulate
	Layer3_Device0_Stage2_Accumulate -> Layer3_Device0_Stage3_Accumulate
	Layer3_Device0_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage3_RecvKV -> Layer3_Device0_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage4_RecvKV -> Layer3_Device0_Stage4_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage4_Attention [label=Q_local]
	Layer3_Device0_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage4_Attention -> Layer3_Device0_Stage4_Accumulate
	Layer3_Device0_Stage3_Accumulate -> Layer3_Device0_Stage4_Accumulate
	Layer3_Device0_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage4_RecvKV -> Layer3_Device0_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage5_RecvKV -> Layer3_Device0_Stage5_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage5_Attention [label=Q_local]
	Layer3_Device0_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage5_Attention -> Layer3_Device0_Stage5_Accumulate
	Layer3_Device0_Stage4_Accumulate -> Layer3_Device0_Stage5_Accumulate
	Layer3_Device0_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage5_RecvKV -> Layer3_Device0_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage6_RecvKV -> Layer3_Device0_Stage6_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage6_Attention [label=Q_local]
	Layer3_Device0_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage6_Attention -> Layer3_Device0_Stage6_Accumulate
	Layer3_Device0_Stage5_Accumulate -> Layer3_Device0_Stage6_Accumulate
	Layer3_Device0_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage6_RecvKV -> Layer3_Device0_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage7_RecvKV -> Layer3_Device0_Stage7_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage7_Attention [label=Q_local]
	Layer3_Device0_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage7_Attention -> Layer3_Device0_Stage7_Accumulate
	Layer3_Device0_Stage6_Accumulate -> Layer3_Device0_Stage7_Accumulate
	Layer3_Device0_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage7_RecvKV -> Layer3_Device0_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage8_RecvKV -> Layer3_Device0_Stage8_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage8_Attention [label=Q_local]
	Layer3_Device0_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage8_Attention -> Layer3_Device0_Stage8_Accumulate
	Layer3_Device0_Stage7_Accumulate -> Layer3_Device0_Stage8_Accumulate
	Layer3_Device0_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage8_RecvKV -> Layer3_Device0_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage9_RecvKV -> Layer3_Device0_Stage9_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage9_Attention [label=Q_local]
	Layer3_Device0_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage9_Attention -> Layer3_Device0_Stage9_Accumulate
	Layer3_Device0_Stage8_Accumulate -> Layer3_Device0_Stage9_Accumulate
	Layer3_Device0_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage9_RecvKV -> Layer3_Device0_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage10_RecvKV -> Layer3_Device0_Stage10_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage10_Attention [label=Q_local]
	Layer3_Device0_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage10_Attention -> Layer3_Device0_Stage10_Accumulate
	Layer3_Device0_Stage9_Accumulate -> Layer3_Device0_Stage10_Accumulate
	Layer3_Device0_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage10_RecvKV -> Layer3_Device0_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage11_RecvKV -> Layer3_Device0_Stage11_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage11_Attention [label=Q_local]
	Layer3_Device0_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage11_Attention -> Layer3_Device0_Stage11_Accumulate
	Layer3_Device0_Stage10_Accumulate -> Layer3_Device0_Stage11_Accumulate
	Layer3_Device0_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage11_RecvKV -> Layer3_Device0_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage12_RecvKV -> Layer3_Device0_Stage12_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage12_Attention [label=Q_local]
	Layer3_Device0_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage12_Attention -> Layer3_Device0_Stage12_Accumulate
	Layer3_Device0_Stage11_Accumulate -> Layer3_Device0_Stage12_Accumulate
	Layer3_Device0_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage12_RecvKV -> Layer3_Device0_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage13_RecvKV -> Layer3_Device0_Stage13_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage13_Attention [label=Q_local]
	Layer3_Device0_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage13_Attention -> Layer3_Device0_Stage13_Accumulate
	Layer3_Device0_Stage12_Accumulate -> Layer3_Device0_Stage13_Accumulate
	Layer3_Device0_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage13_RecvKV -> Layer3_Device0_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage14_RecvKV -> Layer3_Device0_Stage14_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage14_Attention [label=Q_local]
	Layer3_Device0_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage14_Attention -> Layer3_Device0_Stage14_Accumulate
	Layer3_Device0_Stage13_Accumulate -> Layer3_Device0_Stage14_Accumulate
	Layer3_Device0_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device0_Stage14_RecvKV -> Layer3_Device0_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device0_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device0_Stage15_RecvKV -> Layer3_Device0_Stage15_Attention
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage15_Attention [label=Q_local]
	Layer3_Device0_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device0_Stage15_Attention -> Layer3_Device0_Stage15_Accumulate
	Layer3_Device0_Stage14_Accumulate -> Layer3_Device0_Stage15_Accumulate
	Layer3_Device0_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device0_Stage15_Accumulate -> Layer3_Device0_ConcatHeads
	Layer3_Device0_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device0_ConcatHeads -> Layer3_Device0_OutputProj
	Layer3_Device0_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device0_OutputProj -> Layer3_Device0_Residual1
	Layer3_Device0_Input -> Layer3_Device0_Residual1
	Layer3_Device0_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device0_Residual1 -> Layer3_Device0_LayerNorm2
	Layer3_Device0_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device0_LayerNorm2 -> Layer3_Device0_GateProj
	Layer3_Device0_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device0_LayerNorm2 -> Layer3_Device0_UpProj
	Layer3_Device0_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device0_GateProj -> Layer3_Device0_Activation
	Layer3_Device0_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device0_Activation -> Layer3_Device0_ElemMul
	Layer3_Device0_UpProj -> Layer3_Device0_ElemMul
	Layer3_Device0_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device0_ElemMul -> Layer3_Device0_DownProj
	Layer3_Device0_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device0_DownProj -> Layer3_Device0_Residual2
	Layer3_Device0_Residual1 -> Layer3_Device0_Residual2
	Layer3_Device0_Output [label="Layer 3 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device0_Residual2 -> Layer3_Device0_Output
	Layer3_Device1_Input [label="Layer 3 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device1_Output -> Layer3_Device1_Input
	Layer3_Device1_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device1_Input -> Layer3_Device1_LayerNorm1
	Layer3_Device1_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device1_LayerNorm1 -> Layer3_Device1_QKVProj
	Layer3_Device1_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device1_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage0_RecvKV -> Layer3_Device1_Stage0_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage0_Attention [label=Q_local]
	Layer3_Device1_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage0_Attention -> Layer3_Device1_Stage0_Accumulate
	Layer3_Device1_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage0_RecvKV -> Layer3_Device1_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage1_RecvKV -> Layer3_Device1_Stage1_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage1_Attention [label=Q_local]
	Layer3_Device1_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage1_Attention -> Layer3_Device1_Stage1_Accumulate
	Layer3_Device1_Stage0_Accumulate -> Layer3_Device1_Stage1_Accumulate
	Layer3_Device1_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage1_RecvKV -> Layer3_Device1_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage2_RecvKV -> Layer3_Device1_Stage2_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage2_Attention [label=Q_local]
	Layer3_Device1_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage2_Attention -> Layer3_Device1_Stage2_Accumulate
	Layer3_Device1_Stage1_Accumulate -> Layer3_Device1_Stage2_Accumulate
	Layer3_Device1_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage2_RecvKV -> Layer3_Device1_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage3_RecvKV -> Layer3_Device1_Stage3_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage3_Attention [label=Q_local]
	Layer3_Device1_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage3_Attention -> Layer3_Device1_Stage3_Accumulate
	Layer3_Device1_Stage2_Accumulate -> Layer3_Device1_Stage3_Accumulate
	Layer3_Device1_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage3_RecvKV -> Layer3_Device1_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage4_RecvKV -> Layer3_Device1_Stage4_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage4_Attention [label=Q_local]
	Layer3_Device1_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage4_Attention -> Layer3_Device1_Stage4_Accumulate
	Layer3_Device1_Stage3_Accumulate -> Layer3_Device1_Stage4_Accumulate
	Layer3_Device1_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage4_RecvKV -> Layer3_Device1_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage5_RecvKV -> Layer3_Device1_Stage5_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage5_Attention [label=Q_local]
	Layer3_Device1_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage5_Attention -> Layer3_Device1_Stage5_Accumulate
	Layer3_Device1_Stage4_Accumulate -> Layer3_Device1_Stage5_Accumulate
	Layer3_Device1_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage5_RecvKV -> Layer3_Device1_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage6_RecvKV -> Layer3_Device1_Stage6_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage6_Attention [label=Q_local]
	Layer3_Device1_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage6_Attention -> Layer3_Device1_Stage6_Accumulate
	Layer3_Device1_Stage5_Accumulate -> Layer3_Device1_Stage6_Accumulate
	Layer3_Device1_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage6_RecvKV -> Layer3_Device1_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage7_RecvKV -> Layer3_Device1_Stage7_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage7_Attention [label=Q_local]
	Layer3_Device1_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage7_Attention -> Layer3_Device1_Stage7_Accumulate
	Layer3_Device1_Stage6_Accumulate -> Layer3_Device1_Stage7_Accumulate
	Layer3_Device1_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage7_RecvKV -> Layer3_Device1_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage8_RecvKV -> Layer3_Device1_Stage8_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage8_Attention [label=Q_local]
	Layer3_Device1_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage8_Attention -> Layer3_Device1_Stage8_Accumulate
	Layer3_Device1_Stage7_Accumulate -> Layer3_Device1_Stage8_Accumulate
	Layer3_Device1_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage8_RecvKV -> Layer3_Device1_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage9_RecvKV -> Layer3_Device1_Stage9_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage9_Attention [label=Q_local]
	Layer3_Device1_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage9_Attention -> Layer3_Device1_Stage9_Accumulate
	Layer3_Device1_Stage8_Accumulate -> Layer3_Device1_Stage9_Accumulate
	Layer3_Device1_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage9_RecvKV -> Layer3_Device1_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage10_RecvKV -> Layer3_Device1_Stage10_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage10_Attention [label=Q_local]
	Layer3_Device1_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage10_Attention -> Layer3_Device1_Stage10_Accumulate
	Layer3_Device1_Stage9_Accumulate -> Layer3_Device1_Stage10_Accumulate
	Layer3_Device1_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage10_RecvKV -> Layer3_Device1_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage11_RecvKV -> Layer3_Device1_Stage11_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage11_Attention [label=Q_local]
	Layer3_Device1_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage11_Attention -> Layer3_Device1_Stage11_Accumulate
	Layer3_Device1_Stage10_Accumulate -> Layer3_Device1_Stage11_Accumulate
	Layer3_Device1_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage11_RecvKV -> Layer3_Device1_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage12_RecvKV -> Layer3_Device1_Stage12_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage12_Attention [label=Q_local]
	Layer3_Device1_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage12_Attention -> Layer3_Device1_Stage12_Accumulate
	Layer3_Device1_Stage11_Accumulate -> Layer3_Device1_Stage12_Accumulate
	Layer3_Device1_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage12_RecvKV -> Layer3_Device1_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage13_RecvKV -> Layer3_Device1_Stage13_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage13_Attention [label=Q_local]
	Layer3_Device1_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage13_Attention -> Layer3_Device1_Stage13_Accumulate
	Layer3_Device1_Stage12_Accumulate -> Layer3_Device1_Stage13_Accumulate
	Layer3_Device1_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage13_RecvKV -> Layer3_Device1_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage14_RecvKV -> Layer3_Device1_Stage14_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage14_Attention [label=Q_local]
	Layer3_Device1_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage14_Attention -> Layer3_Device1_Stage14_Accumulate
	Layer3_Device1_Stage13_Accumulate -> Layer3_Device1_Stage14_Accumulate
	Layer3_Device1_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device1_Stage14_RecvKV -> Layer3_Device1_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device1_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device1_Stage15_RecvKV -> Layer3_Device1_Stage15_Attention
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage15_Attention [label=Q_local]
	Layer3_Device1_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device1_Stage15_Attention -> Layer3_Device1_Stage15_Accumulate
	Layer3_Device1_Stage14_Accumulate -> Layer3_Device1_Stage15_Accumulate
	Layer3_Device1_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device1_Stage15_Accumulate -> Layer3_Device1_ConcatHeads
	Layer3_Device1_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device1_ConcatHeads -> Layer3_Device1_OutputProj
	Layer3_Device1_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device1_OutputProj -> Layer3_Device1_Residual1
	Layer3_Device1_Input -> Layer3_Device1_Residual1
	Layer3_Device1_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device1_Residual1 -> Layer3_Device1_LayerNorm2
	Layer3_Device1_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device1_LayerNorm2 -> Layer3_Device1_GateProj
	Layer3_Device1_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device1_LayerNorm2 -> Layer3_Device1_UpProj
	Layer3_Device1_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device1_GateProj -> Layer3_Device1_Activation
	Layer3_Device1_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device1_Activation -> Layer3_Device1_ElemMul
	Layer3_Device1_UpProj -> Layer3_Device1_ElemMul
	Layer3_Device1_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device1_ElemMul -> Layer3_Device1_DownProj
	Layer3_Device1_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device1_DownProj -> Layer3_Device1_Residual2
	Layer3_Device1_Residual1 -> Layer3_Device1_Residual2
	Layer3_Device1_Output [label="Layer 3 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device1_Residual2 -> Layer3_Device1_Output
	Layer3_Device2_Input [label="Layer 3 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device2_Output -> Layer3_Device2_Input
	Layer3_Device2_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device2_Input -> Layer3_Device2_LayerNorm1
	Layer3_Device2_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device2_LayerNorm1 -> Layer3_Device2_QKVProj
	Layer3_Device2_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device2_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage0_RecvKV -> Layer3_Device2_Stage0_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage0_Attention [label=Q_local]
	Layer3_Device2_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage0_Attention -> Layer3_Device2_Stage0_Accumulate
	Layer3_Device2_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage0_RecvKV -> Layer3_Device2_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage1_RecvKV -> Layer3_Device2_Stage1_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage1_Attention [label=Q_local]
	Layer3_Device2_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage1_Attention -> Layer3_Device2_Stage1_Accumulate
	Layer3_Device2_Stage0_Accumulate -> Layer3_Device2_Stage1_Accumulate
	Layer3_Device2_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage1_RecvKV -> Layer3_Device2_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage2_RecvKV -> Layer3_Device2_Stage2_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage2_Attention [label=Q_local]
	Layer3_Device2_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage2_Attention -> Layer3_Device2_Stage2_Accumulate
	Layer3_Device2_Stage1_Accumulate -> Layer3_Device2_Stage2_Accumulate
	Layer3_Device2_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage2_RecvKV -> Layer3_Device2_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage3_RecvKV -> Layer3_Device2_Stage3_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage3_Attention [label=Q_local]
	Layer3_Device2_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage3_Attention -> Layer3_Device2_Stage3_Accumulate
	Layer3_Device2_Stage2_Accumulate -> Layer3_Device2_Stage3_Accumulate
	Layer3_Device2_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage3_RecvKV -> Layer3_Device2_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage4_RecvKV -> Layer3_Device2_Stage4_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage4_Attention [label=Q_local]
	Layer3_Device2_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage4_Attention -> Layer3_Device2_Stage4_Accumulate
	Layer3_Device2_Stage3_Accumulate -> Layer3_Device2_Stage4_Accumulate
	Layer3_Device2_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage4_RecvKV -> Layer3_Device2_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage5_RecvKV -> Layer3_Device2_Stage5_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage5_Attention [label=Q_local]
	Layer3_Device2_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage5_Attention -> Layer3_Device2_Stage5_Accumulate
	Layer3_Device2_Stage4_Accumulate -> Layer3_Device2_Stage5_Accumulate
	Layer3_Device2_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage5_RecvKV -> Layer3_Device2_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage6_RecvKV -> Layer3_Device2_Stage6_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage6_Attention [label=Q_local]
	Layer3_Device2_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage6_Attention -> Layer3_Device2_Stage6_Accumulate
	Layer3_Device2_Stage5_Accumulate -> Layer3_Device2_Stage6_Accumulate
	Layer3_Device2_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage6_RecvKV -> Layer3_Device2_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage7_RecvKV -> Layer3_Device2_Stage7_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage7_Attention [label=Q_local]
	Layer3_Device2_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage7_Attention -> Layer3_Device2_Stage7_Accumulate
	Layer3_Device2_Stage6_Accumulate -> Layer3_Device2_Stage7_Accumulate
	Layer3_Device2_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage7_RecvKV -> Layer3_Device2_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage8_RecvKV -> Layer3_Device2_Stage8_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage8_Attention [label=Q_local]
	Layer3_Device2_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage8_Attention -> Layer3_Device2_Stage8_Accumulate
	Layer3_Device2_Stage7_Accumulate -> Layer3_Device2_Stage8_Accumulate
	Layer3_Device2_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage8_RecvKV -> Layer3_Device2_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage9_RecvKV -> Layer3_Device2_Stage9_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage9_Attention [label=Q_local]
	Layer3_Device2_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage9_Attention -> Layer3_Device2_Stage9_Accumulate
	Layer3_Device2_Stage8_Accumulate -> Layer3_Device2_Stage9_Accumulate
	Layer3_Device2_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage9_RecvKV -> Layer3_Device2_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage10_RecvKV -> Layer3_Device2_Stage10_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage10_Attention [label=Q_local]
	Layer3_Device2_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage10_Attention -> Layer3_Device2_Stage10_Accumulate
	Layer3_Device2_Stage9_Accumulate -> Layer3_Device2_Stage10_Accumulate
	Layer3_Device2_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage10_RecvKV -> Layer3_Device2_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage11_RecvKV -> Layer3_Device2_Stage11_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage11_Attention [label=Q_local]
	Layer3_Device2_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage11_Attention -> Layer3_Device2_Stage11_Accumulate
	Layer3_Device2_Stage10_Accumulate -> Layer3_Device2_Stage11_Accumulate
	Layer3_Device2_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage11_RecvKV -> Layer3_Device2_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage12_RecvKV -> Layer3_Device2_Stage12_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage12_Attention [label=Q_local]
	Layer3_Device2_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage12_Attention -> Layer3_Device2_Stage12_Accumulate
	Layer3_Device2_Stage11_Accumulate -> Layer3_Device2_Stage12_Accumulate
	Layer3_Device2_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage12_RecvKV -> Layer3_Device2_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage13_RecvKV -> Layer3_Device2_Stage13_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage13_Attention [label=Q_local]
	Layer3_Device2_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage13_Attention -> Layer3_Device2_Stage13_Accumulate
	Layer3_Device2_Stage12_Accumulate -> Layer3_Device2_Stage13_Accumulate
	Layer3_Device2_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage13_RecvKV -> Layer3_Device2_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage14_RecvKV -> Layer3_Device2_Stage14_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage14_Attention [label=Q_local]
	Layer3_Device2_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage14_Attention -> Layer3_Device2_Stage14_Accumulate
	Layer3_Device2_Stage13_Accumulate -> Layer3_Device2_Stage14_Accumulate
	Layer3_Device2_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device2_Stage14_RecvKV -> Layer3_Device2_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device2_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device2_Stage15_RecvKV -> Layer3_Device2_Stage15_Attention
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage15_Attention [label=Q_local]
	Layer3_Device2_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device2_Stage15_Attention -> Layer3_Device2_Stage15_Accumulate
	Layer3_Device2_Stage14_Accumulate -> Layer3_Device2_Stage15_Accumulate
	Layer3_Device2_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device2_Stage15_Accumulate -> Layer3_Device2_ConcatHeads
	Layer3_Device2_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device2_ConcatHeads -> Layer3_Device2_OutputProj
	Layer3_Device2_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device2_OutputProj -> Layer3_Device2_Residual1
	Layer3_Device2_Input -> Layer3_Device2_Residual1
	Layer3_Device2_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device2_Residual1 -> Layer3_Device2_LayerNorm2
	Layer3_Device2_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device2_LayerNorm2 -> Layer3_Device2_GateProj
	Layer3_Device2_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device2_LayerNorm2 -> Layer3_Device2_UpProj
	Layer3_Device2_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device2_GateProj -> Layer3_Device2_Activation
	Layer3_Device2_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device2_Activation -> Layer3_Device2_ElemMul
	Layer3_Device2_UpProj -> Layer3_Device2_ElemMul
	Layer3_Device2_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device2_ElemMul -> Layer3_Device2_DownProj
	Layer3_Device2_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device2_DownProj -> Layer3_Device2_Residual2
	Layer3_Device2_Residual1 -> Layer3_Device2_Residual2
	Layer3_Device2_Output [label="Layer 3 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device2_Residual2 -> Layer3_Device2_Output
	Layer3_Device3_Input [label="Layer 3 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device3_Output -> Layer3_Device3_Input
	Layer3_Device3_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device3_Input -> Layer3_Device3_LayerNorm1
	Layer3_Device3_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device3_LayerNorm1 -> Layer3_Device3_QKVProj
	Layer3_Device3_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device3_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage0_RecvKV -> Layer3_Device3_Stage0_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage0_Attention [label=Q_local]
	Layer3_Device3_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage0_Attention -> Layer3_Device3_Stage0_Accumulate
	Layer3_Device3_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage0_RecvKV -> Layer3_Device3_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage1_RecvKV -> Layer3_Device3_Stage1_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage1_Attention [label=Q_local]
	Layer3_Device3_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage1_Attention -> Layer3_Device3_Stage1_Accumulate
	Layer3_Device3_Stage0_Accumulate -> Layer3_Device3_Stage1_Accumulate
	Layer3_Device3_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage1_RecvKV -> Layer3_Device3_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage2_RecvKV -> Layer3_Device3_Stage2_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage2_Attention [label=Q_local]
	Layer3_Device3_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage2_Attention -> Layer3_Device3_Stage2_Accumulate
	Layer3_Device3_Stage1_Accumulate -> Layer3_Device3_Stage2_Accumulate
	Layer3_Device3_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage2_RecvKV -> Layer3_Device3_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage3_RecvKV -> Layer3_Device3_Stage3_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage3_Attention [label=Q_local]
	Layer3_Device3_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage3_Attention -> Layer3_Device3_Stage3_Accumulate
	Layer3_Device3_Stage2_Accumulate -> Layer3_Device3_Stage3_Accumulate
	Layer3_Device3_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage3_RecvKV -> Layer3_Device3_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage4_RecvKV -> Layer3_Device3_Stage4_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage4_Attention [label=Q_local]
	Layer3_Device3_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage4_Attention -> Layer3_Device3_Stage4_Accumulate
	Layer3_Device3_Stage3_Accumulate -> Layer3_Device3_Stage4_Accumulate
	Layer3_Device3_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage4_RecvKV -> Layer3_Device3_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage5_RecvKV -> Layer3_Device3_Stage5_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage5_Attention [label=Q_local]
	Layer3_Device3_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage5_Attention -> Layer3_Device3_Stage5_Accumulate
	Layer3_Device3_Stage4_Accumulate -> Layer3_Device3_Stage5_Accumulate
	Layer3_Device3_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage5_RecvKV -> Layer3_Device3_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage6_RecvKV -> Layer3_Device3_Stage6_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage6_Attention [label=Q_local]
	Layer3_Device3_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage6_Attention -> Layer3_Device3_Stage6_Accumulate
	Layer3_Device3_Stage5_Accumulate -> Layer3_Device3_Stage6_Accumulate
	Layer3_Device3_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage6_RecvKV -> Layer3_Device3_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage7_RecvKV -> Layer3_Device3_Stage7_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage7_Attention [label=Q_local]
	Layer3_Device3_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage7_Attention -> Layer3_Device3_Stage7_Accumulate
	Layer3_Device3_Stage6_Accumulate -> Layer3_Device3_Stage7_Accumulate
	Layer3_Device3_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage7_RecvKV -> Layer3_Device3_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage8_RecvKV -> Layer3_Device3_Stage8_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage8_Attention [label=Q_local]
	Layer3_Device3_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage8_Attention -> Layer3_Device3_Stage8_Accumulate
	Layer3_Device3_Stage7_Accumulate -> Layer3_Device3_Stage8_Accumulate
	Layer3_Device3_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage8_RecvKV -> Layer3_Device3_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage9_RecvKV -> Layer3_Device3_Stage9_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage9_Attention [label=Q_local]
	Layer3_Device3_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage9_Attention -> Layer3_Device3_Stage9_Accumulate
	Layer3_Device3_Stage8_Accumulate -> Layer3_Device3_Stage9_Accumulate
	Layer3_Device3_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage9_RecvKV -> Layer3_Device3_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage10_RecvKV -> Layer3_Device3_Stage10_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage10_Attention [label=Q_local]
	Layer3_Device3_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage10_Attention -> Layer3_Device3_Stage10_Accumulate
	Layer3_Device3_Stage9_Accumulate -> Layer3_Device3_Stage10_Accumulate
	Layer3_Device3_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage10_RecvKV -> Layer3_Device3_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage11_RecvKV -> Layer3_Device3_Stage11_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage11_Attention [label=Q_local]
	Layer3_Device3_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage11_Attention -> Layer3_Device3_Stage11_Accumulate
	Layer3_Device3_Stage10_Accumulate -> Layer3_Device3_Stage11_Accumulate
	Layer3_Device3_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage11_RecvKV -> Layer3_Device3_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage12_RecvKV -> Layer3_Device3_Stage12_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage12_Attention [label=Q_local]
	Layer3_Device3_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage12_Attention -> Layer3_Device3_Stage12_Accumulate
	Layer3_Device3_Stage11_Accumulate -> Layer3_Device3_Stage12_Accumulate
	Layer3_Device3_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage12_RecvKV -> Layer3_Device3_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage13_RecvKV -> Layer3_Device3_Stage13_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage13_Attention [label=Q_local]
	Layer3_Device3_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage13_Attention -> Layer3_Device3_Stage13_Accumulate
	Layer3_Device3_Stage12_Accumulate -> Layer3_Device3_Stage13_Accumulate
	Layer3_Device3_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage13_RecvKV -> Layer3_Device3_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage14_RecvKV -> Layer3_Device3_Stage14_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage14_Attention [label=Q_local]
	Layer3_Device3_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage14_Attention -> Layer3_Device3_Stage14_Accumulate
	Layer3_Device3_Stage13_Accumulate -> Layer3_Device3_Stage14_Accumulate
	Layer3_Device3_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device3_Stage14_RecvKV -> Layer3_Device3_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device3_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device3_Stage15_RecvKV -> Layer3_Device3_Stage15_Attention
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage15_Attention [label=Q_local]
	Layer3_Device3_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device3_Stage15_Attention -> Layer3_Device3_Stage15_Accumulate
	Layer3_Device3_Stage14_Accumulate -> Layer3_Device3_Stage15_Accumulate
	Layer3_Device3_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device3_Stage15_Accumulate -> Layer3_Device3_ConcatHeads
	Layer3_Device3_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device3_ConcatHeads -> Layer3_Device3_OutputProj
	Layer3_Device3_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device3_OutputProj -> Layer3_Device3_Residual1
	Layer3_Device3_Input -> Layer3_Device3_Residual1
	Layer3_Device3_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device3_Residual1 -> Layer3_Device3_LayerNorm2
	Layer3_Device3_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device3_LayerNorm2 -> Layer3_Device3_GateProj
	Layer3_Device3_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device3_LayerNorm2 -> Layer3_Device3_UpProj
	Layer3_Device3_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device3_GateProj -> Layer3_Device3_Activation
	Layer3_Device3_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device3_Activation -> Layer3_Device3_ElemMul
	Layer3_Device3_UpProj -> Layer3_Device3_ElemMul
	Layer3_Device3_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device3_ElemMul -> Layer3_Device3_DownProj
	Layer3_Device3_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device3_DownProj -> Layer3_Device3_Residual2
	Layer3_Device3_Residual1 -> Layer3_Device3_Residual2
	Layer3_Device3_Output [label="Layer 3 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device3_Residual2 -> Layer3_Device3_Output
	Layer3_Device4_Input [label="Layer 3 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device4_Output -> Layer3_Device4_Input
	Layer3_Device4_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device4_Input -> Layer3_Device4_LayerNorm1
	Layer3_Device4_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device4_LayerNorm1 -> Layer3_Device4_QKVProj
	Layer3_Device4_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device4_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage0_RecvKV -> Layer3_Device4_Stage0_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage0_Attention [label=Q_local]
	Layer3_Device4_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage0_Attention -> Layer3_Device4_Stage0_Accumulate
	Layer3_Device4_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage0_RecvKV -> Layer3_Device4_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage1_RecvKV -> Layer3_Device4_Stage1_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage1_Attention [label=Q_local]
	Layer3_Device4_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage1_Attention -> Layer3_Device4_Stage1_Accumulate
	Layer3_Device4_Stage0_Accumulate -> Layer3_Device4_Stage1_Accumulate
	Layer3_Device4_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage1_RecvKV -> Layer3_Device4_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage2_RecvKV -> Layer3_Device4_Stage2_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage2_Attention [label=Q_local]
	Layer3_Device4_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage2_Attention -> Layer3_Device4_Stage2_Accumulate
	Layer3_Device4_Stage1_Accumulate -> Layer3_Device4_Stage2_Accumulate
	Layer3_Device4_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage2_RecvKV -> Layer3_Device4_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage3_RecvKV -> Layer3_Device4_Stage3_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage3_Attention [label=Q_local]
	Layer3_Device4_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage3_Attention -> Layer3_Device4_Stage3_Accumulate
	Layer3_Device4_Stage2_Accumulate -> Layer3_Device4_Stage3_Accumulate
	Layer3_Device4_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage3_RecvKV -> Layer3_Device4_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage4_RecvKV -> Layer3_Device4_Stage4_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage4_Attention [label=Q_local]
	Layer3_Device4_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage4_Attention -> Layer3_Device4_Stage4_Accumulate
	Layer3_Device4_Stage3_Accumulate -> Layer3_Device4_Stage4_Accumulate
	Layer3_Device4_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage4_RecvKV -> Layer3_Device4_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage5_RecvKV -> Layer3_Device4_Stage5_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage5_Attention [label=Q_local]
	Layer3_Device4_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage5_Attention -> Layer3_Device4_Stage5_Accumulate
	Layer3_Device4_Stage4_Accumulate -> Layer3_Device4_Stage5_Accumulate
	Layer3_Device4_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage5_RecvKV -> Layer3_Device4_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage6_RecvKV -> Layer3_Device4_Stage6_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage6_Attention [label=Q_local]
	Layer3_Device4_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage6_Attention -> Layer3_Device4_Stage6_Accumulate
	Layer3_Device4_Stage5_Accumulate -> Layer3_Device4_Stage6_Accumulate
	Layer3_Device4_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage6_RecvKV -> Layer3_Device4_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage7_RecvKV -> Layer3_Device4_Stage7_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage7_Attention [label=Q_local]
	Layer3_Device4_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage7_Attention -> Layer3_Device4_Stage7_Accumulate
	Layer3_Device4_Stage6_Accumulate -> Layer3_Device4_Stage7_Accumulate
	Layer3_Device4_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage7_RecvKV -> Layer3_Device4_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage8_RecvKV -> Layer3_Device4_Stage8_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage8_Attention [label=Q_local]
	Layer3_Device4_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage8_Attention -> Layer3_Device4_Stage8_Accumulate
	Layer3_Device4_Stage7_Accumulate -> Layer3_Device4_Stage8_Accumulate
	Layer3_Device4_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage8_RecvKV -> Layer3_Device4_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage9_RecvKV -> Layer3_Device4_Stage9_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage9_Attention [label=Q_local]
	Layer3_Device4_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage9_Attention -> Layer3_Device4_Stage9_Accumulate
	Layer3_Device4_Stage8_Accumulate -> Layer3_Device4_Stage9_Accumulate
	Layer3_Device4_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage9_RecvKV -> Layer3_Device4_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage10_RecvKV -> Layer3_Device4_Stage10_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage10_Attention [label=Q_local]
	Layer3_Device4_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage10_Attention -> Layer3_Device4_Stage10_Accumulate
	Layer3_Device4_Stage9_Accumulate -> Layer3_Device4_Stage10_Accumulate
	Layer3_Device4_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage10_RecvKV -> Layer3_Device4_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage11_RecvKV -> Layer3_Device4_Stage11_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage11_Attention [label=Q_local]
	Layer3_Device4_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage11_Attention -> Layer3_Device4_Stage11_Accumulate
	Layer3_Device4_Stage10_Accumulate -> Layer3_Device4_Stage11_Accumulate
	Layer3_Device4_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage11_RecvKV -> Layer3_Device4_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage12_RecvKV -> Layer3_Device4_Stage12_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage12_Attention [label=Q_local]
	Layer3_Device4_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage12_Attention -> Layer3_Device4_Stage12_Accumulate
	Layer3_Device4_Stage11_Accumulate -> Layer3_Device4_Stage12_Accumulate
	Layer3_Device4_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage12_RecvKV -> Layer3_Device4_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage13_RecvKV -> Layer3_Device4_Stage13_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage13_Attention [label=Q_local]
	Layer3_Device4_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage13_Attention -> Layer3_Device4_Stage13_Accumulate
	Layer3_Device4_Stage12_Accumulate -> Layer3_Device4_Stage13_Accumulate
	Layer3_Device4_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage13_RecvKV -> Layer3_Device4_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage14_RecvKV -> Layer3_Device4_Stage14_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage14_Attention [label=Q_local]
	Layer3_Device4_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage14_Attention -> Layer3_Device4_Stage14_Accumulate
	Layer3_Device4_Stage13_Accumulate -> Layer3_Device4_Stage14_Accumulate
	Layer3_Device4_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device4_Stage14_RecvKV -> Layer3_Device4_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device4_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device4_Stage15_RecvKV -> Layer3_Device4_Stage15_Attention
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage15_Attention [label=Q_local]
	Layer3_Device4_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device4_Stage15_Attention -> Layer3_Device4_Stage15_Accumulate
	Layer3_Device4_Stage14_Accumulate -> Layer3_Device4_Stage15_Accumulate
	Layer3_Device4_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device4_Stage15_Accumulate -> Layer3_Device4_ConcatHeads
	Layer3_Device4_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device4_ConcatHeads -> Layer3_Device4_OutputProj
	Layer3_Device4_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device4_OutputProj -> Layer3_Device4_Residual1
	Layer3_Device4_Input -> Layer3_Device4_Residual1
	Layer3_Device4_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device4_Residual1 -> Layer3_Device4_LayerNorm2
	Layer3_Device4_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device4_LayerNorm2 -> Layer3_Device4_GateProj
	Layer3_Device4_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device4_LayerNorm2 -> Layer3_Device4_UpProj
	Layer3_Device4_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device4_GateProj -> Layer3_Device4_Activation
	Layer3_Device4_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device4_Activation -> Layer3_Device4_ElemMul
	Layer3_Device4_UpProj -> Layer3_Device4_ElemMul
	Layer3_Device4_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device4_ElemMul -> Layer3_Device4_DownProj
	Layer3_Device4_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device4_DownProj -> Layer3_Device4_Residual2
	Layer3_Device4_Residual1 -> Layer3_Device4_Residual2
	Layer3_Device4_Output [label="Layer 3 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device4_Residual2 -> Layer3_Device4_Output
	Layer3_Device5_Input [label="Layer 3 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device5_Output -> Layer3_Device5_Input
	Layer3_Device5_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device5_Input -> Layer3_Device5_LayerNorm1
	Layer3_Device5_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device5_LayerNorm1 -> Layer3_Device5_QKVProj
	Layer3_Device5_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device5_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage0_RecvKV -> Layer3_Device5_Stage0_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage0_Attention [label=Q_local]
	Layer3_Device5_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage0_Attention -> Layer3_Device5_Stage0_Accumulate
	Layer3_Device5_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage0_RecvKV -> Layer3_Device5_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage1_RecvKV -> Layer3_Device5_Stage1_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage1_Attention [label=Q_local]
	Layer3_Device5_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage1_Attention -> Layer3_Device5_Stage1_Accumulate
	Layer3_Device5_Stage0_Accumulate -> Layer3_Device5_Stage1_Accumulate
	Layer3_Device5_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage1_RecvKV -> Layer3_Device5_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage2_RecvKV -> Layer3_Device5_Stage2_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage2_Attention [label=Q_local]
	Layer3_Device5_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage2_Attention -> Layer3_Device5_Stage2_Accumulate
	Layer3_Device5_Stage1_Accumulate -> Layer3_Device5_Stage2_Accumulate
	Layer3_Device5_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage2_RecvKV -> Layer3_Device5_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage3_RecvKV -> Layer3_Device5_Stage3_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage3_Attention [label=Q_local]
	Layer3_Device5_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage3_Attention -> Layer3_Device5_Stage3_Accumulate
	Layer3_Device5_Stage2_Accumulate -> Layer3_Device5_Stage3_Accumulate
	Layer3_Device5_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage3_RecvKV -> Layer3_Device5_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage4_RecvKV -> Layer3_Device5_Stage4_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage4_Attention [label=Q_local]
	Layer3_Device5_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage4_Attention -> Layer3_Device5_Stage4_Accumulate
	Layer3_Device5_Stage3_Accumulate -> Layer3_Device5_Stage4_Accumulate
	Layer3_Device5_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage4_RecvKV -> Layer3_Device5_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage5_RecvKV -> Layer3_Device5_Stage5_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage5_Attention [label=Q_local]
	Layer3_Device5_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage5_Attention -> Layer3_Device5_Stage5_Accumulate
	Layer3_Device5_Stage4_Accumulate -> Layer3_Device5_Stage5_Accumulate
	Layer3_Device5_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage5_RecvKV -> Layer3_Device5_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage6_RecvKV -> Layer3_Device5_Stage6_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage6_Attention [label=Q_local]
	Layer3_Device5_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage6_Attention -> Layer3_Device5_Stage6_Accumulate
	Layer3_Device5_Stage5_Accumulate -> Layer3_Device5_Stage6_Accumulate
	Layer3_Device5_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage6_RecvKV -> Layer3_Device5_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage7_RecvKV -> Layer3_Device5_Stage7_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage7_Attention [label=Q_local]
	Layer3_Device5_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage7_Attention -> Layer3_Device5_Stage7_Accumulate
	Layer3_Device5_Stage6_Accumulate -> Layer3_Device5_Stage7_Accumulate
	Layer3_Device5_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage7_RecvKV -> Layer3_Device5_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage8_RecvKV -> Layer3_Device5_Stage8_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage8_Attention [label=Q_local]
	Layer3_Device5_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage8_Attention -> Layer3_Device5_Stage8_Accumulate
	Layer3_Device5_Stage7_Accumulate -> Layer3_Device5_Stage8_Accumulate
	Layer3_Device5_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage8_RecvKV -> Layer3_Device5_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage9_RecvKV -> Layer3_Device5_Stage9_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage9_Attention [label=Q_local]
	Layer3_Device5_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage9_Attention -> Layer3_Device5_Stage9_Accumulate
	Layer3_Device5_Stage8_Accumulate -> Layer3_Device5_Stage9_Accumulate
	Layer3_Device5_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage9_RecvKV -> Layer3_Device5_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage10_RecvKV -> Layer3_Device5_Stage10_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage10_Attention [label=Q_local]
	Layer3_Device5_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage10_Attention -> Layer3_Device5_Stage10_Accumulate
	Layer3_Device5_Stage9_Accumulate -> Layer3_Device5_Stage10_Accumulate
	Layer3_Device5_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage10_RecvKV -> Layer3_Device5_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage11_RecvKV -> Layer3_Device5_Stage11_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage11_Attention [label=Q_local]
	Layer3_Device5_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage11_Attention -> Layer3_Device5_Stage11_Accumulate
	Layer3_Device5_Stage10_Accumulate -> Layer3_Device5_Stage11_Accumulate
	Layer3_Device5_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage11_RecvKV -> Layer3_Device5_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage12_RecvKV -> Layer3_Device5_Stage12_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage12_Attention [label=Q_local]
	Layer3_Device5_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage12_Attention -> Layer3_Device5_Stage12_Accumulate
	Layer3_Device5_Stage11_Accumulate -> Layer3_Device5_Stage12_Accumulate
	Layer3_Device5_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage12_RecvKV -> Layer3_Device5_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage13_RecvKV -> Layer3_Device5_Stage13_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage13_Attention [label=Q_local]
	Layer3_Device5_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage13_Attention -> Layer3_Device5_Stage13_Accumulate
	Layer3_Device5_Stage12_Accumulate -> Layer3_Device5_Stage13_Accumulate
	Layer3_Device5_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage13_RecvKV -> Layer3_Device5_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage14_RecvKV -> Layer3_Device5_Stage14_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage14_Attention [label=Q_local]
	Layer3_Device5_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage14_Attention -> Layer3_Device5_Stage14_Accumulate
	Layer3_Device5_Stage13_Accumulate -> Layer3_Device5_Stage14_Accumulate
	Layer3_Device5_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device5_Stage14_RecvKV -> Layer3_Device5_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device5_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device5_Stage15_RecvKV -> Layer3_Device5_Stage15_Attention
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage15_Attention [label=Q_local]
	Layer3_Device5_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device5_Stage15_Attention -> Layer3_Device5_Stage15_Accumulate
	Layer3_Device5_Stage14_Accumulate -> Layer3_Device5_Stage15_Accumulate
	Layer3_Device5_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device5_Stage15_Accumulate -> Layer3_Device5_ConcatHeads
	Layer3_Device5_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device5_ConcatHeads -> Layer3_Device5_OutputProj
	Layer3_Device5_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device5_OutputProj -> Layer3_Device5_Residual1
	Layer3_Device5_Input -> Layer3_Device5_Residual1
	Layer3_Device5_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device5_Residual1 -> Layer3_Device5_LayerNorm2
	Layer3_Device5_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device5_LayerNorm2 -> Layer3_Device5_GateProj
	Layer3_Device5_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device5_LayerNorm2 -> Layer3_Device5_UpProj
	Layer3_Device5_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device5_GateProj -> Layer3_Device5_Activation
	Layer3_Device5_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device5_Activation -> Layer3_Device5_ElemMul
	Layer3_Device5_UpProj -> Layer3_Device5_ElemMul
	Layer3_Device5_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device5_ElemMul -> Layer3_Device5_DownProj
	Layer3_Device5_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device5_DownProj -> Layer3_Device5_Residual2
	Layer3_Device5_Residual1 -> Layer3_Device5_Residual2
	Layer3_Device5_Output [label="Layer 3 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device5_Residual2 -> Layer3_Device5_Output
	Layer3_Device6_Input [label="Layer 3 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device6_Output -> Layer3_Device6_Input
	Layer3_Device6_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device6_Input -> Layer3_Device6_LayerNorm1
	Layer3_Device6_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device6_LayerNorm1 -> Layer3_Device6_QKVProj
	Layer3_Device6_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device6_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage0_RecvKV -> Layer3_Device6_Stage0_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage0_Attention [label=Q_local]
	Layer3_Device6_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage0_Attention -> Layer3_Device6_Stage0_Accumulate
	Layer3_Device6_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage0_RecvKV -> Layer3_Device6_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage1_RecvKV -> Layer3_Device6_Stage1_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage1_Attention [label=Q_local]
	Layer3_Device6_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage1_Attention -> Layer3_Device6_Stage1_Accumulate
	Layer3_Device6_Stage0_Accumulate -> Layer3_Device6_Stage1_Accumulate
	Layer3_Device6_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage1_RecvKV -> Layer3_Device6_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage2_RecvKV -> Layer3_Device6_Stage2_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage2_Attention [label=Q_local]
	Layer3_Device6_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage2_Attention -> Layer3_Device6_Stage2_Accumulate
	Layer3_Device6_Stage1_Accumulate -> Layer3_Device6_Stage2_Accumulate
	Layer3_Device6_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage2_RecvKV -> Layer3_Device6_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage3_RecvKV -> Layer3_Device6_Stage3_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage3_Attention [label=Q_local]
	Layer3_Device6_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage3_Attention -> Layer3_Device6_Stage3_Accumulate
	Layer3_Device6_Stage2_Accumulate -> Layer3_Device6_Stage3_Accumulate
	Layer3_Device6_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage3_RecvKV -> Layer3_Device6_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage4_RecvKV -> Layer3_Device6_Stage4_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage4_Attention [label=Q_local]
	Layer3_Device6_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage4_Attention -> Layer3_Device6_Stage4_Accumulate
	Layer3_Device6_Stage3_Accumulate -> Layer3_Device6_Stage4_Accumulate
	Layer3_Device6_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage4_RecvKV -> Layer3_Device6_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage5_RecvKV -> Layer3_Device6_Stage5_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage5_Attention [label=Q_local]
	Layer3_Device6_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage5_Attention -> Layer3_Device6_Stage5_Accumulate
	Layer3_Device6_Stage4_Accumulate -> Layer3_Device6_Stage5_Accumulate
	Layer3_Device6_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage5_RecvKV -> Layer3_Device6_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage6_RecvKV -> Layer3_Device6_Stage6_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage6_Attention [label=Q_local]
	Layer3_Device6_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage6_Attention -> Layer3_Device6_Stage6_Accumulate
	Layer3_Device6_Stage5_Accumulate -> Layer3_Device6_Stage6_Accumulate
	Layer3_Device6_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage6_RecvKV -> Layer3_Device6_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage7_RecvKV -> Layer3_Device6_Stage7_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage7_Attention [label=Q_local]
	Layer3_Device6_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage7_Attention -> Layer3_Device6_Stage7_Accumulate
	Layer3_Device6_Stage6_Accumulate -> Layer3_Device6_Stage7_Accumulate
	Layer3_Device6_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage7_RecvKV -> Layer3_Device6_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage8_RecvKV -> Layer3_Device6_Stage8_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage8_Attention [label=Q_local]
	Layer3_Device6_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage8_Attention -> Layer3_Device6_Stage8_Accumulate
	Layer3_Device6_Stage7_Accumulate -> Layer3_Device6_Stage8_Accumulate
	Layer3_Device6_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage8_RecvKV -> Layer3_Device6_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage9_RecvKV -> Layer3_Device6_Stage9_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage9_Attention [label=Q_local]
	Layer3_Device6_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage9_Attention -> Layer3_Device6_Stage9_Accumulate
	Layer3_Device6_Stage8_Accumulate -> Layer3_Device6_Stage9_Accumulate
	Layer3_Device6_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage9_RecvKV -> Layer3_Device6_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage10_RecvKV -> Layer3_Device6_Stage10_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage10_Attention [label=Q_local]
	Layer3_Device6_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage10_Attention -> Layer3_Device6_Stage10_Accumulate
	Layer3_Device6_Stage9_Accumulate -> Layer3_Device6_Stage10_Accumulate
	Layer3_Device6_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage10_RecvKV -> Layer3_Device6_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage11_RecvKV -> Layer3_Device6_Stage11_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage11_Attention [label=Q_local]
	Layer3_Device6_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage11_Attention -> Layer3_Device6_Stage11_Accumulate
	Layer3_Device6_Stage10_Accumulate -> Layer3_Device6_Stage11_Accumulate
	Layer3_Device6_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage11_RecvKV -> Layer3_Device6_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage12_RecvKV -> Layer3_Device6_Stage12_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage12_Attention [label=Q_local]
	Layer3_Device6_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage12_Attention -> Layer3_Device6_Stage12_Accumulate
	Layer3_Device6_Stage11_Accumulate -> Layer3_Device6_Stage12_Accumulate
	Layer3_Device6_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage12_RecvKV -> Layer3_Device6_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage13_RecvKV -> Layer3_Device6_Stage13_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage13_Attention [label=Q_local]
	Layer3_Device6_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage13_Attention -> Layer3_Device6_Stage13_Accumulate
	Layer3_Device6_Stage12_Accumulate -> Layer3_Device6_Stage13_Accumulate
	Layer3_Device6_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage13_RecvKV -> Layer3_Device6_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage14_RecvKV -> Layer3_Device6_Stage14_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage14_Attention [label=Q_local]
	Layer3_Device6_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage14_Attention -> Layer3_Device6_Stage14_Accumulate
	Layer3_Device6_Stage13_Accumulate -> Layer3_Device6_Stage14_Accumulate
	Layer3_Device6_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device6_Stage14_RecvKV -> Layer3_Device6_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device6_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device6_Stage15_RecvKV -> Layer3_Device6_Stage15_Attention
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage15_Attention [label=Q_local]
	Layer3_Device6_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device6_Stage15_Attention -> Layer3_Device6_Stage15_Accumulate
	Layer3_Device6_Stage14_Accumulate -> Layer3_Device6_Stage15_Accumulate
	Layer3_Device6_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device6_Stage15_Accumulate -> Layer3_Device6_ConcatHeads
	Layer3_Device6_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device6_ConcatHeads -> Layer3_Device6_OutputProj
	Layer3_Device6_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device6_OutputProj -> Layer3_Device6_Residual1
	Layer3_Device6_Input -> Layer3_Device6_Residual1
	Layer3_Device6_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device6_Residual1 -> Layer3_Device6_LayerNorm2
	Layer3_Device6_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device6_LayerNorm2 -> Layer3_Device6_GateProj
	Layer3_Device6_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device6_LayerNorm2 -> Layer3_Device6_UpProj
	Layer3_Device6_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device6_GateProj -> Layer3_Device6_Activation
	Layer3_Device6_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device6_Activation -> Layer3_Device6_ElemMul
	Layer3_Device6_UpProj -> Layer3_Device6_ElemMul
	Layer3_Device6_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device6_ElemMul -> Layer3_Device6_DownProj
	Layer3_Device6_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device6_DownProj -> Layer3_Device6_Residual2
	Layer3_Device6_Residual1 -> Layer3_Device6_Residual2
	Layer3_Device6_Output [label="Layer 3 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device6_Residual2 -> Layer3_Device6_Output
	Layer3_Device7_Input [label="Layer 3 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device7_Output -> Layer3_Device7_Input
	Layer3_Device7_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device7_Input -> Layer3_Device7_LayerNorm1
	Layer3_Device7_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device7_LayerNorm1 -> Layer3_Device7_QKVProj
	Layer3_Device7_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device7_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage0_RecvKV -> Layer3_Device7_Stage0_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage0_Attention [label=Q_local]
	Layer3_Device7_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage0_Attention -> Layer3_Device7_Stage0_Accumulate
	Layer3_Device7_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage0_RecvKV -> Layer3_Device7_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage1_RecvKV -> Layer3_Device7_Stage1_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage1_Attention [label=Q_local]
	Layer3_Device7_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage1_Attention -> Layer3_Device7_Stage1_Accumulate
	Layer3_Device7_Stage0_Accumulate -> Layer3_Device7_Stage1_Accumulate
	Layer3_Device7_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage1_RecvKV -> Layer3_Device7_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage2_RecvKV -> Layer3_Device7_Stage2_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage2_Attention [label=Q_local]
	Layer3_Device7_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage2_Attention -> Layer3_Device7_Stage2_Accumulate
	Layer3_Device7_Stage1_Accumulate -> Layer3_Device7_Stage2_Accumulate
	Layer3_Device7_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage2_RecvKV -> Layer3_Device7_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage3_RecvKV -> Layer3_Device7_Stage3_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage3_Attention [label=Q_local]
	Layer3_Device7_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage3_Attention -> Layer3_Device7_Stage3_Accumulate
	Layer3_Device7_Stage2_Accumulate -> Layer3_Device7_Stage3_Accumulate
	Layer3_Device7_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage3_RecvKV -> Layer3_Device7_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage4_RecvKV -> Layer3_Device7_Stage4_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage4_Attention [label=Q_local]
	Layer3_Device7_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage4_Attention -> Layer3_Device7_Stage4_Accumulate
	Layer3_Device7_Stage3_Accumulate -> Layer3_Device7_Stage4_Accumulate
	Layer3_Device7_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage4_RecvKV -> Layer3_Device7_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage5_RecvKV -> Layer3_Device7_Stage5_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage5_Attention [label=Q_local]
	Layer3_Device7_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage5_Attention -> Layer3_Device7_Stage5_Accumulate
	Layer3_Device7_Stage4_Accumulate -> Layer3_Device7_Stage5_Accumulate
	Layer3_Device7_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage5_RecvKV -> Layer3_Device7_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage6_RecvKV -> Layer3_Device7_Stage6_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage6_Attention [label=Q_local]
	Layer3_Device7_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage6_Attention -> Layer3_Device7_Stage6_Accumulate
	Layer3_Device7_Stage5_Accumulate -> Layer3_Device7_Stage6_Accumulate
	Layer3_Device7_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage6_RecvKV -> Layer3_Device7_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage7_RecvKV -> Layer3_Device7_Stage7_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage7_Attention [label=Q_local]
	Layer3_Device7_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage7_Attention -> Layer3_Device7_Stage7_Accumulate
	Layer3_Device7_Stage6_Accumulate -> Layer3_Device7_Stage7_Accumulate
	Layer3_Device7_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage7_RecvKV -> Layer3_Device7_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage8_RecvKV -> Layer3_Device7_Stage8_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage8_Attention [label=Q_local]
	Layer3_Device7_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage8_Attention -> Layer3_Device7_Stage8_Accumulate
	Layer3_Device7_Stage7_Accumulate -> Layer3_Device7_Stage8_Accumulate
	Layer3_Device7_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage8_RecvKV -> Layer3_Device7_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage9_RecvKV -> Layer3_Device7_Stage9_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage9_Attention [label=Q_local]
	Layer3_Device7_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage9_Attention -> Layer3_Device7_Stage9_Accumulate
	Layer3_Device7_Stage8_Accumulate -> Layer3_Device7_Stage9_Accumulate
	Layer3_Device7_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage9_RecvKV -> Layer3_Device7_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage10_RecvKV -> Layer3_Device7_Stage10_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage10_Attention [label=Q_local]
	Layer3_Device7_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage10_Attention -> Layer3_Device7_Stage10_Accumulate
	Layer3_Device7_Stage9_Accumulate -> Layer3_Device7_Stage10_Accumulate
	Layer3_Device7_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage10_RecvKV -> Layer3_Device7_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage11_RecvKV -> Layer3_Device7_Stage11_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage11_Attention [label=Q_local]
	Layer3_Device7_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage11_Attention -> Layer3_Device7_Stage11_Accumulate
	Layer3_Device7_Stage10_Accumulate -> Layer3_Device7_Stage11_Accumulate
	Layer3_Device7_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage11_RecvKV -> Layer3_Device7_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage12_RecvKV -> Layer3_Device7_Stage12_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage12_Attention [label=Q_local]
	Layer3_Device7_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage12_Attention -> Layer3_Device7_Stage12_Accumulate
	Layer3_Device7_Stage11_Accumulate -> Layer3_Device7_Stage12_Accumulate
	Layer3_Device7_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage12_RecvKV -> Layer3_Device7_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage13_RecvKV -> Layer3_Device7_Stage13_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage13_Attention [label=Q_local]
	Layer3_Device7_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage13_Attention -> Layer3_Device7_Stage13_Accumulate
	Layer3_Device7_Stage12_Accumulate -> Layer3_Device7_Stage13_Accumulate
	Layer3_Device7_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage13_RecvKV -> Layer3_Device7_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage14_RecvKV -> Layer3_Device7_Stage14_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage14_Attention [label=Q_local]
	Layer3_Device7_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage14_Attention -> Layer3_Device7_Stage14_Accumulate
	Layer3_Device7_Stage13_Accumulate -> Layer3_Device7_Stage14_Accumulate
	Layer3_Device7_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device7_Stage14_RecvKV -> Layer3_Device7_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device7_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device7_Stage15_RecvKV -> Layer3_Device7_Stage15_Attention
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage15_Attention [label=Q_local]
	Layer3_Device7_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device7_Stage15_Attention -> Layer3_Device7_Stage15_Accumulate
	Layer3_Device7_Stage14_Accumulate -> Layer3_Device7_Stage15_Accumulate
	Layer3_Device7_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device7_Stage15_Accumulate -> Layer3_Device7_ConcatHeads
	Layer3_Device7_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device7_ConcatHeads -> Layer3_Device7_OutputProj
	Layer3_Device7_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device7_OutputProj -> Layer3_Device7_Residual1
	Layer3_Device7_Input -> Layer3_Device7_Residual1
	Layer3_Device7_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device7_Residual1 -> Layer3_Device7_LayerNorm2
	Layer3_Device7_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device7_LayerNorm2 -> Layer3_Device7_GateProj
	Layer3_Device7_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device7_LayerNorm2 -> Layer3_Device7_UpProj
	Layer3_Device7_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device7_GateProj -> Layer3_Device7_Activation
	Layer3_Device7_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device7_Activation -> Layer3_Device7_ElemMul
	Layer3_Device7_UpProj -> Layer3_Device7_ElemMul
	Layer3_Device7_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device7_ElemMul -> Layer3_Device7_DownProj
	Layer3_Device7_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device7_DownProj -> Layer3_Device7_Residual2
	Layer3_Device7_Residual1 -> Layer3_Device7_Residual2
	Layer3_Device7_Output [label="Layer 3 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device7_Residual2 -> Layer3_Device7_Output
	Layer3_Device8_Input [label="Layer 3 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device8_Output -> Layer3_Device8_Input
	Layer3_Device8_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device8_Input -> Layer3_Device8_LayerNorm1
	Layer3_Device8_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device8_LayerNorm1 -> Layer3_Device8_QKVProj
	Layer3_Device8_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device8_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage0_RecvKV -> Layer3_Device8_Stage0_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage0_Attention [label=Q_local]
	Layer3_Device8_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage0_Attention -> Layer3_Device8_Stage0_Accumulate
	Layer3_Device8_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage0_RecvKV -> Layer3_Device8_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage1_RecvKV -> Layer3_Device8_Stage1_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage1_Attention [label=Q_local]
	Layer3_Device8_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage1_Attention -> Layer3_Device8_Stage1_Accumulate
	Layer3_Device8_Stage0_Accumulate -> Layer3_Device8_Stage1_Accumulate
	Layer3_Device8_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage1_RecvKV -> Layer3_Device8_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage2_RecvKV -> Layer3_Device8_Stage2_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage2_Attention [label=Q_local]
	Layer3_Device8_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage2_Attention -> Layer3_Device8_Stage2_Accumulate
	Layer3_Device8_Stage1_Accumulate -> Layer3_Device8_Stage2_Accumulate
	Layer3_Device8_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage2_RecvKV -> Layer3_Device8_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage3_RecvKV -> Layer3_Device8_Stage3_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage3_Attention [label=Q_local]
	Layer3_Device8_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage3_Attention -> Layer3_Device8_Stage3_Accumulate
	Layer3_Device8_Stage2_Accumulate -> Layer3_Device8_Stage3_Accumulate
	Layer3_Device8_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage3_RecvKV -> Layer3_Device8_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage4_RecvKV -> Layer3_Device8_Stage4_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage4_Attention [label=Q_local]
	Layer3_Device8_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage4_Attention -> Layer3_Device8_Stage4_Accumulate
	Layer3_Device8_Stage3_Accumulate -> Layer3_Device8_Stage4_Accumulate
	Layer3_Device8_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage4_RecvKV -> Layer3_Device8_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage5_RecvKV -> Layer3_Device8_Stage5_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage5_Attention [label=Q_local]
	Layer3_Device8_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage5_Attention -> Layer3_Device8_Stage5_Accumulate
	Layer3_Device8_Stage4_Accumulate -> Layer3_Device8_Stage5_Accumulate
	Layer3_Device8_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage5_RecvKV -> Layer3_Device8_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage6_RecvKV -> Layer3_Device8_Stage6_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage6_Attention [label=Q_local]
	Layer3_Device8_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage6_Attention -> Layer3_Device8_Stage6_Accumulate
	Layer3_Device8_Stage5_Accumulate -> Layer3_Device8_Stage6_Accumulate
	Layer3_Device8_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage6_RecvKV -> Layer3_Device8_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage7_RecvKV -> Layer3_Device8_Stage7_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage7_Attention [label=Q_local]
	Layer3_Device8_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage7_Attention -> Layer3_Device8_Stage7_Accumulate
	Layer3_Device8_Stage6_Accumulate -> Layer3_Device8_Stage7_Accumulate
	Layer3_Device8_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage7_RecvKV -> Layer3_Device8_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage8_RecvKV -> Layer3_Device8_Stage8_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage8_Attention [label=Q_local]
	Layer3_Device8_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage8_Attention -> Layer3_Device8_Stage8_Accumulate
	Layer3_Device8_Stage7_Accumulate -> Layer3_Device8_Stage8_Accumulate
	Layer3_Device8_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage8_RecvKV -> Layer3_Device8_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage9_RecvKV -> Layer3_Device8_Stage9_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage9_Attention [label=Q_local]
	Layer3_Device8_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage9_Attention -> Layer3_Device8_Stage9_Accumulate
	Layer3_Device8_Stage8_Accumulate -> Layer3_Device8_Stage9_Accumulate
	Layer3_Device8_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage9_RecvKV -> Layer3_Device8_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage10_RecvKV -> Layer3_Device8_Stage10_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage10_Attention [label=Q_local]
	Layer3_Device8_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage10_Attention -> Layer3_Device8_Stage10_Accumulate
	Layer3_Device8_Stage9_Accumulate -> Layer3_Device8_Stage10_Accumulate
	Layer3_Device8_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage10_RecvKV -> Layer3_Device8_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage11_RecvKV -> Layer3_Device8_Stage11_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage11_Attention [label=Q_local]
	Layer3_Device8_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage11_Attention -> Layer3_Device8_Stage11_Accumulate
	Layer3_Device8_Stage10_Accumulate -> Layer3_Device8_Stage11_Accumulate
	Layer3_Device8_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage11_RecvKV -> Layer3_Device8_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage12_RecvKV -> Layer3_Device8_Stage12_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage12_Attention [label=Q_local]
	Layer3_Device8_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage12_Attention -> Layer3_Device8_Stage12_Accumulate
	Layer3_Device8_Stage11_Accumulate -> Layer3_Device8_Stage12_Accumulate
	Layer3_Device8_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage12_RecvKV -> Layer3_Device8_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage13_RecvKV -> Layer3_Device8_Stage13_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage13_Attention [label=Q_local]
	Layer3_Device8_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage13_Attention -> Layer3_Device8_Stage13_Accumulate
	Layer3_Device8_Stage12_Accumulate -> Layer3_Device8_Stage13_Accumulate
	Layer3_Device8_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage13_RecvKV -> Layer3_Device8_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage14_RecvKV -> Layer3_Device8_Stage14_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage14_Attention [label=Q_local]
	Layer3_Device8_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage14_Attention -> Layer3_Device8_Stage14_Accumulate
	Layer3_Device8_Stage13_Accumulate -> Layer3_Device8_Stage14_Accumulate
	Layer3_Device8_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device8_Stage14_RecvKV -> Layer3_Device8_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device8_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device8_Stage15_RecvKV -> Layer3_Device8_Stage15_Attention
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage15_Attention [label=Q_local]
	Layer3_Device8_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device8_Stage15_Attention -> Layer3_Device8_Stage15_Accumulate
	Layer3_Device8_Stage14_Accumulate -> Layer3_Device8_Stage15_Accumulate
	Layer3_Device8_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device8_Stage15_Accumulate -> Layer3_Device8_ConcatHeads
	Layer3_Device8_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device8_ConcatHeads -> Layer3_Device8_OutputProj
	Layer3_Device8_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device8_OutputProj -> Layer3_Device8_Residual1
	Layer3_Device8_Input -> Layer3_Device8_Residual1
	Layer3_Device8_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device8_Residual1 -> Layer3_Device8_LayerNorm2
	Layer3_Device8_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device8_LayerNorm2 -> Layer3_Device8_GateProj
	Layer3_Device8_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device8_LayerNorm2 -> Layer3_Device8_UpProj
	Layer3_Device8_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device8_GateProj -> Layer3_Device8_Activation
	Layer3_Device8_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device8_Activation -> Layer3_Device8_ElemMul
	Layer3_Device8_UpProj -> Layer3_Device8_ElemMul
	Layer3_Device8_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device8_ElemMul -> Layer3_Device8_DownProj
	Layer3_Device8_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device8_DownProj -> Layer3_Device8_Residual2
	Layer3_Device8_Residual1 -> Layer3_Device8_Residual2
	Layer3_Device8_Output [label="Layer 3 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device8_Residual2 -> Layer3_Device8_Output
	Layer3_Device9_Input [label="Layer 3 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device9_Output -> Layer3_Device9_Input
	Layer3_Device9_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device9_Input -> Layer3_Device9_LayerNorm1
	Layer3_Device9_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device9_LayerNorm1 -> Layer3_Device9_QKVProj
	Layer3_Device9_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device9_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage0_RecvKV -> Layer3_Device9_Stage0_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage0_Attention [label=Q_local]
	Layer3_Device9_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage0_Attention -> Layer3_Device9_Stage0_Accumulate
	Layer3_Device9_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage0_RecvKV -> Layer3_Device9_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage1_RecvKV -> Layer3_Device9_Stage1_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage1_Attention [label=Q_local]
	Layer3_Device9_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage1_Attention -> Layer3_Device9_Stage1_Accumulate
	Layer3_Device9_Stage0_Accumulate -> Layer3_Device9_Stage1_Accumulate
	Layer3_Device9_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage1_RecvKV -> Layer3_Device9_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage2_RecvKV -> Layer3_Device9_Stage2_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage2_Attention [label=Q_local]
	Layer3_Device9_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage2_Attention -> Layer3_Device9_Stage2_Accumulate
	Layer3_Device9_Stage1_Accumulate -> Layer3_Device9_Stage2_Accumulate
	Layer3_Device9_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage2_RecvKV -> Layer3_Device9_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage3_RecvKV -> Layer3_Device9_Stage3_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage3_Attention [label=Q_local]
	Layer3_Device9_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage3_Attention -> Layer3_Device9_Stage3_Accumulate
	Layer3_Device9_Stage2_Accumulate -> Layer3_Device9_Stage3_Accumulate
	Layer3_Device9_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage3_RecvKV -> Layer3_Device9_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage4_RecvKV -> Layer3_Device9_Stage4_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage4_Attention [label=Q_local]
	Layer3_Device9_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage4_Attention -> Layer3_Device9_Stage4_Accumulate
	Layer3_Device9_Stage3_Accumulate -> Layer3_Device9_Stage4_Accumulate
	Layer3_Device9_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage4_RecvKV -> Layer3_Device9_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage5_RecvKV -> Layer3_Device9_Stage5_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage5_Attention [label=Q_local]
	Layer3_Device9_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage5_Attention -> Layer3_Device9_Stage5_Accumulate
	Layer3_Device9_Stage4_Accumulate -> Layer3_Device9_Stage5_Accumulate
	Layer3_Device9_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage5_RecvKV -> Layer3_Device9_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage6_RecvKV -> Layer3_Device9_Stage6_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage6_Attention [label=Q_local]
	Layer3_Device9_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage6_Attention -> Layer3_Device9_Stage6_Accumulate
	Layer3_Device9_Stage5_Accumulate -> Layer3_Device9_Stage6_Accumulate
	Layer3_Device9_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage6_RecvKV -> Layer3_Device9_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage7_RecvKV -> Layer3_Device9_Stage7_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage7_Attention [label=Q_local]
	Layer3_Device9_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage7_Attention -> Layer3_Device9_Stage7_Accumulate
	Layer3_Device9_Stage6_Accumulate -> Layer3_Device9_Stage7_Accumulate
	Layer3_Device9_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage7_RecvKV -> Layer3_Device9_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage8_RecvKV -> Layer3_Device9_Stage8_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage8_Attention [label=Q_local]
	Layer3_Device9_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage8_Attention -> Layer3_Device9_Stage8_Accumulate
	Layer3_Device9_Stage7_Accumulate -> Layer3_Device9_Stage8_Accumulate
	Layer3_Device9_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage8_RecvKV -> Layer3_Device9_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage9_RecvKV -> Layer3_Device9_Stage9_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage9_Attention [label=Q_local]
	Layer3_Device9_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage9_Attention -> Layer3_Device9_Stage9_Accumulate
	Layer3_Device9_Stage8_Accumulate -> Layer3_Device9_Stage9_Accumulate
	Layer3_Device9_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage9_RecvKV -> Layer3_Device9_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage10_RecvKV -> Layer3_Device9_Stage10_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage10_Attention [label=Q_local]
	Layer3_Device9_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage10_Attention -> Layer3_Device9_Stage10_Accumulate
	Layer3_Device9_Stage9_Accumulate -> Layer3_Device9_Stage10_Accumulate
	Layer3_Device9_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage10_RecvKV -> Layer3_Device9_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage11_RecvKV -> Layer3_Device9_Stage11_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage11_Attention [label=Q_local]
	Layer3_Device9_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage11_Attention -> Layer3_Device9_Stage11_Accumulate
	Layer3_Device9_Stage10_Accumulate -> Layer3_Device9_Stage11_Accumulate
	Layer3_Device9_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage11_RecvKV -> Layer3_Device9_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage12_RecvKV -> Layer3_Device9_Stage12_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage12_Attention [label=Q_local]
	Layer3_Device9_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage12_Attention -> Layer3_Device9_Stage12_Accumulate
	Layer3_Device9_Stage11_Accumulate -> Layer3_Device9_Stage12_Accumulate
	Layer3_Device9_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage12_RecvKV -> Layer3_Device9_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage13_RecvKV -> Layer3_Device9_Stage13_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage13_Attention [label=Q_local]
	Layer3_Device9_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage13_Attention -> Layer3_Device9_Stage13_Accumulate
	Layer3_Device9_Stage12_Accumulate -> Layer3_Device9_Stage13_Accumulate
	Layer3_Device9_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage13_RecvKV -> Layer3_Device9_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage14_RecvKV -> Layer3_Device9_Stage14_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage14_Attention [label=Q_local]
	Layer3_Device9_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage14_Attention -> Layer3_Device9_Stage14_Accumulate
	Layer3_Device9_Stage13_Accumulate -> Layer3_Device9_Stage14_Accumulate
	Layer3_Device9_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device9_Stage14_RecvKV -> Layer3_Device9_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device9_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device9_Stage15_RecvKV -> Layer3_Device9_Stage15_Attention
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage15_Attention [label=Q_local]
	Layer3_Device9_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device9_Stage15_Attention -> Layer3_Device9_Stage15_Accumulate
	Layer3_Device9_Stage14_Accumulate -> Layer3_Device9_Stage15_Accumulate
	Layer3_Device9_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device9_Stage15_Accumulate -> Layer3_Device9_ConcatHeads
	Layer3_Device9_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device9_ConcatHeads -> Layer3_Device9_OutputProj
	Layer3_Device9_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device9_OutputProj -> Layer3_Device9_Residual1
	Layer3_Device9_Input -> Layer3_Device9_Residual1
	Layer3_Device9_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device9_Residual1 -> Layer3_Device9_LayerNorm2
	Layer3_Device9_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device9_LayerNorm2 -> Layer3_Device9_GateProj
	Layer3_Device9_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device9_LayerNorm2 -> Layer3_Device9_UpProj
	Layer3_Device9_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device9_GateProj -> Layer3_Device9_Activation
	Layer3_Device9_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device9_Activation -> Layer3_Device9_ElemMul
	Layer3_Device9_UpProj -> Layer3_Device9_ElemMul
	Layer3_Device9_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device9_ElemMul -> Layer3_Device9_DownProj
	Layer3_Device9_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device9_DownProj -> Layer3_Device9_Residual2
	Layer3_Device9_Residual1 -> Layer3_Device9_Residual2
	Layer3_Device9_Output [label="Layer 3 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device9_Residual2 -> Layer3_Device9_Output
	Layer3_Device10_Input [label="Layer 3 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device10_Output -> Layer3_Device10_Input
	Layer3_Device10_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device10_Input -> Layer3_Device10_LayerNorm1
	Layer3_Device10_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device10_LayerNorm1 -> Layer3_Device10_QKVProj
	Layer3_Device10_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device10_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage0_RecvKV -> Layer3_Device10_Stage0_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage0_Attention [label=Q_local]
	Layer3_Device10_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage0_Attention -> Layer3_Device10_Stage0_Accumulate
	Layer3_Device10_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage0_RecvKV -> Layer3_Device10_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage1_RecvKV -> Layer3_Device10_Stage1_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage1_Attention [label=Q_local]
	Layer3_Device10_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage1_Attention -> Layer3_Device10_Stage1_Accumulate
	Layer3_Device10_Stage0_Accumulate -> Layer3_Device10_Stage1_Accumulate
	Layer3_Device10_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage1_RecvKV -> Layer3_Device10_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage2_RecvKV -> Layer3_Device10_Stage2_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage2_Attention [label=Q_local]
	Layer3_Device10_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage2_Attention -> Layer3_Device10_Stage2_Accumulate
	Layer3_Device10_Stage1_Accumulate -> Layer3_Device10_Stage2_Accumulate
	Layer3_Device10_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage2_RecvKV -> Layer3_Device10_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage3_RecvKV -> Layer3_Device10_Stage3_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage3_Attention [label=Q_local]
	Layer3_Device10_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage3_Attention -> Layer3_Device10_Stage3_Accumulate
	Layer3_Device10_Stage2_Accumulate -> Layer3_Device10_Stage3_Accumulate
	Layer3_Device10_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage3_RecvKV -> Layer3_Device10_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage4_RecvKV -> Layer3_Device10_Stage4_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage4_Attention [label=Q_local]
	Layer3_Device10_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage4_Attention -> Layer3_Device10_Stage4_Accumulate
	Layer3_Device10_Stage3_Accumulate -> Layer3_Device10_Stage4_Accumulate
	Layer3_Device10_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage4_RecvKV -> Layer3_Device10_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage5_RecvKV -> Layer3_Device10_Stage5_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage5_Attention [label=Q_local]
	Layer3_Device10_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage5_Attention -> Layer3_Device10_Stage5_Accumulate
	Layer3_Device10_Stage4_Accumulate -> Layer3_Device10_Stage5_Accumulate
	Layer3_Device10_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage5_RecvKV -> Layer3_Device10_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage6_RecvKV -> Layer3_Device10_Stage6_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage6_Attention [label=Q_local]
	Layer3_Device10_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage6_Attention -> Layer3_Device10_Stage6_Accumulate
	Layer3_Device10_Stage5_Accumulate -> Layer3_Device10_Stage6_Accumulate
	Layer3_Device10_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage6_RecvKV -> Layer3_Device10_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage7_RecvKV -> Layer3_Device10_Stage7_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage7_Attention [label=Q_local]
	Layer3_Device10_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage7_Attention -> Layer3_Device10_Stage7_Accumulate
	Layer3_Device10_Stage6_Accumulate -> Layer3_Device10_Stage7_Accumulate
	Layer3_Device10_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage7_RecvKV -> Layer3_Device10_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage8_RecvKV -> Layer3_Device10_Stage8_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage8_Attention [label=Q_local]
	Layer3_Device10_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage8_Attention -> Layer3_Device10_Stage8_Accumulate
	Layer3_Device10_Stage7_Accumulate -> Layer3_Device10_Stage8_Accumulate
	Layer3_Device10_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage8_RecvKV -> Layer3_Device10_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage9_RecvKV -> Layer3_Device10_Stage9_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage9_Attention [label=Q_local]
	Layer3_Device10_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage9_Attention -> Layer3_Device10_Stage9_Accumulate
	Layer3_Device10_Stage8_Accumulate -> Layer3_Device10_Stage9_Accumulate
	Layer3_Device10_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage9_RecvKV -> Layer3_Device10_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage10_RecvKV -> Layer3_Device10_Stage10_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage10_Attention [label=Q_local]
	Layer3_Device10_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage10_Attention -> Layer3_Device10_Stage10_Accumulate
	Layer3_Device10_Stage9_Accumulate -> Layer3_Device10_Stage10_Accumulate
	Layer3_Device10_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage10_RecvKV -> Layer3_Device10_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage11_RecvKV -> Layer3_Device10_Stage11_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage11_Attention [label=Q_local]
	Layer3_Device10_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage11_Attention -> Layer3_Device10_Stage11_Accumulate
	Layer3_Device10_Stage10_Accumulate -> Layer3_Device10_Stage11_Accumulate
	Layer3_Device10_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage11_RecvKV -> Layer3_Device10_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage12_RecvKV -> Layer3_Device10_Stage12_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage12_Attention [label=Q_local]
	Layer3_Device10_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage12_Attention -> Layer3_Device10_Stage12_Accumulate
	Layer3_Device10_Stage11_Accumulate -> Layer3_Device10_Stage12_Accumulate
	Layer3_Device10_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage12_RecvKV -> Layer3_Device10_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage13_RecvKV -> Layer3_Device10_Stage13_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage13_Attention [label=Q_local]
	Layer3_Device10_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage13_Attention -> Layer3_Device10_Stage13_Accumulate
	Layer3_Device10_Stage12_Accumulate -> Layer3_Device10_Stage13_Accumulate
	Layer3_Device10_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage13_RecvKV -> Layer3_Device10_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage14_RecvKV -> Layer3_Device10_Stage14_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage14_Attention [label=Q_local]
	Layer3_Device10_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage14_Attention -> Layer3_Device10_Stage14_Accumulate
	Layer3_Device10_Stage13_Accumulate -> Layer3_Device10_Stage14_Accumulate
	Layer3_Device10_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device10_Stage14_RecvKV -> Layer3_Device10_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device10_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device10_Stage15_RecvKV -> Layer3_Device10_Stage15_Attention
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage15_Attention [label=Q_local]
	Layer3_Device10_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device10_Stage15_Attention -> Layer3_Device10_Stage15_Accumulate
	Layer3_Device10_Stage14_Accumulate -> Layer3_Device10_Stage15_Accumulate
	Layer3_Device10_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device10_Stage15_Accumulate -> Layer3_Device10_ConcatHeads
	Layer3_Device10_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device10_ConcatHeads -> Layer3_Device10_OutputProj
	Layer3_Device10_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device10_OutputProj -> Layer3_Device10_Residual1
	Layer3_Device10_Input -> Layer3_Device10_Residual1
	Layer3_Device10_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device10_Residual1 -> Layer3_Device10_LayerNorm2
	Layer3_Device10_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device10_LayerNorm2 -> Layer3_Device10_GateProj
	Layer3_Device10_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device10_LayerNorm2 -> Layer3_Device10_UpProj
	Layer3_Device10_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device10_GateProj -> Layer3_Device10_Activation
	Layer3_Device10_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device10_Activation -> Layer3_Device10_ElemMul
	Layer3_Device10_UpProj -> Layer3_Device10_ElemMul
	Layer3_Device10_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device10_ElemMul -> Layer3_Device10_DownProj
	Layer3_Device10_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device10_DownProj -> Layer3_Device10_Residual2
	Layer3_Device10_Residual1 -> Layer3_Device10_Residual2
	Layer3_Device10_Output [label="Layer 3 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device10_Residual2 -> Layer3_Device10_Output
	Layer3_Device11_Input [label="Layer 3 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device11_Output -> Layer3_Device11_Input
	Layer3_Device11_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device11_Input -> Layer3_Device11_LayerNorm1
	Layer3_Device11_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device11_LayerNorm1 -> Layer3_Device11_QKVProj
	Layer3_Device11_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device11_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage0_RecvKV -> Layer3_Device11_Stage0_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage0_Attention [label=Q_local]
	Layer3_Device11_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage0_Attention -> Layer3_Device11_Stage0_Accumulate
	Layer3_Device11_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage0_RecvKV -> Layer3_Device11_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage1_RecvKV -> Layer3_Device11_Stage1_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage1_Attention [label=Q_local]
	Layer3_Device11_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage1_Attention -> Layer3_Device11_Stage1_Accumulate
	Layer3_Device11_Stage0_Accumulate -> Layer3_Device11_Stage1_Accumulate
	Layer3_Device11_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage1_RecvKV -> Layer3_Device11_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage2_RecvKV -> Layer3_Device11_Stage2_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage2_Attention [label=Q_local]
	Layer3_Device11_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage2_Attention -> Layer3_Device11_Stage2_Accumulate
	Layer3_Device11_Stage1_Accumulate -> Layer3_Device11_Stage2_Accumulate
	Layer3_Device11_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage2_RecvKV -> Layer3_Device11_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage3_RecvKV -> Layer3_Device11_Stage3_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage3_Attention [label=Q_local]
	Layer3_Device11_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage3_Attention -> Layer3_Device11_Stage3_Accumulate
	Layer3_Device11_Stage2_Accumulate -> Layer3_Device11_Stage3_Accumulate
	Layer3_Device11_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage3_RecvKV -> Layer3_Device11_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage4_RecvKV -> Layer3_Device11_Stage4_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage4_Attention [label=Q_local]
	Layer3_Device11_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage4_Attention -> Layer3_Device11_Stage4_Accumulate
	Layer3_Device11_Stage3_Accumulate -> Layer3_Device11_Stage4_Accumulate
	Layer3_Device11_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage4_RecvKV -> Layer3_Device11_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage5_RecvKV -> Layer3_Device11_Stage5_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage5_Attention [label=Q_local]
	Layer3_Device11_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage5_Attention -> Layer3_Device11_Stage5_Accumulate
	Layer3_Device11_Stage4_Accumulate -> Layer3_Device11_Stage5_Accumulate
	Layer3_Device11_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage5_RecvKV -> Layer3_Device11_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage6_RecvKV -> Layer3_Device11_Stage6_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage6_Attention [label=Q_local]
	Layer3_Device11_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage6_Attention -> Layer3_Device11_Stage6_Accumulate
	Layer3_Device11_Stage5_Accumulate -> Layer3_Device11_Stage6_Accumulate
	Layer3_Device11_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage6_RecvKV -> Layer3_Device11_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage7_RecvKV -> Layer3_Device11_Stage7_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage7_Attention [label=Q_local]
	Layer3_Device11_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage7_Attention -> Layer3_Device11_Stage7_Accumulate
	Layer3_Device11_Stage6_Accumulate -> Layer3_Device11_Stage7_Accumulate
	Layer3_Device11_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage7_RecvKV -> Layer3_Device11_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage8_RecvKV -> Layer3_Device11_Stage8_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage8_Attention [label=Q_local]
	Layer3_Device11_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage8_Attention -> Layer3_Device11_Stage8_Accumulate
	Layer3_Device11_Stage7_Accumulate -> Layer3_Device11_Stage8_Accumulate
	Layer3_Device11_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage8_RecvKV -> Layer3_Device11_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage9_RecvKV -> Layer3_Device11_Stage9_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage9_Attention [label=Q_local]
	Layer3_Device11_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage9_Attention -> Layer3_Device11_Stage9_Accumulate
	Layer3_Device11_Stage8_Accumulate -> Layer3_Device11_Stage9_Accumulate
	Layer3_Device11_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage9_RecvKV -> Layer3_Device11_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage10_RecvKV -> Layer3_Device11_Stage10_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage10_Attention [label=Q_local]
	Layer3_Device11_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage10_Attention -> Layer3_Device11_Stage10_Accumulate
	Layer3_Device11_Stage9_Accumulate -> Layer3_Device11_Stage10_Accumulate
	Layer3_Device11_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage10_RecvKV -> Layer3_Device11_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage11_RecvKV -> Layer3_Device11_Stage11_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage11_Attention [label=Q_local]
	Layer3_Device11_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage11_Attention -> Layer3_Device11_Stage11_Accumulate
	Layer3_Device11_Stage10_Accumulate -> Layer3_Device11_Stage11_Accumulate
	Layer3_Device11_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage11_RecvKV -> Layer3_Device11_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage12_RecvKV -> Layer3_Device11_Stage12_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage12_Attention [label=Q_local]
	Layer3_Device11_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage12_Attention -> Layer3_Device11_Stage12_Accumulate
	Layer3_Device11_Stage11_Accumulate -> Layer3_Device11_Stage12_Accumulate
	Layer3_Device11_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage12_RecvKV -> Layer3_Device11_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage13_RecvKV -> Layer3_Device11_Stage13_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage13_Attention [label=Q_local]
	Layer3_Device11_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage13_Attention -> Layer3_Device11_Stage13_Accumulate
	Layer3_Device11_Stage12_Accumulate -> Layer3_Device11_Stage13_Accumulate
	Layer3_Device11_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage13_RecvKV -> Layer3_Device11_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage14_RecvKV -> Layer3_Device11_Stage14_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage14_Attention [label=Q_local]
	Layer3_Device11_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage14_Attention -> Layer3_Device11_Stage14_Accumulate
	Layer3_Device11_Stage13_Accumulate -> Layer3_Device11_Stage14_Accumulate
	Layer3_Device11_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device11_Stage14_RecvKV -> Layer3_Device11_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device11_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device11_Stage15_RecvKV -> Layer3_Device11_Stage15_Attention
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage15_Attention [label=Q_local]
	Layer3_Device11_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device11_Stage15_Attention -> Layer3_Device11_Stage15_Accumulate
	Layer3_Device11_Stage14_Accumulate -> Layer3_Device11_Stage15_Accumulate
	Layer3_Device11_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device11_Stage15_Accumulate -> Layer3_Device11_ConcatHeads
	Layer3_Device11_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device11_ConcatHeads -> Layer3_Device11_OutputProj
	Layer3_Device11_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device11_OutputProj -> Layer3_Device11_Residual1
	Layer3_Device11_Input -> Layer3_Device11_Residual1
	Layer3_Device11_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device11_Residual1 -> Layer3_Device11_LayerNorm2
	Layer3_Device11_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device11_LayerNorm2 -> Layer3_Device11_GateProj
	Layer3_Device11_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device11_LayerNorm2 -> Layer3_Device11_UpProj
	Layer3_Device11_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device11_GateProj -> Layer3_Device11_Activation
	Layer3_Device11_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device11_Activation -> Layer3_Device11_ElemMul
	Layer3_Device11_UpProj -> Layer3_Device11_ElemMul
	Layer3_Device11_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device11_ElemMul -> Layer3_Device11_DownProj
	Layer3_Device11_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device11_DownProj -> Layer3_Device11_Residual2
	Layer3_Device11_Residual1 -> Layer3_Device11_Residual2
	Layer3_Device11_Output [label="Layer 3 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device11_Residual2 -> Layer3_Device11_Output
	Layer3_Device12_Input [label="Layer 3 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device12_Output -> Layer3_Device12_Input
	Layer3_Device12_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device12_Input -> Layer3_Device12_LayerNorm1
	Layer3_Device12_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device12_LayerNorm1 -> Layer3_Device12_QKVProj
	Layer3_Device12_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device12_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage0_RecvKV -> Layer3_Device12_Stage0_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage0_Attention [label=Q_local]
	Layer3_Device12_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage0_Attention -> Layer3_Device12_Stage0_Accumulate
	Layer3_Device12_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage0_RecvKV -> Layer3_Device12_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage1_RecvKV -> Layer3_Device12_Stage1_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage1_Attention [label=Q_local]
	Layer3_Device12_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage1_Attention -> Layer3_Device12_Stage1_Accumulate
	Layer3_Device12_Stage0_Accumulate -> Layer3_Device12_Stage1_Accumulate
	Layer3_Device12_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage1_RecvKV -> Layer3_Device12_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage2_RecvKV -> Layer3_Device12_Stage2_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage2_Attention [label=Q_local]
	Layer3_Device12_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage2_Attention -> Layer3_Device12_Stage2_Accumulate
	Layer3_Device12_Stage1_Accumulate -> Layer3_Device12_Stage2_Accumulate
	Layer3_Device12_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage2_RecvKV -> Layer3_Device12_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage3_RecvKV -> Layer3_Device12_Stage3_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage3_Attention [label=Q_local]
	Layer3_Device12_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage3_Attention -> Layer3_Device12_Stage3_Accumulate
	Layer3_Device12_Stage2_Accumulate -> Layer3_Device12_Stage3_Accumulate
	Layer3_Device12_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage3_RecvKV -> Layer3_Device12_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage4_RecvKV -> Layer3_Device12_Stage4_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage4_Attention [label=Q_local]
	Layer3_Device12_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage4_Attention -> Layer3_Device12_Stage4_Accumulate
	Layer3_Device12_Stage3_Accumulate -> Layer3_Device12_Stage4_Accumulate
	Layer3_Device12_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage4_RecvKV -> Layer3_Device12_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage5_RecvKV -> Layer3_Device12_Stage5_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage5_Attention [label=Q_local]
	Layer3_Device12_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage5_Attention -> Layer3_Device12_Stage5_Accumulate
	Layer3_Device12_Stage4_Accumulate -> Layer3_Device12_Stage5_Accumulate
	Layer3_Device12_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage5_RecvKV -> Layer3_Device12_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage6_RecvKV -> Layer3_Device12_Stage6_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage6_Attention [label=Q_local]
	Layer3_Device12_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage6_Attention -> Layer3_Device12_Stage6_Accumulate
	Layer3_Device12_Stage5_Accumulate -> Layer3_Device12_Stage6_Accumulate
	Layer3_Device12_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage6_RecvKV -> Layer3_Device12_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage7_RecvKV -> Layer3_Device12_Stage7_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage7_Attention [label=Q_local]
	Layer3_Device12_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage7_Attention -> Layer3_Device12_Stage7_Accumulate
	Layer3_Device12_Stage6_Accumulate -> Layer3_Device12_Stage7_Accumulate
	Layer3_Device12_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage7_RecvKV -> Layer3_Device12_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage8_RecvKV -> Layer3_Device12_Stage8_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage8_Attention [label=Q_local]
	Layer3_Device12_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage8_Attention -> Layer3_Device12_Stage8_Accumulate
	Layer3_Device12_Stage7_Accumulate -> Layer3_Device12_Stage8_Accumulate
	Layer3_Device12_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage8_RecvKV -> Layer3_Device12_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage9_RecvKV -> Layer3_Device12_Stage9_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage9_Attention [label=Q_local]
	Layer3_Device12_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage9_Attention -> Layer3_Device12_Stage9_Accumulate
	Layer3_Device12_Stage8_Accumulate -> Layer3_Device12_Stage9_Accumulate
	Layer3_Device12_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage9_RecvKV -> Layer3_Device12_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage10_RecvKV -> Layer3_Device12_Stage10_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage10_Attention [label=Q_local]
	Layer3_Device12_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage10_Attention -> Layer3_Device12_Stage10_Accumulate
	Layer3_Device12_Stage9_Accumulate -> Layer3_Device12_Stage10_Accumulate
	Layer3_Device12_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage10_RecvKV -> Layer3_Device12_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage11_RecvKV -> Layer3_Device12_Stage11_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage11_Attention [label=Q_local]
	Layer3_Device12_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage11_Attention -> Layer3_Device12_Stage11_Accumulate
	Layer3_Device12_Stage10_Accumulate -> Layer3_Device12_Stage11_Accumulate
	Layer3_Device12_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage11_RecvKV -> Layer3_Device12_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage12_RecvKV -> Layer3_Device12_Stage12_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage12_Attention [label=Q_local]
	Layer3_Device12_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage12_Attention -> Layer3_Device12_Stage12_Accumulate
	Layer3_Device12_Stage11_Accumulate -> Layer3_Device12_Stage12_Accumulate
	Layer3_Device12_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage12_RecvKV -> Layer3_Device12_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage13_RecvKV -> Layer3_Device12_Stage13_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage13_Attention [label=Q_local]
	Layer3_Device12_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage13_Attention -> Layer3_Device12_Stage13_Accumulate
	Layer3_Device12_Stage12_Accumulate -> Layer3_Device12_Stage13_Accumulate
	Layer3_Device12_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage13_RecvKV -> Layer3_Device12_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage14_RecvKV -> Layer3_Device12_Stage14_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage14_Attention [label=Q_local]
	Layer3_Device12_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage14_Attention -> Layer3_Device12_Stage14_Accumulate
	Layer3_Device12_Stage13_Accumulate -> Layer3_Device12_Stage14_Accumulate
	Layer3_Device12_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device12_Stage14_RecvKV -> Layer3_Device12_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device12_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device12_Stage15_RecvKV -> Layer3_Device12_Stage15_Attention
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage15_Attention [label=Q_local]
	Layer3_Device12_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device12_Stage15_Attention -> Layer3_Device12_Stage15_Accumulate
	Layer3_Device12_Stage14_Accumulate -> Layer3_Device12_Stage15_Accumulate
	Layer3_Device12_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device12_Stage15_Accumulate -> Layer3_Device12_ConcatHeads
	Layer3_Device12_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device12_ConcatHeads -> Layer3_Device12_OutputProj
	Layer3_Device12_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device12_OutputProj -> Layer3_Device12_Residual1
	Layer3_Device12_Input -> Layer3_Device12_Residual1
	Layer3_Device12_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device12_Residual1 -> Layer3_Device12_LayerNorm2
	Layer3_Device12_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device12_LayerNorm2 -> Layer3_Device12_GateProj
	Layer3_Device12_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device12_LayerNorm2 -> Layer3_Device12_UpProj
	Layer3_Device12_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device12_GateProj -> Layer3_Device12_Activation
	Layer3_Device12_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device12_Activation -> Layer3_Device12_ElemMul
	Layer3_Device12_UpProj -> Layer3_Device12_ElemMul
	Layer3_Device12_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device12_ElemMul -> Layer3_Device12_DownProj
	Layer3_Device12_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device12_DownProj -> Layer3_Device12_Residual2
	Layer3_Device12_Residual1 -> Layer3_Device12_Residual2
	Layer3_Device12_Output [label="Layer 3 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device12_Residual2 -> Layer3_Device12_Output
	Layer3_Device13_Input [label="Layer 3 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device13_Output -> Layer3_Device13_Input
	Layer3_Device13_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device13_Input -> Layer3_Device13_LayerNorm1
	Layer3_Device13_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device13_LayerNorm1 -> Layer3_Device13_QKVProj
	Layer3_Device13_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device13_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage0_RecvKV -> Layer3_Device13_Stage0_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage0_Attention [label=Q_local]
	Layer3_Device13_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage0_Attention -> Layer3_Device13_Stage0_Accumulate
	Layer3_Device13_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage0_RecvKV -> Layer3_Device13_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage1_RecvKV -> Layer3_Device13_Stage1_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage1_Attention [label=Q_local]
	Layer3_Device13_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage1_Attention -> Layer3_Device13_Stage1_Accumulate
	Layer3_Device13_Stage0_Accumulate -> Layer3_Device13_Stage1_Accumulate
	Layer3_Device13_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage1_RecvKV -> Layer3_Device13_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage2_RecvKV -> Layer3_Device13_Stage2_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage2_Attention [label=Q_local]
	Layer3_Device13_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage2_Attention -> Layer3_Device13_Stage2_Accumulate
	Layer3_Device13_Stage1_Accumulate -> Layer3_Device13_Stage2_Accumulate
	Layer3_Device13_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage2_RecvKV -> Layer3_Device13_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage3_RecvKV -> Layer3_Device13_Stage3_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage3_Attention [label=Q_local]
	Layer3_Device13_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage3_Attention -> Layer3_Device13_Stage3_Accumulate
	Layer3_Device13_Stage2_Accumulate -> Layer3_Device13_Stage3_Accumulate
	Layer3_Device13_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage3_RecvKV -> Layer3_Device13_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage4_RecvKV -> Layer3_Device13_Stage4_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage4_Attention [label=Q_local]
	Layer3_Device13_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage4_Attention -> Layer3_Device13_Stage4_Accumulate
	Layer3_Device13_Stage3_Accumulate -> Layer3_Device13_Stage4_Accumulate
	Layer3_Device13_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage4_RecvKV -> Layer3_Device13_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage5_RecvKV -> Layer3_Device13_Stage5_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage5_Attention [label=Q_local]
	Layer3_Device13_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage5_Attention -> Layer3_Device13_Stage5_Accumulate
	Layer3_Device13_Stage4_Accumulate -> Layer3_Device13_Stage5_Accumulate
	Layer3_Device13_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage5_RecvKV -> Layer3_Device13_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage6_RecvKV -> Layer3_Device13_Stage6_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage6_Attention [label=Q_local]
	Layer3_Device13_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage6_Attention -> Layer3_Device13_Stage6_Accumulate
	Layer3_Device13_Stage5_Accumulate -> Layer3_Device13_Stage6_Accumulate
	Layer3_Device13_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage6_RecvKV -> Layer3_Device13_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage7_RecvKV -> Layer3_Device13_Stage7_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage7_Attention [label=Q_local]
	Layer3_Device13_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage7_Attention -> Layer3_Device13_Stage7_Accumulate
	Layer3_Device13_Stage6_Accumulate -> Layer3_Device13_Stage7_Accumulate
	Layer3_Device13_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage7_RecvKV -> Layer3_Device13_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage8_RecvKV -> Layer3_Device13_Stage8_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage8_Attention [label=Q_local]
	Layer3_Device13_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage8_Attention -> Layer3_Device13_Stage8_Accumulate
	Layer3_Device13_Stage7_Accumulate -> Layer3_Device13_Stage8_Accumulate
	Layer3_Device13_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage8_RecvKV -> Layer3_Device13_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage9_RecvKV -> Layer3_Device13_Stage9_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage9_Attention [label=Q_local]
	Layer3_Device13_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage9_Attention -> Layer3_Device13_Stage9_Accumulate
	Layer3_Device13_Stage8_Accumulate -> Layer3_Device13_Stage9_Accumulate
	Layer3_Device13_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage9_RecvKV -> Layer3_Device13_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage10_RecvKV -> Layer3_Device13_Stage10_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage10_Attention [label=Q_local]
	Layer3_Device13_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage10_Attention -> Layer3_Device13_Stage10_Accumulate
	Layer3_Device13_Stage9_Accumulate -> Layer3_Device13_Stage10_Accumulate
	Layer3_Device13_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage10_RecvKV -> Layer3_Device13_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage11_RecvKV -> Layer3_Device13_Stage11_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage11_Attention [label=Q_local]
	Layer3_Device13_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage11_Attention -> Layer3_Device13_Stage11_Accumulate
	Layer3_Device13_Stage10_Accumulate -> Layer3_Device13_Stage11_Accumulate
	Layer3_Device13_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage11_RecvKV -> Layer3_Device13_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage12_RecvKV -> Layer3_Device13_Stage12_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage12_Attention [label=Q_local]
	Layer3_Device13_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage12_Attention -> Layer3_Device13_Stage12_Accumulate
	Layer3_Device13_Stage11_Accumulate -> Layer3_Device13_Stage12_Accumulate
	Layer3_Device13_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage12_RecvKV -> Layer3_Device13_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage13_RecvKV -> Layer3_Device13_Stage13_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage13_Attention [label=Q_local]
	Layer3_Device13_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage13_Attention -> Layer3_Device13_Stage13_Accumulate
	Layer3_Device13_Stage12_Accumulate -> Layer3_Device13_Stage13_Accumulate
	Layer3_Device13_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage13_RecvKV -> Layer3_Device13_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage14_RecvKV -> Layer3_Device13_Stage14_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage14_Attention [label=Q_local]
	Layer3_Device13_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage14_Attention -> Layer3_Device13_Stage14_Accumulate
	Layer3_Device13_Stage13_Accumulate -> Layer3_Device13_Stage14_Accumulate
	Layer3_Device13_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device13_Stage14_RecvKV -> Layer3_Device13_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device13_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device13_Stage15_RecvKV -> Layer3_Device13_Stage15_Attention
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage15_Attention [label=Q_local]
	Layer3_Device13_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device13_Stage15_Attention -> Layer3_Device13_Stage15_Accumulate
	Layer3_Device13_Stage14_Accumulate -> Layer3_Device13_Stage15_Accumulate
	Layer3_Device13_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device13_Stage15_Accumulate -> Layer3_Device13_ConcatHeads
	Layer3_Device13_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device13_ConcatHeads -> Layer3_Device13_OutputProj
	Layer3_Device13_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device13_OutputProj -> Layer3_Device13_Residual1
	Layer3_Device13_Input -> Layer3_Device13_Residual1
	Layer3_Device13_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device13_Residual1 -> Layer3_Device13_LayerNorm2
	Layer3_Device13_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device13_LayerNorm2 -> Layer3_Device13_GateProj
	Layer3_Device13_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device13_LayerNorm2 -> Layer3_Device13_UpProj
	Layer3_Device13_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device13_GateProj -> Layer3_Device13_Activation
	Layer3_Device13_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device13_Activation -> Layer3_Device13_ElemMul
	Layer3_Device13_UpProj -> Layer3_Device13_ElemMul
	Layer3_Device13_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device13_ElemMul -> Layer3_Device13_DownProj
	Layer3_Device13_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device13_DownProj -> Layer3_Device13_Residual2
	Layer3_Device13_Residual1 -> Layer3_Device13_Residual2
	Layer3_Device13_Output [label="Layer 3 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device13_Residual2 -> Layer3_Device13_Output
	Layer3_Device14_Input [label="Layer 3 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device14_Output -> Layer3_Device14_Input
	Layer3_Device14_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device14_Input -> Layer3_Device14_LayerNorm1
	Layer3_Device14_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device14_LayerNorm1 -> Layer3_Device14_QKVProj
	Layer3_Device14_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device14_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage0_RecvKV -> Layer3_Device14_Stage0_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage0_Attention [label=Q_local]
	Layer3_Device14_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage0_Attention -> Layer3_Device14_Stage0_Accumulate
	Layer3_Device14_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage0_RecvKV -> Layer3_Device14_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage1_RecvKV -> Layer3_Device14_Stage1_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage1_Attention [label=Q_local]
	Layer3_Device14_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage1_Attention -> Layer3_Device14_Stage1_Accumulate
	Layer3_Device14_Stage0_Accumulate -> Layer3_Device14_Stage1_Accumulate
	Layer3_Device14_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage1_RecvKV -> Layer3_Device14_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage2_RecvKV -> Layer3_Device14_Stage2_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage2_Attention [label=Q_local]
	Layer3_Device14_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage2_Attention -> Layer3_Device14_Stage2_Accumulate
	Layer3_Device14_Stage1_Accumulate -> Layer3_Device14_Stage2_Accumulate
	Layer3_Device14_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage2_RecvKV -> Layer3_Device14_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage3_RecvKV -> Layer3_Device14_Stage3_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage3_Attention [label=Q_local]
	Layer3_Device14_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage3_Attention -> Layer3_Device14_Stage3_Accumulate
	Layer3_Device14_Stage2_Accumulate -> Layer3_Device14_Stage3_Accumulate
	Layer3_Device14_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage3_RecvKV -> Layer3_Device14_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage4_RecvKV -> Layer3_Device14_Stage4_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage4_Attention [label=Q_local]
	Layer3_Device14_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage4_Attention -> Layer3_Device14_Stage4_Accumulate
	Layer3_Device14_Stage3_Accumulate -> Layer3_Device14_Stage4_Accumulate
	Layer3_Device14_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage4_RecvKV -> Layer3_Device14_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage5_RecvKV -> Layer3_Device14_Stage5_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage5_Attention [label=Q_local]
	Layer3_Device14_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage5_Attention -> Layer3_Device14_Stage5_Accumulate
	Layer3_Device14_Stage4_Accumulate -> Layer3_Device14_Stage5_Accumulate
	Layer3_Device14_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage5_RecvKV -> Layer3_Device14_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage6_RecvKV -> Layer3_Device14_Stage6_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage6_Attention [label=Q_local]
	Layer3_Device14_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage6_Attention -> Layer3_Device14_Stage6_Accumulate
	Layer3_Device14_Stage5_Accumulate -> Layer3_Device14_Stage6_Accumulate
	Layer3_Device14_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage6_RecvKV -> Layer3_Device14_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage7_RecvKV -> Layer3_Device14_Stage7_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage7_Attention [label=Q_local]
	Layer3_Device14_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage7_Attention -> Layer3_Device14_Stage7_Accumulate
	Layer3_Device14_Stage6_Accumulate -> Layer3_Device14_Stage7_Accumulate
	Layer3_Device14_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage7_RecvKV -> Layer3_Device14_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage8_RecvKV -> Layer3_Device14_Stage8_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage8_Attention [label=Q_local]
	Layer3_Device14_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage8_Attention -> Layer3_Device14_Stage8_Accumulate
	Layer3_Device14_Stage7_Accumulate -> Layer3_Device14_Stage8_Accumulate
	Layer3_Device14_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage8_RecvKV -> Layer3_Device14_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage9_RecvKV -> Layer3_Device14_Stage9_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage9_Attention [label=Q_local]
	Layer3_Device14_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage9_Attention -> Layer3_Device14_Stage9_Accumulate
	Layer3_Device14_Stage8_Accumulate -> Layer3_Device14_Stage9_Accumulate
	Layer3_Device14_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage9_RecvKV -> Layer3_Device14_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage10_RecvKV -> Layer3_Device14_Stage10_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage10_Attention [label=Q_local]
	Layer3_Device14_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage10_Attention -> Layer3_Device14_Stage10_Accumulate
	Layer3_Device14_Stage9_Accumulate -> Layer3_Device14_Stage10_Accumulate
	Layer3_Device14_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage10_RecvKV -> Layer3_Device14_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage11_RecvKV -> Layer3_Device14_Stage11_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage11_Attention [label=Q_local]
	Layer3_Device14_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage11_Attention -> Layer3_Device14_Stage11_Accumulate
	Layer3_Device14_Stage10_Accumulate -> Layer3_Device14_Stage11_Accumulate
	Layer3_Device14_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage11_RecvKV -> Layer3_Device14_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage12_RecvKV -> Layer3_Device14_Stage12_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage12_Attention [label=Q_local]
	Layer3_Device14_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage12_Attention -> Layer3_Device14_Stage12_Accumulate
	Layer3_Device14_Stage11_Accumulate -> Layer3_Device14_Stage12_Accumulate
	Layer3_Device14_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage12_RecvKV -> Layer3_Device14_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage13_RecvKV -> Layer3_Device14_Stage13_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage13_Attention [label=Q_local]
	Layer3_Device14_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage13_Attention -> Layer3_Device14_Stage13_Accumulate
	Layer3_Device14_Stage12_Accumulate -> Layer3_Device14_Stage13_Accumulate
	Layer3_Device14_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage13_RecvKV -> Layer3_Device14_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage14_RecvKV -> Layer3_Device14_Stage14_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage14_Attention [label=Q_local]
	Layer3_Device14_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage14_Attention -> Layer3_Device14_Stage14_Accumulate
	Layer3_Device14_Stage13_Accumulate -> Layer3_Device14_Stage14_Accumulate
	Layer3_Device14_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device14_Stage14_RecvKV -> Layer3_Device14_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device14_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device14_Stage15_RecvKV -> Layer3_Device14_Stage15_Attention
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage15_Attention [label=Q_local]
	Layer3_Device14_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device14_Stage15_Attention -> Layer3_Device14_Stage15_Accumulate
	Layer3_Device14_Stage14_Accumulate -> Layer3_Device14_Stage15_Accumulate
	Layer3_Device14_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device14_Stage15_Accumulate -> Layer3_Device14_ConcatHeads
	Layer3_Device14_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device14_ConcatHeads -> Layer3_Device14_OutputProj
	Layer3_Device14_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device14_OutputProj -> Layer3_Device14_Residual1
	Layer3_Device14_Input -> Layer3_Device14_Residual1
	Layer3_Device14_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device14_Residual1 -> Layer3_Device14_LayerNorm2
	Layer3_Device14_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device14_LayerNorm2 -> Layer3_Device14_GateProj
	Layer3_Device14_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device14_LayerNorm2 -> Layer3_Device14_UpProj
	Layer3_Device14_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device14_GateProj -> Layer3_Device14_Activation
	Layer3_Device14_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device14_Activation -> Layer3_Device14_ElemMul
	Layer3_Device14_UpProj -> Layer3_Device14_ElemMul
	Layer3_Device14_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device14_ElemMul -> Layer3_Device14_DownProj
	Layer3_Device14_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device14_DownProj -> Layer3_Device14_Residual2
	Layer3_Device14_Residual1 -> Layer3_Device14_Residual2
	Layer3_Device14_Output [label="Layer 3 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device14_Residual2 -> Layer3_Device14_Output
	Layer3_Device15_Input [label="Layer 3 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer2_Device15_Output -> Layer3_Device15_Input
	Layer3_Device15_LayerNorm1 [label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device15_Input -> Layer3_Device15_LayerNorm1
	Layer3_Device15_QKVProj [label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device15_LayerNorm1 -> Layer3_Device15_QKVProj
	Layer3_Device15_Stage0_RecvKV [label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage0_RecvKV [label="Local K,V"]
	Layer3_Device15_Stage0_Attention [label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage0_RecvKV -> Layer3_Device15_Stage0_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage0_Attention [label=Q_local]
	Layer3_Device15_Stage0_Accumulate [label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage0_Attention -> Layer3_Device15_Stage0_Accumulate
	Layer3_Device15_Stage1_RecvKV [label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage0_RecvKV -> Layer3_Device15_Stage1_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage1_Attention [label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage1_RecvKV -> Layer3_Device15_Stage1_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage1_Attention [label=Q_local]
	Layer3_Device15_Stage1_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage1_Attention -> Layer3_Device15_Stage1_Accumulate
	Layer3_Device15_Stage0_Accumulate -> Layer3_Device15_Stage1_Accumulate
	Layer3_Device15_Stage2_RecvKV [label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage1_RecvKV -> Layer3_Device15_Stage2_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage2_Attention [label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage2_RecvKV -> Layer3_Device15_Stage2_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage2_Attention [label=Q_local]
	Layer3_Device15_Stage2_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage2_Attention -> Layer3_Device15_Stage2_Accumulate
	Layer3_Device15_Stage1_Accumulate -> Layer3_Device15_Stage2_Accumulate
	Layer3_Device15_Stage3_RecvKV [label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage2_RecvKV -> Layer3_Device15_Stage3_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage3_Attention [label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage3_RecvKV -> Layer3_Device15_Stage3_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage3_Attention [label=Q_local]
	Layer3_Device15_Stage3_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage3_Attention -> Layer3_Device15_Stage3_Accumulate
	Layer3_Device15_Stage2_Accumulate -> Layer3_Device15_Stage3_Accumulate
	Layer3_Device15_Stage4_RecvKV [label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage3_RecvKV -> Layer3_Device15_Stage4_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage4_Attention [label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage4_RecvKV -> Layer3_Device15_Stage4_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage4_Attention [label=Q_local]
	Layer3_Device15_Stage4_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage4_Attention -> Layer3_Device15_Stage4_Accumulate
	Layer3_Device15_Stage3_Accumulate -> Layer3_Device15_Stage4_Accumulate
	Layer3_Device15_Stage5_RecvKV [label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage4_RecvKV -> Layer3_Device15_Stage5_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage5_Attention [label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage5_RecvKV -> Layer3_Device15_Stage5_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage5_Attention [label=Q_local]
	Layer3_Device15_Stage5_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage5_Attention -> Layer3_Device15_Stage5_Accumulate
	Layer3_Device15_Stage4_Accumulate -> Layer3_Device15_Stage5_Accumulate
	Layer3_Device15_Stage6_RecvKV [label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage5_RecvKV -> Layer3_Device15_Stage6_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage6_Attention [label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage6_RecvKV -> Layer3_Device15_Stage6_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage6_Attention [label=Q_local]
	Layer3_Device15_Stage6_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage6_Attention -> Layer3_Device15_Stage6_Accumulate
	Layer3_Device15_Stage5_Accumulate -> Layer3_Device15_Stage6_Accumulate
	Layer3_Device15_Stage7_RecvKV [label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage6_RecvKV -> Layer3_Device15_Stage7_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage7_Attention [label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage7_RecvKV -> Layer3_Device15_Stage7_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage7_Attention [label=Q_local]
	Layer3_Device15_Stage7_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage7_Attention -> Layer3_Device15_Stage7_Accumulate
	Layer3_Device15_Stage6_Accumulate -> Layer3_Device15_Stage7_Accumulate
	Layer3_Device15_Stage8_RecvKV [label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage7_RecvKV -> Layer3_Device15_Stage8_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage8_Attention [label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage8_RecvKV -> Layer3_Device15_Stage8_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage8_Attention [label=Q_local]
	Layer3_Device15_Stage8_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage8_Attention -> Layer3_Device15_Stage8_Accumulate
	Layer3_Device15_Stage7_Accumulate -> Layer3_Device15_Stage8_Accumulate
	Layer3_Device15_Stage9_RecvKV [label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage8_RecvKV -> Layer3_Device15_Stage9_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage9_Attention [label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage9_RecvKV -> Layer3_Device15_Stage9_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage9_Attention [label=Q_local]
	Layer3_Device15_Stage9_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage9_Attention -> Layer3_Device15_Stage9_Accumulate
	Layer3_Device15_Stage8_Accumulate -> Layer3_Device15_Stage9_Accumulate
	Layer3_Device15_Stage10_RecvKV [label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage9_RecvKV -> Layer3_Device15_Stage10_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage10_Attention [label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage10_RecvKV -> Layer3_Device15_Stage10_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage10_Attention [label=Q_local]
	Layer3_Device15_Stage10_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage10_Attention -> Layer3_Device15_Stage10_Accumulate
	Layer3_Device15_Stage9_Accumulate -> Layer3_Device15_Stage10_Accumulate
	Layer3_Device15_Stage11_RecvKV [label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage10_RecvKV -> Layer3_Device15_Stage11_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage11_Attention [label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage11_RecvKV -> Layer3_Device15_Stage11_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage11_Attention [label=Q_local]
	Layer3_Device15_Stage11_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage11_Attention -> Layer3_Device15_Stage11_Accumulate
	Layer3_Device15_Stage10_Accumulate -> Layer3_Device15_Stage11_Accumulate
	Layer3_Device15_Stage12_RecvKV [label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage11_RecvKV -> Layer3_Device15_Stage12_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage12_Attention [label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage12_RecvKV -> Layer3_Device15_Stage12_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage12_Attention [label=Q_local]
	Layer3_Device15_Stage12_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage12_Attention -> Layer3_Device15_Stage12_Accumulate
	Layer3_Device15_Stage11_Accumulate -> Layer3_Device15_Stage12_Accumulate
	Layer3_Device15_Stage13_RecvKV [label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage12_RecvKV -> Layer3_Device15_Stage13_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage13_Attention [label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage13_RecvKV -> Layer3_Device15_Stage13_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage13_Attention [label=Q_local]
	Layer3_Device15_Stage13_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage13_Attention -> Layer3_Device15_Stage13_Accumulate
	Layer3_Device15_Stage12_Accumulate -> Layer3_Device15_Stage13_Accumulate
	Layer3_Device15_Stage14_RecvKV [label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage13_RecvKV -> Layer3_Device15_Stage14_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage14_Attention [label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage14_RecvKV -> Layer3_Device15_Stage14_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage14_Attention [label=Q_local]
	Layer3_Device15_Stage14_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage14_Attention -> Layer3_Device15_Stage14_Accumulate
	Layer3_Device15_Stage13_Accumulate -> Layer3_Device15_Stage14_Accumulate
	Layer3_Device15_Stage15_RecvKV [label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightyellow shape=parallelogram style=dashed]
	Layer3_Device15_Stage14_RecvKV -> Layer3_Device15_Stage15_RecvKV [label="Ring transfer"]
	Layer3_Device15_Stage15_Attention [label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightpink shape=rectangle]
	Layer3_Device15_Stage15_RecvKV -> Layer3_Device15_Stage15_Attention
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage15_Attention [label=Q_local]
	Layer3_Device15_Stage15_Accumulate [label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]" fillcolor=lightcyan shape=rectangle]
	Layer3_Device15_Stage15_Attention -> Layer3_Device15_Stage15_Accumulate
	Layer3_Device15_Stage14_Accumulate -> Layer3_Device15_Stage15_Accumulate
	Layer3_Device15_ConcatHeads [label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightblue shape=rectangle]
	Layer3_Device15_Stage15_Accumulate -> Layer3_Device15_ConcatHeads
	Layer3_Device15_OutputProj [label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device15_ConcatHeads -> Layer3_Device15_OutputProj
	Layer3_Device15_Residual1 [label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device15_OutputProj -> Layer3_Device15_Residual1
	Layer3_Device15_Input -> Layer3_Device15_Residual1
	Layer3_Device15_LayerNorm2 [label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device15_Residual1 -> Layer3_Device15_LayerNorm2
	Layer3_Device15_GateProj [label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device15_LayerNorm2 -> Layer3_Device15_GateProj
	Layer3_Device15_UpProj [label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device15_LayerNorm2 -> Layer3_Device15_UpProj
	Layer3_Device15_Activation [label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device15_GateProj -> Layer3_Device15_Activation
	Layer3_Device15_ElemMul [label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]" fillcolor=lightgreen shape=rectangle]
	Layer3_Device15_Activation -> Layer3_Device15_ElemMul
	Layer3_Device15_UpProj -> Layer3_Device15_ElemMul
	Layer3_Device15_DownProj [label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightcoral shape=rectangle]
	Layer3_Device15_ElemMul -> Layer3_Device15_DownProj
	Layer3_Device15_Residual2 [label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=rectangle]
	Layer3_Device15_DownProj -> Layer3_Device15_Residual2
	Layer3_Device15_Residual1 -> Layer3_Device15_Residual2
	Layer3_Device15_Output [label="Layer 3 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]" fillcolor=lightgray shape=ellipse]
	Layer3_Device15_Residual2 -> Layer3_Device15_Output
	Sequence_Aggregate [label="Sequence Aggregate\nInput: 16Ã—[batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]" fillcolor=lightyellow shape=parallelogram]
	Layer3_Device0_Output -> Sequence_Aggregate
	Layer3_Device1_Output -> Sequence_Aggregate
	Layer3_Device2_Output -> Sequence_Aggregate
	Layer3_Device3_Output -> Sequence_Aggregate
	Layer3_Device4_Output -> Sequence_Aggregate
	Layer3_Device5_Output -> Sequence_Aggregate
	Layer3_Device6_Output -> Sequence_Aggregate
	Layer3_Device7_Output -> Sequence_Aggregate
	Layer3_Device8_Output -> Sequence_Aggregate
	Layer3_Device9_Output -> Sequence_Aggregate
	Layer3_Device10_Output -> Sequence_Aggregate
	Layer3_Device11_Output -> Sequence_Aggregate
	Layer3_Device12_Output -> Sequence_Aggregate
	Layer3_Device13_Output -> Sequence_Aggregate
	Layer3_Device14_Output -> Sequence_Aggregate
	Layer3_Device15_Output -> Sequence_Aggregate
	Output_Total [label="Final Output\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]" fillcolor=lightblue shape=ellipse]
	Sequence_Aggregate -> Output_Total
}
