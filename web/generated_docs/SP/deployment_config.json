{
  "deployment_config": {
    "baseline": {
      "model_name": "Dense_Transformer_Baseline",
      "architecture": {
        "layers": 4,
        "hidden_size": 8192,
        "attention_heads": 16,
        "head_dimension": 512,
        "mlp_hidden_size": 32768,
        "sequence_length": 10000,
        "batch_size": 1024,
        "precision": "fp16"
      },
      "parallel_strategy": {
        "tensor_parallelism": {
          "enabled": true,
          "degree": 8,
          "partition_method": "column_row",
          "communication": "all_reduce"
        },
        "pipeline_parallelism": {
          "enabled": true,
          "degree": 2,
          "layers_per_stage": [2, 2],
          "communication": "send_recv"
        },
        "sequence_parallelism": {
          "enabled": false,
          "degree": 1
        },
        "ring_attention": {
          "enabled": false,
          "ring_size": 1
        }
      },
      "device_mapping": {
        "total_devices": 16,
        "tp_devices": 8,
        "pp_stages": 2,
        "stage_0": {
          "devices": [0, 1, 2, 3, 4, 5, 6, 7],
          "layers": [0, 1],
          "tensor_parallel_group": "0-7"
        },
        "stage_1": {
          "devices": [8, 9, 10, 11, 12, 13, 14, 15],
          "layers": [2, 3],
          "tensor_parallel_group": "8-15"
        }
      },
      "module_division": {
        "attention_layers": {
          "projection_qkv": {
            "type": "column_parallel",
            "devices": "per_tp_group",
            "parameters": {
              "q_proj": [4096, 8192],
              "k_proj": [4096, 8192],
              "v_proj": [4096, 8192]
            }
          },
          "attention_output": {
            "type": "row_parallel",
            "devices": "per_tp_group",
            "parameters": [8192, 8192]
          }
        },
        "mlp_layers": {
          "gate_up_proj": {
            "type": "column_parallel",
            "devices": "per_tp_group",
            "parameters": [32768, 8192]
          },
          "down_proj": {
            "type": "row_parallel",
            "devices": "per_tp_group",
            "parameters": [8192, 32768]
          }
        }
      }
    },
    "ra_sp_model": {
      "model_name": "Dense_Transformer_RA_SP",
      "architecture": {
        "layers": 4,
        "hidden_size": 8192,
        "attention_heads": 16,
        "head_dimension": 512,
        "mlp_hidden_size": 32768,
        "sequence_length": 10000,
        "batch_size": 1024,
        "precision": "fp16"
      },
      "parallel_strategy": {
        "sequence_parallelism": {
          "enabled": true,
          "degree": 16,
          "sequence_chunk_size": 625,
          "communication": "ring_all_reduce"
        },
        "ring_attention": {
          "enabled": true,
          "ring_size": 16,
          "topology": "logical_ring",
          "communication": "send_recv_p2p",
          "stages": 16,
          "overlap": true
        },
        "tensor_parallelism": {
          "enabled": false,
          "degree": 1
        },
        "pipeline_parallelism": {
          "enabled": false,
          "degree": 1
        }
      },
      "device_mapping": {
        "total_devices": 16,
        "ring_devices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
        "sequence_chunks": {
          "device_0": {"sequence_range": [0, 625], "layers": [0, 1, 2, 3]},
          "device_1": {"sequence_range": [625, 1250], "layers": [0, 1, 2, 3]},
          "device_2": {"sequence_range": [1250, 1875], "layers": [0, 1, 2, 3]},
          "device_3": {"sequence_range": [1875, 2500], "layers": [0, 1, 2, 3]},
          "device_4": {"sequence_range": [2500, 3125], "layers": [0, 1, 2, 3]},
          "device_5": {"sequence_range": [3125, 3750], "layers": [0, 1, 2, 3]},
          "device_6": {"sequence_range": [3750, 4375], "layers": [0, 1, 2, 3]},
          "device_7": {"sequence_range": [4375, 5000], "layers": [0, 1, 2, 3]},
          "device_8": {"sequence_range": [5000, 5625], "layers": [0, 1, 2, 3]},
          "device_9": {"sequence_range": [5625, 6250], "layers": [0, 1, 2, 3]},
          "device_10": {"sequence_range": [6250, 6875], "layers": [0, 1, 2, 3]},
          "device_11": {"sequence_range": [6875, 7500], "layers": [0, 1, 2, 3]},
          "device_12": {"sequence_range": [7500, 8125], "layers": [0, 1, 2, 3]},
          "device_13": {"sequence_range": [8125, 8750], "layers": [0, 1, 2, 3]},
          "device_14": {"sequence_range": [8750, 9375], "layers": [0, 1, 2, 3]},
          "device_15": {"sequence_range": [9375, 10000], "layers": [0, 1, 2, 3]}
        }
      },
      "module_division": {
        "attention_layers": {
          "projection_qkv": {
            "type": "full_replicated",
            "devices": "all_devices",
            "parameters": {
              "q_proj": [4096, 8192],
              "k_proj": [4096, 8192],
              "v_proj": [4096, 8192]
            },
            "sequence_partition": "each_device_processes_625_tokens"
          },
          "attention_output": {
            "type": "full_replicated",
            "devices": "all_devices",
            "parameters": [8192, 8192],
            "sequence_partition": "each_device_produces_625_tokens"
          }
        },
        "mlp_layers": {
          "gate_up_proj": {
            "type": "full_replicated",
            "devices": "all_devices",
            "parameters": [32768, 8192],
            "sequence_partition": "each_device_processes_625_tokens"
          },
          "down_proj": {
            "type": "full_replicated",
            "devices": "all_devices",
            "parameters": [8192, 32768],
            "sequence_partition": "each_device_produces_625_tokens"
          }
        },
        "ring_attention_modules": {
          "local_kv_computation": {
            "type": "sequence_parallel",
            "devices": "per_device",
            "sequence_range": "device_specific"
          },
          "ring_kv_exchange": {
            "type": "ring_communication",
            "ring_size": 16,
            "stages": 16,
            "data_transfer": "625×512×2_bytes_per_stage",
            "overlap_computation": true
          }
        }
      },
      "communication_schedule": {
        "stage_0": {
          "device_0": {"send_to": 1, "recv_from": 15, "data": "KV_chunk_0"},
          "device_1": {"send_to": 2, "recv_from": 0, "data": "KV_chunk_1"},
          "device_2": {"send_to": 3, "recv_from": 1, "data": "KV_chunk_2"},
          "device_3": {"send_to": 4, "recv_from": 2, "data": "KV_chunk_3"},
          "device_4": {"send_to": 5, "recv_from": 3, "data": "KV_chunk_4"},
          "device_5": {"send_to": 6, "recv_from": 4, "data": "KV_chunk_5"},
          "device_6": {"send_to": 7, "recv_from": 5, "data": "KV_chunk_6"},
          "device_7": {"send_to": 8, "recv_from": 6, "data": "KV_chunk_7"},
          "device_8": {"send_to": 9, "recv_from": 7, "data": "KV_chunk_8"},
          "device_9": {"send_to": 10, "recv_from": 8, "data": "KV_chunk_9"},
          "device_10": {"send_to": 11, "recv_from": 9, "data": "KV_chunk_10"},
          "device_11": {"send_to": 12, "recv_from": 10, "data": "KV_chunk_11"},
          "device_12": {"send_to": 13, "recv_from": 11, "data": "KV_chunk_12"},
          "device_13": {"send_to": 14, "recv_from": 12, "data": "KV_chunk_13"},
          "device_14": {"send_to": 15, "recv_from": 13, "data": "KV_chunk_14"},
          "device_15": {"send_to": 0, "recv_from": 14, "data": "KV_chunk_15"}
        }
      },
      "memory_footprint": {
        "per_device_activation": "1/16th_of_baseline",
        "sequence_chunk_size": 625,
        "local_kv_storage": "625×512×2_fp16_tensors",
        "total_communication": "16_stages_×_625×512×2_bytes"
      }
    }
  }
}