{
  "metadata": {
    "version": "1.0",
    "category": "Large Model Module Optimization",
    "total_score": 100,
    "classification_threshold": 45,
    "description": "Structured evaluation schema for determining whether a paper focuses on optimizing a specific module or component within a large model architecture."
  },
  "criteria": [
    {
      "id": "C1",
      "name": "Attention Mechanism Optimization",
      "keywords": [
        "attention optimization",
        "efficient attention",
        "sparse attention",
        "linear attention",
        "flash attention",
        "memory-efficient attention",
        "KV cache optimization"
      ],
      "score_weight": 20,
      "detection_rule": "Assign full score if the paper proposes or analyzes modifications to the attention mechanism for efficiency or scalability; partial score if only mentions attention improvement.",
      "examples": [
        "We propose a linear-time attention variant that reduces quadratic complexity.",
        "FlashAttention is integrated to improve GPU throughput during training."
      ]
    },
    {
      "id": "C2",
      "name": "Feedforward / MLP Block Optimization",
      "keywords": [
        "feedforward network optimization",
        "MLP layer optimization",
        "activation function optimization",
        "layer fusion",
        "low-rank approximation",
        "FFN compression"
      ],
      "score_weight": 15,
      "detection_rule": "Award score if the paper targets efficiency or accuracy improvements in feedforward or MLP components.",
      "examples": [
        "We replace standard FFN blocks with low-rank decompositions to reduce FLOPs."
      ]
    },
    {
      "id": "C3",
      "name": "Embedding Layer Optimization",
      "keywords": [
        "embedding optimization",
        "shared embeddings",
        "vocabulary compression",
        "subword embedding",
        "hash embedding",
        "learned positional encoding"
      ],
      "score_weight": 10,
      "detection_rule": "Assign score if the paper optimizes embedding computation, size, or memory usage.",
      "examples": [
        "The model employs adaptive input embeddings to reduce memory footprint."
      ]
    },
    {
      "id": "C4",
      "name": "Normalization and Residual Optimization",
      "keywords": [
        "layer normalization optimization",
        "rmsnorm",
        "skip connection optimization",
        "residual scaling",
        "normalization-free transformer"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if the paper modifies normalization or residual mechanisms for performance, stability, or efficiency.",
      "examples": [
        "We replace LayerNorm with RMSNorm to stabilize large-scale training."
      ]
    },
    {
      "id": "C5",
      "name": "Optimizer and Training Dynamics Optimization",
      "keywords": [
        "optimizer optimization",
        "adaptive learning rate",
        "AdamW",
        "LAMB",
        "momentum tuning",
        "gradient clipping",
        "training stability"
      ],
      "score_weight": 15,
      "detection_rule": "Assign score if the paper introduces or evaluates new optimization algorithms or training dynamics targeted for large models.",
      "examples": [
        "We adopt the LAMB optimizer to stabilize large-batch training in billion-scale models."
      ]
    },
    {
      "id": "C6",
      "name": "Memory and Computation Efficiency Techniques",
      "keywords": [
        "quantization",
        "pruning",
        "knowledge distillation",
        "mixed precision",
        "low-bit training",
        "tensor decomposition"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if the paper applies general efficiency methods specifically to one or more internal modules of large models.",
      "examples": [
        "We apply block-wise quantization to the attention and FFN layers."
      ]
    },
    {
      "id": "C7",
      "name": "Module-Specific Hardware or Kernel Optimization",
      "keywords": [
        "custom kernel",
        "fused operator",
        "CUDA kernel optimization",
        "kernel fusion",
        "hardware-aware optimization",
        "Tensor Core utilization"
      ],
      "score_weight": 10,
      "detection_rule": "Assign score if the paper targets performance improvements via kernel-level or hardware-specific optimizations for a given model module.",
      "examples": [
        "We implement a fused attention kernel optimized for Tensor Cores."
      ]
    },
    {
      "id": "C8",
      "name": "Performance Evaluation and Ablation",
      "keywords": [
        "ablation study",
        "module efficiency analysis",
        "latency reduction",
        "throughput improvement",
        "FLOPs reduction",
        "speedup measurement"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if the paper quantitatively measures the impact of module-level optimizations on model performance or efficiency.",
      "examples": [
        "An ablation study shows a 40% reduction in attention FLOPs."
      ]
    }
  ],
  "evaluation_rule": {
    "method": "sum_weighted_scores",
    "decision_logic": {
      ">=45": "Classified as 'Large Model Module Optimization' research paper",
      "30-44": "Partially related; optimization of specific modules discussed but not the main focus",
      "<30": "Not classified as Large Model Module Optimization research"
    },
    "scoring_guideline": "Each criterion may receive partial credit (0â€“100% of its weight) depending on coverage depth and technical relevance."
  }
}
