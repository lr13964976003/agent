{
  "metadata": {
    "version": "2.0",
    "category": "Large Model Parallelism",
    "total_score": 100,
    "classification_threshold": 45,
    "description": "Structured evaluation schema for determining whether a paper belongs to the Large Model Parallelism research area."
  },
  "criteria": [
    {
      "id": "C1",
      "name": "Model Parallelism",
      "keywords": [
        "model parallelism",
        "tensor parallelism",
        "layer parallelism",
        "intra-layer parallelism",
        "operator-level parallelism"
      ],
      "score_weight": 20,
      "detection_rule": "Award full score if the paper explicitly discusses partitioning model parameters, tensors, or layers across multiple devices or processes; award partial score if only briefly mentioned.",
      "examples": [
        "The paper proposes a tensor-parallel training scheme for transformer layers.",
        "Layer partitioning is implemented to distribute attention blocks across GPUs."
      ]
    },
    {
      "id": "C2",
      "name": "Pipeline Parallelism",
      "keywords": [
        "pipeline parallelism",
        "GPipe",
        "PipeDream",
        "stage partitioning",
        "micro-batch scheduling"
      ],
      "score_weight": 15,
      "detection_rule": "Assign score if the paper introduces or analyzes stage-based model execution pipelines or asynchronous scheduling of model stages.",
      "examples": [
        "We adopt a pipeline-parallel approach similar to GPipe to overlap forward and backward passes."
      ]
    },
    {
      "id": "C3",
      "name": "Data Parallelism and Distributed Training",
      "keywords": [
        "data parallelism",
        "distributed training",
        "synchronous gradient update",
        "gradient synchronization",
        "all-reduce"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if data parallelism is discussed as a baseline, combined approach, or comparison to model/pipeline parallelism.",
      "examples": [
        "We combine data parallelism with model parallelism for large-scale transformer training."
      ]
    },
    {
      "id": "C4",
      "name": "Hybrid or 3D Parallelism",
      "keywords": [
        "hybrid parallelism",
        "3D parallelism",
        "tensor + pipeline parallelism",
        "ZeRO",
        "DeepSpeed",
        "Megatron-LM"
      ],
      "score_weight": 15,
      "detection_rule": "Assign score if the paper integrates multiple parallelism strategies (data + tensor + pipeline) or implements systems like ZeRO or Megatron for hybrid scaling.",
      "examples": [
        "We use 3D parallelism combining data, tensor, and pipeline parallelism for trillion-parameter training."
      ]
    },
    {
      "id": "C5",
      "name": "Communication Optimization",
      "keywords": [
        "communication overlap",
        "bandwidth optimization",
        "collective operations",
        "NCCL",
        "AllReduce",
        "parameter synchronization"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if the paper proposes or evaluates methods to reduce communication overhead in distributed training.",
      "examples": [
        "We overlap communication with computation to reduce AllReduce latency."
      ]
    },
    {
      "id": "C6",
      "name": "Memory Optimization for Parallel Training",
      "keywords": [
        "activation checkpointing",
        "offloading",
        "parameter sharding",
        "memory-efficient parallel training",
        "optimizer state partitioning"
      ],
      "score_weight": 10,
      "detection_rule": "Assign score if the paper includes methods to reduce memory footprint in large-scale parallel models.",
      "examples": [
        "Activation checkpointing is applied to enable training of 100B+ parameter models."
      ]
    },
    {
      "id": "C7",
      "name": "Parallel Training Systems or Frameworks",
      "keywords": [
        "DeepSpeed",
        "Megatron",
        "Colossal-AI",
        "Alpa",
        "Mesh TensorFlow",
        "PaLM",
        "TPU Mesh"
      ],
      "score_weight": 10,
      "detection_rule": "Award score if the paper introduces, extends, or benchmarks a distributed training system or framework related to parallelism.",
      "examples": [
        "Our method is implemented on top of Megatron-LM and DeepSpeed frameworks."
      ]
    },
    {
      "id": "C8",
      "name": "Scalability and Performance Analysis",
      "keywords": [
        "scalability",
        "speedup",
        "throughput",
        "efficiency",
        "strong scaling",
        "weak scaling",
        "benchmarking"
      ],
      "score_weight": 10,
      "detection_rule": "Assign score if the paper reports experimental results demonstrating parallel efficiency, scalability, or performance metrics across devices or clusters.",
      "examples": [
        "The proposed scheme achieves 85% strong scaling efficiency across 512 GPUs."
      ]
    }
  ],
  "evaluation_rule": {
    "method": "sum_weighted_scores",
    "decision_logic": {
      ">=45": "Classified as 'Large Model Parallelism' research paper",
      "30-44": "Partially related; contains relevant parallelism discussion but not a central focus",
      "<30": "Not classified as Large Model Parallelism research"
    },
    "scoring_guideline": "Each criterion may be partially fulfilled (0â€“100% of its weight). The evaluator can assign partial credit based on the extent of coverage or technical relevance."
  }
}
