<p>1
Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6:
Industry Track), pages 1–9
June 16-21, 2024 ©2024 Association for Computational Linguistics</p>
<p>HPipe:LargeLanguageModelPipelineParallelismforLongContextonHeterogeneousCost-effectiveDevicesRuilongMa<em>,XiangYang</em>,JingyuWang,QiQi,HaifengSun†,JingWang†,ZiruiZhuang,JianxinLiaoStateKeyLaboratoryofNetworkingandSwitchingTechnologyBeijingUniversityofPostsandTelecommunications{maruilong,yangxiang,wangjingyu,qiqi8266,hfsun,wangjingzhuangzirui,liaojx}@bupt.edu.cnAbstractMicro-enterprisesandindividualdevelopersemergelongcontextanalysisdemandswithpowerfulLargeLanguageModels(LLMs).TheytrytodeploytheLLMsatlocal,butonlypossessvariouscommoditydevicesandtheun-reliableinterconnectionbetweendevices.Ex-istingparalleltechniquescannotfullyperforminlimitedenvironment.Theheterogeneityofdevices,coupledwiththeirlimitedcapacityandexpensivecommunication,bringschallengestoprivatedeploymentformaximizedutiliza-tionofavailabledeviceswhilemaskinglatency.Hence,weintroduceHPipe,apipelineinfer-enceframeworkthatsuccessfullymitigatesLLMsfromhigh-performanceclusterstohet-erogeneouscommoditydevices.Byensuringabalanceddistributionofworkloads,HPipefacilitatestheinferencethroughpipeliningthesequencesonthetokendimension.Theeval-uationconductedonLLaMA-7BandGPT3-2BdemonstratesthatHPipeholdsthepotentialforlongcontextanalysisonLLMwithheterogene-itydevices,achievinganimpressivespeedupinlatencyandthroughputupto2.28times.1IntroductionTheemergenceofLLMshassignificantlyenhancedautomatedcontentcomprehension,astheyadeptlycapturesemanticinformationwithinextensivecon-texts.Enterprisesemploytechniquessuchassenti-mentanalysis(Zhangetal.,2023;Dengetal.,2023;Wangetal.,2023)andcontentanalysis(Gubel-mannetal.,2023)toharnessthepotentialvaluetofacilitatetheanticipationofuserengagementandstrategicdecision-making.However,duetothestringentmemoryandcomputationalrequirementsofLLMs,theyarecommonlydeployedonhigh-performancecomputingclusters.Theadvanceddevicesandhigh-velocitytransmissionlikeNV-link,boastingtransferratesapproaching900GB/s,*EqualContribution.†CorrespondingAuthor.enablerapidcomputationandefficientsynchroniza-tion.Whilemicro-enterprisesintroducedemandstoleveragetheprivateLLM,theyonlyhavein-consistentweakerdevices.Theinterconnectionamongthesedevicesalsosuffersfromlimitedband-width.Devicesconnectedviawirelessnetworkex-hibitstransferratemerelyupto1GB/s.Thus,thecustomizedLLMdeploymentschemaformicro-enterprisesdeservesfurtherexploration.Forthedemandsofeffectiveinference,infer-enceengines(Aminabadietal.,2022b;Lietal.,2023)provideshybriddataandpipelineparallelism(Huangetal.,2019;Narayananetal.,2021)andcombinedwithtensorparallelism(Shoeybietal.,2019;Jiaetal.,2019).Inhigh-performancecom-putingcenters,theysubstantiallyalleviatecompu-tationalandmemorypressure,therebyaugmentinginferencespeedandenhancingthroughput.However,existingmethodscannotbedirectlyapplicabletothescenariosofmicro-enterprises.Thedeploymentforthemicro-enterprisespresentsseveralproblems.1)Extendedtext:AsLLMsup-portlongerinputs,theexpandedcontextwindowbringshigherarithmeticpressure.Themicro-batchpipelinestrugglestomaintainefficiency.Eachstageofthepipelinedemandslongerprocessingdurations,andthecoarsergranularitydiminishestheparallelism.2)Communicationdiscrepancy:Theconditionsforcommunicationbetweendevicesarediscrepant.GPUswithinadevicegenerallyex-changedataviaPCIe,andGPUsbetweendevicesrelyonthenetwork.Thisimpedestheefficacyofcommunication-intensivemethodssuchastensorparallelism.3)Heterogeneousdevices:Itisessen-tialthatintegratingheterogeneousdevicestoem-ployallavailableresourcesformicro-enterprises.Thedualheterogeneityofbothcomputationandtransmission,coupledwithexpensivecommunica-tion,bringchallengestoorchestratingtheavailabledevicesofmicro-enterprisesforLLMsdeployment.Toaddressthesechallenges,weproposeHPipe,2</p>
<p>apipelineinferenceframeworkdedicatedtocon-tentcomprehensionforprivateLLMs.ItdeploystheLLMsonheterogeneousdeviceswithpipelineparallelismonthetokendimension.HPipeshieldstheheterogeneityofdevicesbydistributingLLMsbasedoncomputingcapabilitiesandtransmissionconditions.Forextendedcontext,HPipeslicesthemintosegmentsbyadynamicprogrammingal-gorithmandpipelinesthecomputationofsegmentstoamplifythedegreeofparallelism.HPipesuc-cessfullymitigatesLLMsfromhighperformanceclusterstoheterogeneousdevices,achievinguptoa2.28×increaseinbothlatencyandthroughput,alongsidea68.2%reductioninenergyconsump-tioncomparedtoothermethods.2BackgroundandMotivation2.1ParallelismPipelineandtensorparallelismaretwopopularmethodsforacceleratingtheinferenceofLLMsasshowninFig.1.Matrixmultiplication(Mat-Mul)contributestomostoftheoverallcomputa-tionamount.SolvingaMatMulcanbeconvertedintothesolvingsumofseveralsmallerMatMul.Tensorparallelismleveragesthisbydividinganddistributingtheweightmatrixtomultipledevicestoenablethecomputationinparallel.Oncethecomputationcompletes,deviceswillcommunicatetosynchronizetheresults.Thus,tensorparallelismiscommonlyusedwhenthetransmissionisguar-anteed.ThepipelinemechanismdistributesLLMsacrossmultipledevices,witheachdevicededicatedtoastageofcomputation.Therequestisusuallysegmentedintomicro-batchesandprocessedse-quentially.Transmissionisonlyrequiredforin-termediateresult.Whilepipelineiscommunica-tionlightweight,pipelineinbatchdimensionstillbringchallengewhenLLMsareservingformicro-enterprises.Memoryconstraintslimitthebatchsizeofrequests,whichreducesspaceofdividingdataandhindersthedegreeofparallelism.More-over,assequencelengthincreases,eachpipelinestagespendsmoretime.Theincreasingexecutiontimeofstagesintroducesmoreidlewaiting.2.2UtilizationofDevicesAstheemergingdemandsofanalysislongse-quence,thecontextwindowofLLMscontinuestoexpand,occasionallysurpassing8000tokens.Processinglengthysequencesatoncecanoverbur-denthedevices.Conversely,workingwithshortDevice 1Device 2EmbeddingAttenLinear(Matmul)WTopWDownLinear(Matmul)AttenConcat…Replicate synchronizationDevice 3Device 2Device 1Slice Data into micro-batchB1B2B3………Pipeline ParallelismTensor ParallelismWLeftWRightB1B2B3B1B2B1Figure1:Twopopularparallelismapproaches:tensorparallelism(left)andpipelineparallelism(right).025050075010001250150017502000Sequence length (tokens)0255075100FLOPs Utilization (%)LLaMA-7BGPT3-2.7BFigure2:TheFLOPsutilizationforatransformerblockwithdifferentsequencelengthsonRTX3090GPU.sequencesispronetounderutilizingthecomputa-tionalpower.Toexploretherelationshipbetweensequencelengthandresourceutilization,wein-troduceFLOPsutilization,whichreferstothera-tioofactualfloating-pointoperationspersecond(FLOPs)achievedtothemaximumFLOPssup-portedbythehardware.Fig.2showstheresults.Asthesequencelengthexpands,FLOPsutilizationini-tiallyimprovesandundergoesadecreasebeforeconverging.Atfirst,FLOPsutilizationincreasesasmoretokensarefed,leadingtofullutilizationofresources.ThegainsareultimatelyconstrainedbyfrequentI/Ooperations.Thelow-bandwidthmem-oryaccesscausesthebottleneckasthelongerem-beddinginvolves.Wealsofindfluctuationswhenthelengthincrease.GPUsconductMatMulbydi-vidingmatricesintotilestoparallelthemondistinctthreadblocks,whichreferstoagroupofthreadscomputingthesamearithmeticoperations.There-fore,MatMulachievesmaximumGPUutilizationwhenthematrixdimensionsaredivisiblebythetilesize.Otherwise,duetotilequantization(Nvidia),somethreadblocksperformwastedcomputation.Therefore,selectingtheappropriatelengthforev-eryprocesscanincreasedeviceutilization.2.3MotivationOnthebasisofthediscussionabove,pipelinepar-allelismisadvantageousforLLMsinferenceinconstrainedenvironments.Itallowsthereduction3</p>
<p>TransformerblocksL×Network ConditionSpecification of available devicesOnline TextWorkload Partition AlgorithmSequence Slicing AlgorithmDevice 1 Device 3,…,n ……Latency MeasureDevice 2 t1……t4t2t3tn≈≈≈≈≈…Balance WorkloadDistributionStep 2 S1S2S1S2S1S2…Step 3 …S2S3S1S3S2S1S3S1S2S4S2S3S1…S2S3S1S4Step 1 …prepare phase Analyze the following passages as positive or negative “To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer .. Optimal Sequence Slicing(a)subsequencesT41T31T21T11T42T32T22T12T43T33T23T13Device 1 Device 2 Device 3 Time…………runtime phase Device 3Device 2Device 1…S1S2S3S4S4S3……S2…(b)Figure3:HPipeworkflowconsistsoftwophases.Inthepreparephase,HPipedeterminestheoptimalschemaofworkloaddistributionandthesequenceslicingthroughdynamicprogramming.Intherun-timephase,HPipepipelinestheinferenceonthetokendimensionasscheduled.ofmassivecomputationalloadsandonlyincurstol-erantcommunication.Meanwhile,decoder-basedtransformersinherentlyfacilitatepipelineinference.Itenablespipelineonthetokendimensionforlongcontext,whichdoesnotaffecttheresultsasthesub-sequencesarefedinsequentially.TheK,Vvaluesofeachsubsequencearecachedforthecalcula-tionsofsubsequenttokens.Segmentinglengthysentencesintomultiplefragmentsforfinegranular-ityexecutionmaximizesresourceutilization.WeleveragetheseobservationsanddesignHPipe.3Method3.1WorkflowFig.3showstheHPipeworkflow.Takingintoac-countthespecificationsofthedevicesandnetworkconditions,LLMisproperlydistributedacrossmul-tipledevicestomaximizetheutilizationofeachde-viceandavoidheavytransmissionoverhead.HPipepreprocessestheoptimalslicingschemesforinputsofallsupportinglengths.OnceasequenceSar-rives,itisdividedintosubsequencess0,...,smandexecutedsequentiallyacrossdevices.Devicedicanhandlethecomputationtaskforsiinvolvingsi+1andsi−1isprocessingondi−1anddi+1.Thiseffectivelyreconstructsthepipeline,allowingforparallelonthetokendimension.3.2FormulationAssumingthattheLLMiscomposedofnlay-ers{l1,...ln},theyaredividedintoNblocks{b1,...,bN}anddistributedacrossNdevices.Meanwhile,theinputsequencewillbesegmentedintoMsubsequencesinthetokendimension.Weusetijtodenotetheexecutiontimeofeachstageinthepipeline,whichisthecomputationtimeofeachsubsequencesiindevicedjplusthetransmissiontimetothesuccessordj+1.Thecomputationoftheembeddingforsubsequencesconsistsoftwosteps:computingtheinitialembeddingfortokensandcombininginformationfromtheprevioustokenswiththerelevancescores.ThetransmissiontimeisrelatedtothesizeoftheintermediateactivationderivedbythelastlayerljandthebandwidthB.Theexecutiontimetijcanbepresentedas:tij=tc si,i−1Xm=1sm;dj!+tt(lj,si,B).(1)Weusetctodenotethewholecomputationla-tencyforgivensiandtheprevioussubsequencess1,...,si−1,andtttodenotethetransmissiontime.Ourgoalisfindingabalancedworkloadpar-tition{b1,...,bN}andtheproperslicingscheme{s0,...,sM}thatachievesoptimallatencyT∗OtoclosetheidealstateasshowninFig.3.Toimprovetheefficiencyofpipeline,itisessentialtoequalizethestageexecutiontimes.Weestablishaconstrainttoprogressivelyapproachtheoptimalschema:T∗≤maxi∈N(MXj=0tij)+(N−1)max0≤i&lt;M,0≤j&lt;N{tij}.(2)Thefirsttermisthecompleteinferencelatencyontheslowestdevice;Thesecondtermistheover-headbroughtbythepipelineexecution,whichis4</p>
<p>determinedbythesloweststage.Theconstraintallowsustodeterminetheoptimalsolutionbyre-strictingtheupperlimitoflatency.Itisobviousthattheslowestdeviceanddevicetijdominatesthetotallatency.Hence,eliminatingthegapbe-tweendevicesandstageswillfacilitatethepipelineinference.Weequalizethepipelineinferencebydistributionbalanceandsequenceschedule.3.3DistributionBalanceAbalancedmodelpartitionminimizestheimpactofheterogeneitypresentinbothdevicesandtrans-missionconditions.WefirstoptimizethepipelinebydistributingtheLLMstoalignwithcapabili-tiesofdeviceswhileconsideringtransmissionover-head.Wetakelayerasthepartitiongranularityinsteadoftransformerblock,whichprovidestheopportunitytoexploremorebalancedpartition.TheobjectiveofbalancedistributionistofindthetheN−1cutpointstopartitionaLLMintoNsubsets.Eachhasconsecutivelayersandisas-signedtoaspecificdevice.Intheheterogeneousenvironment,thiscanbeestablishedasadeviceplacementproblemandhasbeenprovenasNP-hardin(BenoitandRobert,2008).Toaddressthischallenge,wemaketheassumptionthatthesequenceofdevicesremainsconstant,thatis,theblockbjcorrespondstothedevicedj.SincetheLLMiscomposedofrepeatingblocks,thecon-stantsequenceofdevicesbarelylosestheoptimalsolution,andtheproblemcanbesimplified.Theexecutiontimeforprocessingthelayersfromla+1tolbondevicedmencompassestwocomponents:thecumulativecomputationtimeofthelayersandthecommunicationtimetotransfertheintermediateactivation.Itcanbeobtainedby:T(a,b,m)=bXk=atcomp(lk;dm)+tcomm(lj,m).(3)Fortheoptimalpartition,itcanbebrokenintoanoptimalsub-pipelineconsistingoflayersfroml1throughlkwithm−1devicesfollowedbyasinglestagewithlayerslk+1tolbondevicedm.Usingtheoptimalsub-problemproperty,wecandetermineaplacementschemethatstrivestoequalizetheexecutiontimeamongdevicesinstepwisemanner:A[b][m]=min1≤k&lt;j{max{A[k][m−1],T(k+1,b,m)}},(4)whereA[b][m−1]isthetimetakenbythesloweststageoftheoptimalsub-pipelinefroml1tolbwithformerm−1edgedevices.Algorithm1inAp-pendixA.1showsthepseudocodeofhowweusedynamicprogrammingtoobtainbalancedpartition.3.4SequenceScheduleWiththebalancedworkloaddistribution,theexecu-tiontimeofthesequenceonthedevicesissimilar.Thus,pipelineefficiencynowisdeterminedbythemostexpensivesubsequence.Wefurtherimprovethepipelinebyoptimallyslicingthesequence.Somestudies(Zhengetal.,2023;Lietal.,2021)observedthatexecutingtimeoftokenislinearlyincreaseasthelocationindexgrowssincemorepre-vioustokensinvolvesincomputation.Therefore,anidealslicingshouldincludelongerslicesatthebeginningandshorterslicestowardtheend.Fur-thermore,thegranularityofdividingthesequencealsoisofsignificance,asdiscussedinSection2.2.Employingafiner-grainedslicingapproach,char-acterizedbysmallervaluesof|si|resultsintheun-derutilizationofthecomputationalpowerofGPUs.Incontrast,adoptingacoarserslicingapproach,involvinghighervaluesof|si|,reducesthenumberofpipelinestages,whichdecreasesthedegreeofparallelismandmayoverburdenthedevices.Thus,itisnecessarytofindthemostsuitableslicinggran-ularitytofullyleveragedevices.Thetm=max{tij}isthekeytominimizetheoveralllatency.WeenumeratepossibletmtofindtheoptimalslicingS∗fromslicingspaceS:T∗≤mintm{maxi∈N{minS∗∈S{MXj=0tij|tij≤tm}}+(N−1)tm}.(5)tmrestrictseachslicetohavethesimilarexecu-tiontime,whichleadtominimumpipelinelatency.SincetheoptimizationofsequenceScanderivefromS−sn,weemployadynamicprogrammingalgorithmtoproduceanoptimalslicingschemainallpossibletm.ThedetailsareprovidedinAp-pendixA.2Algorithm2.4Evaluation4.1ExperimentalSetupWeestablishedtheHPipeprototypewithacom-putationalclusteroftwohostmachines.ThefirstmachinecontainsfourPascal100(P100),whilethesecondisfittedwithtwoRTX3090.Communica-tionbetweenhostsisviaawirednetworkwithabandwidthof1000Mbps,andintra-hostcommu-nicationisviaPCIe.Weusethisheterogeneous5</p>
<p>(cid:2)(cid:1)(cid:7)(cid:8)(cid:3)(cid:1)(cid:6)(cid:10)(cid:5)(cid:1)(cid:2)(cid:8)(cid:3)(cid:1)(cid:10)(cid:10)(cid:4)(cid:1)(cid:4)(cid:3)(cid:1)(cid:11)(cid:9)(cid:7)(cid:1)(cid:2)(cid:5)(cid:8)(cid:14)(cid:16)(cid:15)(cid:9)(cid:12)(cid:9)(cid:12)(cid:1)(cid:8)(cid:11)(cid:9)(cid:13)(cid:12)(cid:13)(cid:12)(cid:1)(cid:13)(cid:10)(cid:12)(cid:2)(cid:7)(cid:3)(cid:2)(cid:3)(cid:7)(cid:4)(cid:2)(cid:4)(cid:7)(cid:1)(cid:2)(cid:4)(cid:13)(cid:6)(cid:9)(cid:5)(cid:15)(cid:1)(cid:3)(cid:8)(cid:12)(cid:10)(cid:14)(cid:7)(cid:8)(cid:11)(cid:14)(cid:13)(cid:6)(cid:8)(cid:19)(cid:10)(cid:14)(cid:9)(cid:21)(cid:1)(cid:2)(cid:18)(cid:3)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:7)(cid:12)(cid:17)(cid:15)(cid:20)(cid:11)(cid:12)(cid:16)(cid:20)(cid:19)(cid:1)(cid:2)(cid:5)(cid:1)(cid:19)(cid:15)(cid:13)(cid:10)(cid:14)(cid:18)(cid:4)(cid:18)(cid:3)(a)(cid:3)(cid:1)(cid:8)(cid:2)(cid:5)(cid:1)(cid:7)(cid:2)(cid:3)(cid:1)(cid:5)(cid:7)(cid:4)(cid:1)(cid:4)(cid:6)(cid:3)(cid:1)(cid:8)(cid:3)(cid:8)(cid:1)(cid:7)(cid:3)(cid:11)(cid:17)(cid:19)(cid:18)(cid:12)(cid:15)(cid:12)(cid:15)(cid:1)(cid:11)(cid:14)(cid:12)(cid:16)(cid:15)(cid:16)(cid:15)(cid:1)(cid:16)(cid:13)(cid:15)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:1)(cid:2)(cid:4)(cid:13)(cid:6)(cid:9)(cid:5)(cid:15)(cid:1)(cid:3)(cid:8)(cid:12)(cid:10)(cid:14)(cid:7)(cid:8)(cid:11)(cid:14)(cid:13)(cid:6)(cid:8)(cid:19)(cid:10)(cid:14)(cid:9)(cid:21)(cid:1)(cid:2)(cid:18)(cid:3)(cid:6)(cid:8)(cid:10)(cid:3)(cid:2)(cid:1)(cid:7)(cid:12)(cid:17)(cid:15)(cid:20)(cid:11)(cid:12)(cid:16)(cid:20)(cid:19)(cid:1)(cid:2)(cid:5)(cid:1)(cid:19)(cid:15)(cid:13)(cid:10)(cid:14)(cid:18)(cid:4)(cid:18)(cid:3)(b)Figure4:Thelatencyandthroughputofdifferentap-proachontheLLaMA-7B(left)andGPT3-2B(right).clustertomimicacommodityhardwaresetup.WeevaluateHPipeonGPT3-2B,LLaMA-7B.Thelengthoftheinputsequenceissetas2048tokenstosimu-latecontentanalysisforlongsequence.ThebatchsizeofGPT3-2BandLLaMA-7Baresetas12and6.4.2PerformanceWecompareHPipe(HP)withthefollowingmethod(1)Base:LLMisuniformlydistributedacrosseachGPU,andinferenceisperformedsequentiallyacrossthecluster.(2)GPipe(GP)(Huangetal.,2019):EvenlydistributetheLLMacrossGPUandpipelinetheinferencewithmicro-batch(3)GP-B:GPipewiththeworkloaddistributionproposedbyHPipe.(4)Megatron-LM(MG)(Shoeybietal.,2019):combinetensorparallelismwithGPipe(5)Terapipe(Lietal.,2021):EvenlydistributetheLLMacrossGPUandpipelinetheinferenceonthetokendimension.(6)TP-T:CombinetensorparallelismwithTeraPipe.4.2.1LatencyandThroughputFig.4presentsthelatencyandthroughputofdif-ferentmethods.HarnessingmultipledevicesforparallelismallowsefficientLLMinference.OnLLaMA-7B,HPmarkedlyreduceslatencyto2.24s,achievingaspeedupof9.06×comparedtoBase.Italsoincreasesthethroughputfrom0.56kto5.03ktokens/s,greatlyimprovingtheefficiency.GPpipelinesinferenceinmicro-batch.Thecoarsegranularityofparallelremainsroomforoptimiza-tion.MGintroducestensorparallelismtosharethecomputationbutislimitedtothetransmissioncost.Whilesmallvolumesofsynchronizeddataenableaccelerationthroughtensorparallelism,largervol-umessufferfromsignificanttransmissionoverhead,therebyimpedingperformance.Withabalancedworkloaddistribution,GP-BandHPdemonstratethelatencyreductionof51~56%andthethrough-putenhancementof2.06~2.28×.Theseimprove-(cid:9)(cid:15)(cid:17)(cid:16)(cid:10)(cid:13)(cid:10)(cid:13)(cid:1)(cid:9)(cid:12)(cid:10)(cid:14)(cid:13)(cid:14)(cid:13)(cid:1)(cid:14)(cid:11)(cid:13)(cid:2)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:3)(cid:6)(cid:4)(cid:9)(cid:7)(cid:10)(cid:8)(cid:11)(cid:1)(cid:2)(cid:6)(cid:5)(cid:3)(a)(cid:9)(cid:15)(cid:17)(cid:16)(cid:10)(cid:13)(cid:10)(cid:13)(cid:1)(cid:9)(cid:12)(cid:10)(cid:14)(cid:13)(cid:14)(cid:13)(cid:1)(cid:14)(cid:11)(cid:13)(cid:3)(cid:2)(cid:3)(cid:4)(cid:2)(cid:7)(cid:5)(cid:2)(cid:3)(cid:6)(cid:2)(cid:7)(cid:8)(cid:2)(cid:3)(cid:4)(cid:9)(cid:7)(cid:10)(cid:8)(cid:11)(cid:1)(cid:2)(cid:6)(cid:5)(cid:3)(b)Figure5:TheEnergyconsumptionofclusterduringinferenceontheLLaMA-7B(left)andGPT3-2B.(right)mentsareattributedtojudiciouslymanagingthecomputingresourcesofthecluster.Whatismore,pipeliningonthetokendimensionfurtherexpeditestheinference,aresultofthesmallerexecutiongran-ularityachievedbyHPipe.Itfacilitateshigherpar-allelismdegree,minimizesdeviceidletime,andop-timizesdeviceutilizationduringinference,leadingtolatencyreductionby33.1~39.3%.ComparsionofTPandTP-Tshowstensorparallelismisnotsuit-abletocombinewithpipelineontokendimension.Thisisbecauseslicingtokensintofine-granularitysegmentsintroducesmorefrequentsynchroniza-tion,whichcausesadditionaloverhead.4.2.2EnergyConsumptionEnergyconsumptionisanimportantmetricofinfer-enceperformance.Fig.5showstheleastdynamicenergyconsumptionthatHPipetakes.Theopti-mizationofGP,MGandTPdoesnotconsiderthepowercharacteristicsofdifferenttypesofdevicessothattheworkloadisprocessedinanenergy-lavishmanner.Incontrast,byjointlyoptimizingthetrade-offbetweencomputationandcommunica-tionprovideddevices’computingcapabilitiesandnetworkconditions,HPipeachievesthelowesten-ergycosts.ItcomesthatHPipefindsthesequencelengththatapproximatesthemaximumutilizationofclusterexecutionthroughatwo-stepoptimiza-tion.Theinferenceisexecutedunderhighresourceutilization,thusreflectinglessenergyconsumption.4.2.3MemoryFootprintWerecordthememoryfootprintofdevicesasshowninTable1.TensorParallelismcanreducethememorypressurebydistributingtheweight.MeanWhile,withbalancedworkloaddistribution,LLMsareapportionedamongmachinesaccordingtotheircomputingcapabilities,therebymitigatingthememoryburdenpermachineastheincreaseddevices.WealsofindthatthememoryofP@4andR@1isrelativelylowercomparedtopeerde-6</p>
<p>Table1:Memoryfootprintofdifferentmethodsdur-inginferenceondevices.OOMmeansdeviceisoutofmemoryduringtheruntime.PdenotesP100andRdenotesRTX3090ModelMethodsMemoryfootprints(MB)P@1P@2P@3P@4R@1R@2LLaMA-7BBase114791147911019110191146111461GP703170316593659355095509GP-B2897313536553031969110739MG585158515493549359435943TP545954594505450549574957TP-P486948694583458350135013HP1873297731431991871310087GPT3-2BBaseOOMOOMOOMOOM--GP703170316593659355095509GP-B366535053495317785258627MG469546954595459550575043TP660166016629662966816681TP-P495249525032503254335437HP469346513153295397579855vices.Thisdisparityisattributedtotheinclusionoftheheterogeneouscommunicationenvironment.Deviceswithhighercommunicationoverheadareallocatedfewerlayerstooffsettheincreasedbur-denofcommunication,whichisreflectedinthememorywithfewerparameters.4.3ResourceUtilizationToaffirmHPipeinleveragingcomputationalre-sources,wevisualizetheinferencesinFig.6,whicharemeasuredonLLaMA-7Bandbatchsizeissetas1.Fig.6ashowstheresultofequaldistri-butionoftheLLM,alongwiththeevenlyslicingofsequences.RTX3090exhibitsatinyexecutiontimecomparedtoP100,ascribedtoLLMdistribu-tionfailingtofullyharnessthedevice’scapabili-ties.RTX3090rapidlycompletesthecomputationtaskofeachsubsequenceandfallsintoawaitingstateforthenextsubsequence.Asignificantpor-tionofthecomputationalresourcesremainidle.Moreover,uniformslicingsequencesleadtolongerexecutiontimesforsubsequentsubsequences,caus-ingabottleneckinthepipelineefficiency.Fig.6bdemonstratesthatHPipeschedulestheexecu-tionofsubsequences.Computationallypowerfuldevicesareburdenedwithheaviercomputationaltasks,whichgivesanapproximateexecutiontimeforeachsubsequence.Meanwhile,increasinglyshortersubsequencesbalancethepipeline.5RelatedWorkParallelaccelerationondeepneuralnetworkshasbeenwidelystudied.Onlyusingthedataparal-lelism(Houetal.,2022;Zhangetal.,2021;Maetal.,2023)isnotenoughasparametersofLLMsexpand.Pipelineparallelism(Huangetal.,2019;��������������������������������������������������������������������������������(a)���������������������������������������������������������������������������������������������������(b)Figure6:TheperformanceofthepipelineinferencewithorwithoutHPipe.Distinctcoloredblocksrep-resenttheexecutiontimeofsubsequences.Thegapsbetweenblocksarethecommunicationtimefortrans-ferringintermediateactivation.Aminabadietal.,2022a;Lietal.,2021)andten-sorparallelism(Shoeybietal.,2019;Bianetal.,2021)distributethemodeltomultipleGPUs,thusreducingthememoryburdenofthedeviceandallowingefficientscalingofLLMinference.Onthebasisofthem,lotsofworkachieveinferencespeedup.Byte-Transformer(Zhaietal.,2023)pro-posesapadding-freealgorithmthatliberatesinfer-encefromredundantcomputationsonzeropaddedtokenswhenfacedwithvariable-lengthsequences.Kernelfusion(Choietal.,2022;Daoetal.,2022)optimizedCUDAkernelstoreducememoryaccessandimprovecomputationspeed.Thesemethodsfocusonlatency-orientedscenarioswithadvanceddevices,limitingtheirdeploymenttoeasilyacces-siblehardwarewithweakercomputingcapabilityandmemorystorage.Incomparison,thispaperderivestheparallelismschemaonaheterogeneousclusterofcommoditydevicestocatertotheprivateapplicationrequirements.Inaddition,techniquesproposedbyHPipeareorthogonaltotheoptimizedmethods,includingquantization(Dettmersetal.,2022)andkerneloptimization(Lietal.,2022),hencetheycanbecombinedwiththemforbetterperformance.6ConclusionThispaperintroducesHPipe,aninferenceframe-worktoacceleratethecontentanalysiswithLLMsprototypedontheclusterofcommoditydevices.Iteffectivelyintegratescomputingresources,allow-ingafine-granularitypipelineonheterogeneousdevices.HPipedemonstratesthepotentialtoac-celerateLLMsinferencewithlongsequenceinput,offeringasolutionforLLMsdeploymentinhetero-7</p>
<p>geneouscommodityhardwareenvironments.7AcknowledgementsThisworkwassupportedbytheNationalNat-uralScienceFoundationofChinaunderGrants(62201072,62101064,62171057,U23B2001,62001054,62071067),theMinistryofEducationandChinaMobileJointFund(MCM20200202,MCM20180101)ReferencesRezaYazdaniAminabadi,SamyamRajbhandari,Am-marAhmadAwan,ChengLi,DuLi,EltonZheng,OlatunjiRuwase,ShadenSmith,MinjiaZhang,JeffRasley,andYuxiongHe.2022a.Deepspeed-in-ference:Enablingefficientinferenceoftransformermodelsatunprecedentedscale.InSC22:Interna-tionalConferenceforHighPerformanceComputing,Networking,StorageandAnalysis.RezaYazdaniAminabadi,SamyamRajbhandari,Am-marAhmadAwan,ChengLi,DuLi,EltonZheng,OlatunjiRuwase,ShadenSmith,MinjiaZhang,JeffRasley,etal.2022b.Deepspeed-inference:enablingefficientinferenceoftransformermodelsatunprece-dentedscale.InSC22:InternationalConferenceforHighPerformanceComputing,Networking,StorageandAnalysis,pages1–15.IEEE.AnneBenoitandYvesRobert.2008.Mappingpipelineskeletonsontoheterogeneousplatforms.JournalofParallelandDistributedComputing,pages790–808.ZhengdaBian,HongxinLiu,BoxiangWang,HaichenHuang,YongbinLi,ChuanruiWang,FanCui,andYangYou.2021.Colossal-ai:Aunifieddeeplearningsystemforlarge-scaleparalleltraining.CoRR.JaewanChoi,HailongLi,ByeonghoKim,SeunghwanHwang,andJungHoAhn.2022.Acceleratingtrans-formernetworksthroughrecomposingsoftmaxlay-ers.In2022IEEEInternationalSymposiumonWork-loadCharacterization(IISWC),pages92–103.TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé.2022.Flashattention:Fastandmemory-efficientexactattentionwithio-awareness.AdvancesinNeuralInformationProcessingSystems,pages16344–16359.XiangDeng,VasilisaBashlovkina,FengHan,SimonBaumgartner,andMichaelBendersky.2023.Llmstothemoon?redditmarketsentimentanalysiswithlargelanguagemodels.InCompanionProceed-ingsoftheACMWebConference2023,WWW2023,Austin,TX,USA,30April2023-4May2023,pages1014–1019.TimDettmers,MikeLewis,YounesBelkada,andLukeZettlemoyer.2022.Llm.int8():8-bitmatrixmul-tiplicationfortransformersatscale.arXivpreprintarXiv:2208.07339.RetoGubelmann,Aikaterini-LidaKalouli,ChristinaNiklaus,andSiegfriedHandschuh.2023.Whentruthmatters-addressingpragmaticcategoriesinnaturallanguageinference(NLI)bylargelanguagemod-els(llms).InProceedingsoftheThe12thJointConferenceonLexicalandComputationalSeman-tics,*SEM@ACL2023,Toronto,Canada,July13-14,2023.XueyuHou,YongjieGuan,TaoHan,andNingZhang.2022.Distredge:Speedingupconvolutionalneuralnetworkinferenceondistributededgedevices.In2022IEEEInternationalParallelandDistributedProcessingSymposium(IPDPS),pages1097–1107.YanpingHuang,YoulongCheng,AnkurBapna,OrhanFirat,DehaoChen,MiaChen,HyoukJoongLee,Ji-quanNgiam,QuocVLe,YonghuiWu,etal.2019.Gpipe:Efficienttrainingofgiantneuralnetworksusingpipelineparallelism.Advancesinneuralinfor-mationprocessingsystems,32.ZhihaoJia,MateiZaharia,andAlexAiken.2019.Be-yonddataandmodelparallelismfordeepneuralnet-works.ProceedingsofMachineLearningandSys-tems,1:1–13.GongzhengLi,YadongXi,JingzhenDing,DuanWang,ZiyangLuo,RongshengZhang,BaiLiu,ChangjieFan,XiaoxiMao,andZengZhao.2022.Easyandefficienttransformer:ScalableinferencesolutionforlargeNLPmodel.InProceedingsofthe2022Con-ferenceoftheNorthAmericanChapteroftheAsso-ciationforComputationalLinguistics:HumanLan-guageTechnologies:IndustryTrack,NAACL2022,Hybrid:Seattle,Washington,USA+Online,July10-15,2022.ZhuohanLi,LianminZheng,YinminZhong,Vin-centLiu,YingSheng,XinJin,YanpingHuang,ZhifengChen,HaoZhang,JosephEGonzalez,etal.2023.{AlpaServe}:Statisticalmultiplexingwithmodelparallelismfordeeplearningserving.In17thUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI23),pages663–679.ZhuohanLi,SiyuanZhuang,ShiyuanGuo,DanyangZhuo,HaoZhang,DawnSong,andIonStoica.2021.Terapipe:Token-levelpipelineparallelismfortrain-inglarge-scalelanguagemodels.InInternationalConferenceonMachineLearning,pages6543–6552.RuilongMa,XiangYang,QiQi,JingyuWang,ZiruiZhuang,JingWang,andXinWang.2023.Briefannouncement:Acceleratecnninferencewithzoninggraphatdynamicgranularity.InProceedingsofthe35thACMSymposiumonParallelisminAlgorithmsandArchitectures,pages295–298.DeepakNarayanan,MohammadShoeybi,JaredCasper,PatrickLeGresley,MostofaPatwary,VijayKor-thikanti,DmitriVainbrand,PrethviKashinkunti,JulieBernauer,BryanCatanzaro,etal.2021.Ef-ficientlarge-scalelanguagemodeltrainingongpuclustersusingmegatron-lm.InProceedingsofthe8</p>
<p>InternationalConferenceforHighPerformanceCom-puting,Networking,StorageandAnalysis,pages1–15.Nvidia.Matrixmultiplicationbackgrounduser’sguide.docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html.MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-zaro.2019.Megatron-lm:Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.arXivpreprintarXiv:1909.08053.ZengzhiWang,QimingXie,ZixiangDing,YiFeng,andRuiXia.2023.Ischatgptagoodsentimentanalyzer?Apreliminarystudy.CoRR.YujiaZhai,ChengquanJiang,LeyuanWang,XiaoyingJia,ShangZhang,ZizhongChen,XinLiu,andYiboZhu.2023.Bytetransformer:Ahigh-performancetransformerboostedforvariable-lengthinputs.In2023IEEEInternationalParallelandDistributedProcessingSymposium(IPDPS),pages344–355.ShuaiZhang,ShengZhang,ZhuzhongQian,JieWu,YiboJin,andSangluLu.2021.Deepslicing:collab-orativeandadaptivecnninferencewithlowlatency.IEEETransactionsonParallelandDistributedSys-tems,pages2175–2187.WenxuanZhang,YueDeng,BingLiu,SinnoJialinPan,andLidongBing.2023.Sentimentanalysisintheeraoflargelanguagemodels:Arealitycheck.CoRR,abs/2305.15005.ZangweiZheng,XiaozheRen,FuzhaoXue,YangLuo,XinJiang,andYangYou.2023.Responselengthperceptionandsequencescheduling:Anllm-empoweredllminferencepipeline.arXivpreprintarXiv:2305.13144.AAppendixA.1WorkloaddistributionAlgorithmAlgorithm1showsthepseudocodeofbalanceworkloaddistributiontoshieldtheheterogeneityofcluster.Line1-2initializestheexecutiontimeofdifferentnumbersoflayersassignedtothefirstdevice.Line3-5outlinesthedynamicprogram-mingapproachforbalancedworkloaddistribution.A[N][j]recordthetheminimumexecutiontimeofthestagesthatassignthefirstNlayerstothefirstjlayers,whichisdeterminedbythelesseras-signmentofthefirstklayersofthemodeltothefirstn−1devicesandthek+1tomlayerstothedevicen.Thecut-offpointsarerecordedinpi.Line6-9derivestheworkloaddistributionschemaaccordingtothecutpoints.Algorithm1WorkloaddistributionInput:Computationandcommunicationtimeperlayerofeachdevice.Output:MinimalslowestexecutiontimeA[N][M]andcorrespondingworkloaddistri-butionschema.1:forifrom1toNdo2:calculateA[i][1]using(3)3:forjfrom2toMdo4:A[N][j]←min1≤k≤N{max{A[k][j−1],T(k+1,N,j)}}5:pi←argmin1≤k≤N{max{A[k][j−1],T(k+1,N,j)}}▷Dynamicprogrammingforthebalanceworkloaddistribution6:i←N,p←{}7:whilei&gt;0do8:p.append(pi)9:i←i−pi▷Derivetheworkloaddistributionscheme(cid:3)(cid:2)(cid:7)(cid:3)(cid:5)(cid:4)(cid:3)(cid:7)(cid:5)(cid:9)(cid:7)(cid:2)(cid:3)(cid:8)(cid:10)(cid:10)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:4)(cid:5)(cid:10)(cid:7)(cid:8)(cid:6)(cid:11)(cid:1)(cid:2)(cid:9)(cid:3)(cid:2)(cid:14)(cid:10)(cid:4)(cid:6)(cid:12)(cid:13)(cid:1)(cid:11)(cid:7)(cid:1)(cid:3)(cid:9)(cid:8)(cid:5)(cid:6)(cid:13)(a)(cid:3)(cid:10)(cid:3)(cid:8)(cid:5)(cid:4)(cid:8)(cid:6)(cid:11)(cid:8)(cid:3)(cid:4)(cid:10)(cid:12)(cid:12)(cid:2)(cid:1)(cid:2)(cid:3)(cid:1)(cid:7)(cid:5)(cid:1)(cid:2)(cid:6)(cid:1)(cid:7)(cid:8)(cid:1)(cid:2)(cid:9)(cid:1)(cid:7)(cid:4)(cid:5)(cid:10)(cid:7)(cid:8)(cid:6)(cid:11)(cid:1)(cid:2)(cid:9)(cid:3)(cid:2)(cid:14)(cid:10)(cid:4)(cid:6)(cid:12)(cid:13)(cid:1)(cid:11)(cid:7)(cid:1)(cid:3)(cid:9)(cid:8)(cid:5)(cid:6)(cid:13)(b)Figure7:Thelatencyofthepipelineinferencewithuniformslicefrom1to128inthetokendimensionandthesequenceschedule(SS).(a)GPT3-2B(b)LLaMA-7BA.2SequenceSlicingAlgorithmAlgorithm2showsthedetailofsequenceslicing.Line4-13showstheiterationthatfindstheopti-malslicingwithtmax.Eachtimewesliceasub-sequenceinthefrontandtreattheremainingse-quenceasanewsequenceuntilthesequenceisdivided.Theleastlatencyofasequencewithdif-ferentlengthsisstoredinL[scur]andthelengthofthejustsegmentedsubsequenceisstoredinS[scur].Line16-19derivestheoptimalsequenceslicingbasedontherecordinS.Line20-22getstheop-timalslicingschemeamongtheenumerationofdifferenttmax.A.3DynamicSequenceScheduleWeconductanablationstudyonthedynamicse-quenceschedule(SS)introducedinSection3.4.To9</p>
<p>Algorithm2SequenceslicingInput:Themaximumexecutiontimeofslicestmax,executiontimeforslicesofdifferentlengthsG.ArraystorecordthelatencyandtracethesequenceslicingL,SOutput:Theoptimalslicing{s0,...,sn}1:T←allpossiblelatencyinG2:T∗←∞,S∗←None3:fortmaxinTdo4:forscurfrom1toNdo5:L[scur]←∞6:forsstepfrom1toscurdo7:lstep←G[scur][scur−sstep]8:ltotal←L[scur−sstep]+lstep9:ifscur≤tmax&amp;&amp;ltotal<L[scur]then10:L[scur]←ltotal11:S[scur]←sstep▷Dynamicprogrammingfortheoptimalslicingunderthetmax12:i←|S|,S←{}13:whilei>0do14:S.append(S[i))15:i←i−S[i]▷Derivethesequenceslicing16:T=(M−1)∗tmax+L[N]17:ifT&lt;T∗then18:T∗←T,S∗←S▷SelecttheoptimalschemaS∗contrasttheinferencelatencyoftheslicingschemedeterminedbythesequenceschedulewiththatofaheuristicthatslicestheinputsequenceuniformly,wetestedboththeGPT3-2BandLLaMA-7Bmodelsusingasequencelengthof2048tokens.Thebatchsizesweresetat12and6,respectively.Intheuni-formslicingapproach,theentireinputwasslicedonthetokendimension,withthenumberofslicesrangingfrom1to128.Wemeasuredtheinferencelatencyforeachslicingconfiguration.ThefindingsareillustratedinFig.7andalignwithourhypothe-ses.PipelineswithfinegranularitysufferfromGPUunderutilization,whereasthosewithcoarsergranularitypresentlargepipelinebubbles,culmi-natinginincreasedinferencelatency.Moreover,duetothemaskmechanismofthedecoder-basedtransformer,theuniformslicehidesthediscrep-ancyincomputationalvolumebetweenfrontandrearsubsequences.HPipewithapropersequencescheduleoutperformsthebestuniformslicingcon-figuration.</p>