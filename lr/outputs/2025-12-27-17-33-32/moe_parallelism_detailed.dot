digraph {
	graph [bb="0,0,1616,4378.5",
		bgcolor=white,
		rankdir=TB,
		size="30,40"
	];
	node [fillcolor=lightyellow,
		fontname=Arial,
		fontsize=10,
		label="\N",
		shape=parallelogram,
		style=filled
	];
	subgraph cluster_layer1 {
		graph [bb="8,3327.4,1608,4297",
			fillcolor=lightgray,
			label="Layer 1 (PP Stage 1)",
			lheight=0.21,
			lp="808,4285.5",
			lwidth=2.06,
			style="rounded,filled"
		];
		sp_split_1	[height=1.4444,
			label="SP Split\nGPU: 0-1\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="825,4214",
			width=6.8253];
		attn_q_1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Q Projection\nGPU: 0\nTP Rank 0\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="188,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_q_1	[pos="e,306.84,4124 593.2,4176.6 508.94,4162.5 412.86,4145.6 316.74,4126.1"];
		attn_q_1_tp1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Q Projection\nGPU: 1\nTP Rank 1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="953,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_q_1_tp1	[pos="e,919.51,4126.2 880.77,4161.8 891.29,4152.1 902.12,4142.2 912.08,4133"];
		attn_k_1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention K Projection\nGPU: 0\nTP Rank 0\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="443,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_k_1	[pos="e,542.17,4126 659.12,4161.9 622.83,4150.8 585.31,4139.2 551.93,4129"];
		attn_k_1_tp1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention K Projection\nGPU: 1\nTP Rank 1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="1208,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_k_1_tp1	[pos="e,1108.5,4126 975.27,4166.9 1016.5,4154.2 1060.4,4140.7 1098.9,4128.9"];
		attn_v_1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention V Projection\nGPU: 0\nTP Rank 0\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="698,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_v_1	[pos="e,731.23,4126.2 769.67,4161.8 759.23,4152.1 748.49,4142.2 738.6,4133"];
		attn_v_1_tp1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention V Projection\nGPU: 1\nTP Rank 1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="1463,4094.5",
			shape=rectangle,
			width=3.2917];
		sp_split_1 -> attn_v_1_tp1	[pos="e,1344.4,4124.2 994.7,4187 1091.7,4171.4 1215.7,4150.4 1334.4,4126.2"];
		attn_comp_1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Computation\nGPU: 0\nTP Rank 0\nInput: [batch_size=128, seq_len=5120, heads=16, d_k=32]\nOutput: [batch_size=128, seq_\
len=5120, dim=256]",
			pos="664,3995.5",
			shape=rectangle,
			width=3.7917];
		attn_q_1 -> attn_comp_1	[pos="e,527.36,4021.1 306.51,4065 309.71,4064.3 312.87,4063.6 316,4063 381.94,4048.8 455.22,4034.6 517.47,4022.9"];
		attn_comp_1_tp1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Computation\nGPU: 1\nTP Rank 1\nInput: [batch_size=128, seq_len=5120, heads=16, d_k=32]\nOutput: [batch_size=128, seq_\
len=5120, dim=256]",
			pos="1073,3995.5",
			shape=rectangle,
			width=3.7917];
		attn_q_1_tp1 -> attn_comp_1_tp1	[pos="e,1034.9,4027.3 990.97,4062.8 1002.3,4053.6 1015,4043.4 1026.8,4033.8"];
		attn_k_1 -> attn_comp_1	[pos="e,594.35,4027 512.62,4062.9 535.63,4052.8 561.39,4041.5 585.05,4031.1"];
		attn_k_1_tp1 -> attn_comp_1_tp1	[pos="e,1115.9,4027.3 1165.3,4062.8 1152.2,4053.4 1137.7,4043 1124.2,4033.2"];
		attn_v_1 -> attn_comp_1	[pos="e,674.8,4027.3 687.24,4062.8 684.36,4054.5 681.2,4045.5 678.16,4036.8"];
		attn_v_1_tp1 -> attn_comp_1_tp1	[pos="e,1195.8,4027 1344.4,4064 1300.6,4053.1 1250.6,4040.6 1205.6,4029.4"];
		attn_out_1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Output Proj\nGPU: 0\nTP Rank 0\nInput: [batch_size=128, seq_len=5120, dim=256]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="707,3896.5",
			shape=rectangle,
			width=3.2917];
		attn_comp_1 -> attn_out_1	[pos="e,693.34,3928.3 677.61,3963.8 681.29,3955.4 685.34,3946.3 689.22,3937.6"];
		attn_out_1_tp1	[fillcolor=lightgreen,
			height=0.875,
			label="Attention Output Proj\nGPU: 1\nTP Rank 1\nInput: [batch_size=128, seq_len=5120, dim=256]\nOutput: [batch_size=128, seq_len=5120, \
dim=256]",
			pos="1005,3896.5",
			shape=rectangle,
			width=3.2917];
		attn_comp_1_tp1 -> attn_out_1_tp1	[pos="e,1026.6,3928.3 1051.5,3963.8 1045.5,3955.2 1038.8,3945.7 1032.5,3936.7"];
		attn_allreduce_1	[fillcolor=lightblue,
			height=1.0214,
			label="TP All-Reduce\nGPU: 0-1\nInput: [batch_size=128, seq_len=5120, dim=256]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="971,3792.2",
			shape=ellipse,
			width=4.6551];
		attn_out_1 -> attn_allreduce_1	[pos="e,890.27,3824.5 785.85,3864.9 815.73,3853.3 849.97,3840.1 880.82,3828.1"];
		attn_out_1_tp1 -> attn_allreduce_1	[pos="e,982.87,3828.9 994.8,3864.8 992.04,3856.5 989,3847.3 986.03,3838.4"];
		gate_1	[height=1.4444,
			label="Expert Gate\nGPU: 0-1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, num_experts=16]",
			pos="298,3667.4",
			width=7.8332];
		attn_allreduce_1 -> gate_1	[pos="e,577.8,3719.5 844.02,3768 771.51,3754.8 677.43,3737.6 587.86,3721.3",
			style=dashed];
		expert_0_1	[fillcolor=lightgreen,
			height=0.875,
			label="Expert 0\nGPU: 2\nEP Rank 0\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="716,3667.4",
			shape=rectangle,
			width=3.2917];
		attn_allreduce_1 -> expert_0_1	[pos="e,779.43,3699 903,3758.4 867.79,3741.5 824.77,3720.8 788.87,3703.5"];
		expert_1_1	[fillcolor=lightgreen,
			height=0.875,
			label="Expert 1\nGPU: 3\nEP Rank 1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="971,3667.4",
			shape=rectangle,
			width=3.2917];
		attn_allreduce_1 -> expert_1_1	[pos="e,971,3699.1 971,3755.3 971,3740.9 971,3724 971,3709.1"];
		expert_2_1	[fillcolor=lightgreen,
			height=0.875,
			label="Expert 2\nGPU: 4\nEP Rank 2\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="1226,3667.4",
			shape=rectangle,
			width=3.2917];
		attn_allreduce_1 -> expert_2_1	[pos="e,1162.6,3699 1039,3758.4 1074.2,3741.5 1117.2,3720.8 1153.1,3703.5"];
		expert_3_1	[fillcolor=lightgreen,
			height=0.875,
			label="Expert 3\nGPU: 5\nEP Rank 3\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="1481,3667.4",
			shape=rectangle,
			width=3.2917];
		attn_allreduce_1 -> expert_3_1	[pos="e,1412,3699 1110.8,3771.8 1184.1,3759.9 1274.8,3742.5 1354,3719.4 1369.9,3714.8 1386.5,3708.9 1402.3,3702.8"];
		expert_agg_1	[height=1.4444,
			label="Expert Aggregation\nGPU: 0-1\nInput: [batch_size=128, seq_len=5120, dim=512] x 16\nOutput: [batch_size=128, seq_len=5120, dim=512]",
			pos="1098,3527.4",
			width=7.2285];
		expert_0_1 -> expert_agg_1	[pos="e,945.12,3579.4 790.89,3635.9 808.26,3629 826.73,3621.8 844,3615.4 873.39,3604.5 904.85,3593.4 935.28,3582.8"];
		expert_1_1 -> expert_agg_1	[pos="e,1050.9,3579.6 999.19,3635.8 1012.4,3621.5 1028.5,3603.9 1043.9,3587.2"];
		expert_2_1 -> expert_agg_1	[pos="e,1145.4,3579.6 1197.6,3635.8 1184.3,3621.5 1168,3603.9 1152.5,3587.2"];
		expert_3_1 -> expert_agg_1	[pos="e,1252.8,3579.5 1406.7,3635.8 1389.5,3628.9 1371.2,3621.8 1354,3615.4 1324.5,3604.5 1292.9,3593.3 1262.3,3582.8"];
		sp_merge_1	[height=1.4444,
			label="SP Merge\nGPU: 0-1\nInput: [batch_size=128, seq_len=5120, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,3387.4",
			width=6.9693];
		expert_agg_1 -> sp_merge_1	[pos="e,1098,3439.8 1098,3475 1098,3466.9 1098,3458.3 1098,3450"];
	}
	subgraph cluster_layer2 {
		graph [bb="969,3111.3,1227,3202.3",
			fillcolor=lightgray,
			label="Layer 2 (PP Stage 2)",
			lheight=0.21,
			lp="1098,3190.8",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_2_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 2 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,3145.3",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer3 {
		graph [bb="969,2895.2,1227,2986.2",
			fillcolor=lightgray,
			label="Layer 3 (PP Stage 3)",
			lheight=0.21,
			lp="1098,2974.7",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_3_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 3 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,2929.2",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer4 {
		graph [bb="969,2679.1,1227,2770.1",
			fillcolor=lightgray,
			label="Layer 4 (PP Stage 4)",
			lheight=0.21,
			lp="1098,2758.6",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_4_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 4 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,2713.1",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer5 {
		graph [bb="969,2463,1227,2554",
			fillcolor=lightgray,
			label="Layer 5 (PP Stage 5)",
			lheight=0.21,
			lp="1098,2542.5",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_5_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 5 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,2497",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer6 {
		graph [bb="969,2246.9,1227,2337.9",
			fillcolor=lightgray,
			label="Layer 6 (PP Stage 6)",
			lheight=0.21,
			lp="1098,2326.4",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_6_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 6 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,2280.9",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer7 {
		graph [bb="969,2030.8,1227,2121.8",
			fillcolor=lightgray,
			label="Layer 7 (PP Stage 7)",
			lheight=0.21,
			lp="1098,2110.3",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_7_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 7 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,2064.8",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer8 {
		graph [bb="969,1814.7,1227,1905.7",
			fillcolor=lightgray,
			label="Layer 8 (PP Stage 8)",
			lheight=0.21,
			lp="1098,1894.2",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_8_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 8 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,1848.7",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer9 {
		graph [bb="969,1598.7,1227,1689.7",
			fillcolor=lightgray,
			label="Layer 9 (PP Stage 9)",
			lheight=0.21,
			lp="1098,1678.2",
			lwidth=2.06,
			style="rounded,filled"
		];
		layer_9_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 9 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,1632.7",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer10 {
		graph [bb="969,1382.6,1227,1473.6",
			fillcolor=lightgray,
			label="Layer 10 (PP Stage 10)",
			lheight=0.21,
			lp="1098,1462.1",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_10_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 10 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,1416.6",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer11 {
		graph [bb="969,1166.5,1227,1257.5",
			fillcolor=lightgray,
			label="Layer 11 (PP Stage 11)",
			lheight=0.21,
			lp="1098,1246",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_11_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 11 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,1200.5",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer12 {
		graph [bb="969,950.36,1227,1041.4",
			fillcolor=lightgray,
			label="Layer 12 (PP Stage 12)",
			lheight=0.21,
			lp="1098,1029.9",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_12_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 12 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,984.36",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer13 {
		graph [bb="969,734.27,1227,825.27",
			fillcolor=lightgray,
			label="Layer 13 (PP Stage 13)",
			lheight=0.21,
			lp="1098,813.77",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_13_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 13 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,768.27",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer14 {
		graph [bb="969,518.17,1227,609.17",
			fillcolor=lightgray,
			label="Layer 14 (PP Stage 14)",
			lheight=0.21,
			lp="1098,597.67",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_14_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 14 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,552.17",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer15 {
		graph [bb="969,302.08,1227,393.08",
			fillcolor=lightgray,
			label="Layer 15 (PP Stage 15)",
			lheight=0.21,
			lp="1098,381.58",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_15_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 15 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,336.08",
			shape=rectangle,
			width=3.3611];
	}
	subgraph cluster_layer16 {
		graph [bb="969,85.983,1227,176.98",
			fillcolor=lightgray,
			label="Layer 16 (PP Stage 16)",
			lheight=0.21,
			lp="1098,165.48",
			lwidth=2.31,
			style="rounded,filled"
		];
		layer_16_block	[fillcolor=lightgreen,
			height=0.72222,
			label="Layer 16 Complete\nGPU: 0-255\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
			pos="1098,119.98",
			shape=rectangle,
			width=3.3611];
	}
	input	[fillcolor=lightblue,
		height=0.80532,
		label="Input\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, dim=512]",
		pos="825,4341.7",
		shape=ellipse,
		width=4.7533];
	input -> sp_split_1	[pos="e,825,4266.1 825,4312.5 825,4301.7 825,4288.9 825,4276.2"];
	pp_comm_1_2	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 1 -> 2\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,3254.9",
		shape=ellipse,
		width=4.7533];
	sp_merge_1 -> pp_comm_1_2	[pos="e,1098,3299.8 1098,3335.2 1098,3326.9 1098,3318.2 1098,3309.8"];
	pp_comm_1_2 -> layer_2_block	[pos="e,1098,3171.5 1098,3210.1 1098,3200.7 1098,3190.8 1098,3181.7"];
	pp_comm_2_3	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 2 -> 3\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,3038.8",
		shape=ellipse,
		width=4.7533];
	pp_comm_2_3 -> layer_3_block	[pos="e,1098,2955.4 1098,2994 1098,2984.6 1098,2974.7 1098,2965.6"];
	pp_comm_3_4	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 3 -> 4\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,2822.7",
		shape=ellipse,
		width=4.7533];
	pp_comm_3_4 -> layer_4_block	[pos="e,1098,2739.3 1098,2777.9 1098,2768.5 1098,2758.6 1098,2749.5"];
	pp_comm_4_5	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 4 -> 5\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,2606.6",
		shape=ellipse,
		width=4.7533];
	pp_comm_4_5 -> layer_5_block	[pos="e,1098,2523.2 1098,2561.8 1098,2552.4 1098,2542.5 1098,2533.4"];
	pp_comm_5_6	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 5 -> 6\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,2390.5",
		shape=ellipse,
		width=4.7533];
	pp_comm_5_6 -> layer_6_block	[pos="e,1098,2307.1 1098,2345.7 1098,2336.3 1098,2326.4 1098,2317.3"];
	pp_comm_6_7	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 6 -> 7\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,2174.4",
		shape=ellipse,
		width=4.7533];
	pp_comm_6_7 -> layer_7_block	[pos="e,1098,2091 1098,2129.6 1098,2120.2 1098,2110.3 1098,2101.2"];
	pp_comm_7_8	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 7 -> 8\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,1958.3",
		shape=ellipse,
		width=4.7533];
	pp_comm_7_8 -> layer_8_block	[pos="e,1098,1874.9 1098,1913.6 1098,1904.1 1098,1894.2 1098,1885.1"];
	pp_comm_8_9	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 8 -> 9\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,1742.2",
		shape=ellipse,
		width=4.7533];
	pp_comm_8_9 -> layer_9_block	[pos="e,1098,1658.8 1098,1697.5 1098,1688 1098,1678.1 1098,1669.1"];
	pp_comm_9_10	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 9 -> 10\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,1526.1",
		shape=ellipse,
		width=4.7533];
	pp_comm_9_10 -> layer_10_block	[pos="e,1098,1442.7 1098,1481.4 1098,1471.9 1098,1462 1098,1453"];
	pp_comm_10_11	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 10 -> 11\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,1310",
		shape=ellipse,
		width=4.7533];
	pp_comm_10_11 -> layer_11_block	[pos="e,1098,1226.6 1098,1265.3 1098,1255.8 1098,1245.9 1098,1236.9"];
	pp_comm_11_12	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 11 -> 12\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,1093.9",
		shape=ellipse,
		width=4.7533];
	pp_comm_11_12 -> layer_12_block	[pos="e,1098,1010.5 1098,1049.2 1098,1039.7 1098,1029.8 1098,1020.8"];
	pp_comm_12_13	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 12 -> 13\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,877.82",
		shape=ellipse,
		width=4.7533];
	pp_comm_12_13 -> layer_13_block	[pos="e,1098,794.45 1098,833.07 1098,823.63 1098,813.75 1098,804.67"];
	pp_comm_13_14	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 13 -> 14\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,661.72",
		shape=ellipse,
		width=4.7533];
	pp_comm_13_14 -> layer_14_block	[pos="e,1098,578.35 1098,616.98 1098,607.53 1098,597.66 1098,588.58"];
	pp_comm_14_15	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 14 -> 15\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,445.63",
		shape=ellipse,
		width=4.7533];
	pp_comm_14_15 -> layer_15_block	[pos="e,1098,362.26 1098,400.88 1098,391.44 1098,381.56 1098,372.48"];
	pp_comm_15_16	[fillcolor=lightblue,
		height=1.2374,
		label="PP Communication\nLayer 15 -> 16\nGPU: All GPUs\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=\
10240, dim=512]",
		pos="1098,229.53",
		shape=ellipse,
		width=4.7533];
	pp_comm_15_16 -> layer_16_block	[pos="e,1098,146.16 1098,184.79 1098,175.34 1098,165.47 1098,156.39"];
	output	[fillcolor=lightblue,
		height=0.80532,
		label="Output\nInput: [batch_size=128, seq_len=10240, dim=512]\nOutput: [batch_size=128, seq_len=10240, vocab_size=50000]",
		pos="1098,28.991",
		shape=ellipse,
		width=5.6372];
	layer_2_block -> pp_comm_2_3	[pos="e,1098,3083.4 1098,3119.3 1098,3111.5 1098,3102.5 1098,3093.5"];
	layer_3_block -> pp_comm_3_4	[pos="e,1098,2867.3 1098,2903.2 1098,2895.4 1098,2886.4 1098,2877.4"];
	layer_4_block -> pp_comm_4_5	[pos="e,1098,2651.2 1098,2687.1 1098,2679.3 1098,2670.3 1098,2661.3"];
	layer_5_block -> pp_comm_5_6	[pos="e,1098,2435.1 1098,2471 1098,2463.2 1098,2454.2 1098,2445.2"];
	layer_6_block -> pp_comm_6_7	[pos="e,1098,2219 1098,2254.9 1098,2247.1 1098,2238.1 1098,2229.1"];
	layer_7_block -> pp_comm_7_8	[pos="e,1098,2002.9 1098,2038.8 1098,2031 1098,2022 1098,2013.1"];
	layer_8_block -> pp_comm_8_9	[pos="e,1098,1786.8 1098,1822.7 1098,1814.9 1098,1806 1098,1797"];
	layer_9_block -> pp_comm_9_10	[pos="e,1098,1570.7 1098,1606.6 1098,1598.8 1098,1589.9 1098,1580.9"];
	layer_10_block -> pp_comm_10_11	[pos="e,1098,1354.6 1098,1390.5 1098,1382.7 1098,1373.8 1098,1364.8"];
	layer_11_block -> pp_comm_11_12	[pos="e,1098,1138.5 1098,1174.4 1098,1166.6 1098,1157.7 1098,1148.7"];
	layer_12_block -> pp_comm_12_13	[pos="e,1098,922.45 1098,958.32 1098,950.51 1098,941.57 1098,932.57"];
	layer_13_block -> pp_comm_13_14	[pos="e,1098,706.35 1098,742.23 1098,734.41 1098,725.48 1098,716.48"];
	layer_14_block -> pp_comm_14_15	[pos="e,1098,490.26 1098,526.13 1098,518.32 1098,509.38 1098,500.38"];
	layer_15_block -> pp_comm_15_16	[pos="e,1098,274.16 1098,310.04 1098,302.22 1098,293.29 1098,284.29"];
	layer_16_block -> output	[pos="e,1098,58.134 1098,93.719 1098,85.828 1098,76.938 1098,68.358"];
	dp_replicas	[fillcolor=lightcoral,
		height=1.0214,
		label="Data Parallelism\n7 Additional Replicas\nGPU: 256-2047\nEach replica: 256 GPUs",
		pos="1100,4341.7",
		shape=ellipse,
		width=2.3963];
}
