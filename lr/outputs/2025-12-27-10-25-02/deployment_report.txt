
MOE Parallel Strategy Deployment Report
=====================================

Hardware Configuration:
- GPU Compute Power: 400 TFlops (effective: 240.0 TFlops)
- GPU Memory: 64 GB
- GPU Bandwidth: 1.8 TBps (effective: 1.4400000000000002 TBps)

Parallel Strategy Configuration:
- Expert Parallelism: 16 (experts per layer)
- Pipeline Parallelism: 4 (layer groups)
- Data Parallelism: 2 (batch replicas)
- Tensor Parallelism: 2 (within experts)
- Total GPUs: 256

Memory Usage per GPU:
- Parameters: 0.15 GB
- KV Cache: 2.50 GB
- Activations: 0.08 GB
- Total: 2.72 GB (limit: 64 GB)

Performance Metrics:
- Throughput per GPU: 0.2 tokens/ms (requirement: 100.0 tokens/ms)
- Time to First Token: 33.554 seconds (requirement: 10.0 seconds)

Module Division Analysis:
- Total module divisions: 256
- Expert divisions: 16
- Layer divisions: 4
- Batch divisions: 2
- Tensor divisions: 2

Validation Results:
- Memory OK: True
- Throughput OK: False
- TTFT OK: False
- Module division OK: True

All requirements met: False
