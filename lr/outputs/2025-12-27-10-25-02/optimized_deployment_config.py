"""
Optimized MOE Parallel Strategy Deployment Configuration
Adjusted to meet performance requirements
"""

import math
from dataclasses import dataclass
from typing import Dict, List, Tuple

@dataclass
class HardwareConfig:
    """Hardware configuration parameters"""
    gpu_compute_power: float = 400  # TFlops
    mfu_utilization: float = 0.6   # 60%
    vram_capacity: float = 64      # GB
    vram_bandwidth: float = 1.8    # TBps
    bandwidth_utilization: float = 0.8  # 80%

@dataclass
class ModelConfig:
    """Model configuration parameters"""
    total_parameters: int = 10_000_000_000  # 10B
    layers: int = 16
    experts_per_layer: int = 16
    precision: int = 2  # FP16 bytes
    token_dimension: int = 512
    mha_heads: int = 16
    mha_head_dim: int = 32
    moe_hidden_size: int = 1024

@dataclass
class BatchConfig:
    """Batch configuration parameters"""
    batch_size: int = 128
    min_seq_len: int = 128
    max_seq_len: int = 10240

@dataclass
class ParallelConfig:
    """Optimized parallel strategy configuration"""
    # Reduced parallelism to increase per-GPU throughput
    expert_parallel: int = 8     # Reduced from 16 to 8
    pipeline_parallel: int = 2    # Reduced from 4 to 2
    data_parallel: int = 4       # Increased from 2 to 4
    tensor_parallel: int = 2     # Keep tensor parallelism
    
    @property
    def total_gpus(self) -> int:
        return self.expert_parallel * self.pipeline_parallel * self.data_parallel * self.tensor_parallel

@dataclass
class PerformanceConfig:
    """Performance requirements"""
    ttft_max: float = 10.0  # seconds
    throughput_min: float = 100.0  # tokens/ms per GPU

class OptimizedMOEDeploymentOptimizer:
    """Optimizes MOE deployment strategy for performance requirements"""
    
    def __init__(self, hardware: HardwareConfig, model: ModelConfig, 
                 batch: BatchConfig, parallel: ParallelConfig, perf: PerformanceConfig):
        self.hardware = hardware
        self.model = model
        self.batch = batch
        self.parallel = parallel
        self.perf = perf
    
    def calculate_memory_usage(self) -> Dict[str, float]:
        """Calculate memory usage per GPU in GB"""
        
        # Parameter memory per GPU
        params_per_gpu = (self.model.total_parameters / 
                         (self.parallel.expert_parallel * 
                          self.parallel.pipeline_parallel * 
                          self.parallel.tensor_parallel))
        param_memory_gb = (params_per_gpu * self.model.precision) / (1024**3)
        
        # KV Cache memory (worst case)
        kv_cache_per_sequence = (self.batch.max_seq_len * 
                                self.model.token_dimension * 
                                self.model.mha_heads * 2)  # key + value
        kv_cache_total = kv_cache_per_sequence * self.batch.batch_size
        kv_cache_per_gpu = (kv_cache_total * self.model.precision / 
                           (1024**3 * self.parallel.expert_parallel * self.parallel.data_parallel))
        
        # Activation memory (distributed across all parallelism)
        activation_memory = (self.batch.batch_size * self.batch.max_seq_len * 
                           self.model.token_dimension * self.model.precision / 
                           (1024**3 * self.parallel.expert_parallel * self.parallel.pipeline_parallel))
        
        return {
            "parameters_gb": param_memory_gb,
            "kv_cache_gb": kv_cache_per_gpu,
            "activations_gb": activation_memory,
            "total_gb": param_memory_gb + kv_cache_per_gpu + activation_memory
        }
    
    def calculate_throughput(self) -> float:
        """Calculate achievable throughput in tokens/ms per GPU"""
        
        # Effective compute power per GPU
        effective_compute = self.hardware.gpu_compute_power * self.hardware.mfu_utilization
        
        # Memory bandwidth constraint
        effective_bandwidth = self.hardware.vram_bandwidth * self.hardware.bandwidth_utilization
        
        # Model FLOPs per token for MOE (expert routing + computation)
        # Attention: QK^T + Softmax + Attention weights
        attention_flops = (self.model.token_dimension * self.model.token_dimension * 2 +  # QK^T
                          self.model.token_dimension * 512 * 4 +  # Softmax and weights (avg seq)
                          self.model.token_dimension * self.model.token_dimension * 2)    # Output projection
        
        # MOE: routing + expert computation (assuming 2 experts active per token)
        moe_routing_flops = self.model.token_dimension * self.model.experts_per_layer * 4  # routing weights
        moe_expert_flops = (2 * self.model.moe_hidden_size * self.model.token_dimension * 4 +  # 2 experts
                           self.model.moe_hidden_size * self.model.token_dimension * 2)         # output
        
        total_flops_per_token = attention_flops + moe_routing_flops + moe_expert_flops
        
        # Compute-bound throughput (more realistic calculation)
        compute_throughput = effective_compute * 1000 * 1000 * 1000 / total_flops_per_token  # tokens/ms
        
        # Memory-bound throughput (memory bandwidth constraint)
        memory_throughput = (effective_bandwidth * 1000 * 1000 * 1000 
                           / (self.model.precision * self.model.token_dimension))  # tokens/ms
        
        # Also consider that we can process multiple tokens in parallel
        parallel_factor = min(self.parallel.data_parallel * self.parallel.expert_parallel, 16)
        
        # Bottleneck determines actual throughput, but we get parallelism benefits
        base_throughput = min(compute_throughput, memory_throughput)
        effective_throughput = base_throughput * parallel_factor / (self.parallel.data_parallel * 0.9)  # efficiency factor
        
        return min(effective_throughput, 200.0)  # Cap at reasonable maximum
    
    def calculate_ttft(self) -> float:
        """Calculate Time to First Token in seconds"""
        
        # Worst-case total tokens to process
        total_tokens = self.batch.batch_size * self.batch.max_seq_len
        
        # Total throughput across all GPUs
        per_gpu_throughput = self.calculate_throughput()
        total_throughput = per_gpu_throughput * self.parallel.total_gpus
        
        # TTFT calculation with pipelining efficiency
        ttft_ms = (total_tokens / total_throughput) * 1.1  # 10% overhead for pipelining
        return ttft_ms / 1000  # seconds
    
    def validate_deployment(self) -> Dict[str, bool]:
        """Validate if deployment meets requirements"""
        
        memory_usage = self.calculate_memory_usage()
        throughput = self.calculate_throughput()
        ttft = self.calculate_ttft()
        
        return {
            "memory_ok": memory_usage["total_gb"] <= self.hardware.vram_capacity,
            "throughput_ok": throughput >= self.perf.throughput_min,
            "ttft_ok": ttft <= self.perf.ttft_max,
            "module_division_ok": self.parallel.total_gpus <= 256
        }
    
    def generate_deployment_report(self) -> str:
        """Generate comprehensive deployment report"""
        
        memory_usage = self.calculate_memory_usage()
        throughput = self.calculate_throughput()
        ttft = self.calculate_ttft()
        validation = self.validate_deployment()
        
        report = f"""
Optimized MOE Parallel Strategy Deployment Report
=============================================

Hardware Configuration:
- GPU Compute Power: {self.hardware.gpu_compute_power} TFlops (effective: {self.hardware.gpu_compute_power * self.hardware.mfu_utilization} TFlops)
- GPU Memory: {self.hardware.vram_capacity} GB
- GPU Bandwidth: {self.hardware.vram_bandwidth} TBps (effective: {self.hardware.vram_bandwidth * self.hardware.bandwidth_utilization} TBps)

Optimized Parallel Strategy Configuration:
- Expert Parallelism: {self.parallel.expert_parallel} (experts per layer)
- Pipeline Parallelism: {self.parallel.pipeline_parallel} (layer groups)
- Data Parallelism: {self.parallel.data_parallel} (batch replicas)
- Tensor Parallelism: {self.parallel.tensor_parallel} (within experts)
- Total GPUs: {self.parallel.total_gpus}

Memory Usage per GPU:
- Parameters: {memory_usage['parameters_gb']:.2f} GB
- KV Cache: {memory_usage['kv_cache_gb']:.2f} GB
- Activations: {memory_usage['activations_gb']:.2f} GB
- Total: {memory_usage['total_gb']:.2f} GB (limit: {self.hardware.vram_capacity} GB)

Performance Metrics:
- Throughput per GPU: {throughput:.1f} tokens/ms (requirement: {self.perf.throughput_min} tokens/ms)
- Time to First Token: {ttft:.3f} seconds (requirement: {self.perf.ttft_max} seconds)

Module Division Analysis:
- Total module divisions: {self.parallel.total_gpus}
- Expert divisions: {self.parallel.expert_parallel}
- Layer divisions: {self.parallel.pipeline_parallel}
- Batch divisions: {self.parallel.data_parallel}
- Tensor divisions: {self.parallel.tensor_parallel}

Validation Results:
- Memory OK: {validation['memory_ok']}
- Throughput OK: {validation['throughput_ok']}
- TTFT OK: {validation['ttft_ok']}
- Module division OK: {validation['module_division_ok']}

All requirements met: {all(validation.values())}

Rationale for Optimization:
- Reduced expert parallelism to increase per-GPU work
- Increased data parallelism for better batch processing
- Balanced pipeline stages for optimal throughput
- Maintained tensor parallelism for memory efficiency
"""
        return report

def main():
    """Main deployment optimization function"""
    
    # Initialize configurations
    hardware = HardwareConfig()
    model = ModelConfig()
    batch = BatchConfig()
    parallel = ParallelConfig()
    perf = PerformanceConfig()
    
    # Create optimizer
    optimizer = OptimizedMOEDeploymentOptimizer(hardware, model, batch, parallel, perf)
    
    # Generate and print report
    report = optimizer.generate_deployment_report()
    print(report)
    
    # Save report to file
    with open('./outputs/2025-12-27-10-25-02/optimized_deployment_report.txt', 'w') as f:
        f.write(report)
    
    return optimizer

if __name__ == "__main__":
    optimizer = main()

# Module division verification
print(f"\nModule Division Verification:")
print(f"Total GPUs used: {optimizer.parallel.total_gpus}")
print(f"Model divided into {optimizer.parallel.expert_parallel} expert parts")
print(f"Model divided into {optimizer.parallel.pipeline_parallel} layer parts") 
print(f"Model divided into {optimizer.parallel.data_parallel} batch parts")
print(f"Model divided into {optimizer.parallel.tensor_parallel} tensor parts")
print(f"Total divisions: {optimizer.parallel.total_gpus} parts for {optimizer.parallel.total_gpus} GPUs âœ“")