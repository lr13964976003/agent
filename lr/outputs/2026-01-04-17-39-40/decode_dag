// Decode Phase DAG
digraph {
	rankdir=TB size="20,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="INPUT\nInput: [batch_size=4, seq_len=1, hidden=6144]\nOutput: [batch_size=4, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
	subgraph stage0 {
		rank=same
		s0_emb_0 [label="GPU0: Embedding (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_0 [label="GPU0: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_0 [label="GPU0: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_0 [label="GPU0: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_0 [label="GPU0: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_0 [label="GPU0: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_0 [label="GPU0: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_0 [label="GPU0: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_0 [label="GPU0: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_0 [label="GPU0: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_0 [label="GPU0: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_0 [label="GPU0: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_0 [label="GPU0: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_1 [label="GPU1: Embedding (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_1 [label="GPU1: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_1 [label="GPU1: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_1 [label="GPU1: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_1 [label="GPU1: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_1 [label="GPU1: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_1 [label="GPU1: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_1 [label="GPU1: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_1 [label="GPU1: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_1 [label="GPU1: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_1 [label="GPU1: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_1 [label="GPU1: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_1 [label="GPU1: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_2 [label="GPU2: Embedding (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_2 [label="GPU2: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_2 [label="GPU2: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_2 [label="GPU2: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_2 [label="GPU2: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_2 [label="GPU2: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_2 [label="GPU2: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_2 [label="GPU2: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_2 [label="GPU2: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_2 [label="GPU2: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_2 [label="GPU2: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_2 [label="GPU2: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_2 [label="GPU2: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_3 [label="GPU3: Embedding (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_3 [label="GPU3: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_3 [label="GPU3: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_3 [label="GPU3: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_3 [label="GPU3: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_3 [label="GPU3: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_3 [label="GPU3: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_3 [label="GPU3: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_3 [label="GPU3: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_3 [label="GPU3: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_3 [label="GPU3: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_3 [label="GPU3: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_3 [label="GPU3: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_4 [label="GPU4: Embedding (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_4 [label="GPU4: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_4 [label="GPU4: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_4 [label="GPU4: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_4 [label="GPU4: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_4 [label="GPU4: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_4 [label="GPU4: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_4 [label="GPU4: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_4 [label="GPU4: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_4 [label="GPU4: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_4 [label="GPU4: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_4 [label="GPU4: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_4 [label="GPU4: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_5 [label="GPU5: Embedding (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_5 [label="GPU5: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_5 [label="GPU5: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_5 [label="GPU5: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_5 [label="GPU5: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_5 [label="GPU5: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_5 [label="GPU5: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_5 [label="GPU5: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_5 [label="GPU5: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_5 [label="GPU5: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_5 [label="GPU5: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_5 [label="GPU5: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_5 [label="GPU5: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_6 [label="GPU6: Embedding (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_6 [label="GPU6: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_6 [label="GPU6: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_6 [label="GPU6: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_6 [label="GPU6: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_6 [label="GPU6: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_6 [label="GPU6: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_6 [label="GPU6: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_6 [label="GPU6: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_6 [label="GPU6: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_6 [label="GPU6: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_6 [label="GPU6: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_6 [label="GPU6: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_7 [label="GPU7: Embedding (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_7 [label="GPU7: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_7 [label="GPU7: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_7 [label="GPU7: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_7 [label="GPU7: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_7 [label="GPU7: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_7 [label="GPU7: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_7 [label="GPU7: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_7 [label="GPU7: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_7 [label="GPU7: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_7 [label="GPU7: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_7 [label="GPU7: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_7 [label="GPU7: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_8 [label="GPU8: Embedding (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_8 [label="GPU8: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_8 [label="GPU8: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_8 [label="GPU8: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_8 [label="GPU8: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_8 [label="GPU8: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_8 [label="GPU8: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_8 [label="GPU8: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_8 [label="GPU8: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_8 [label="GPU8: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_8 [label="GPU8: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_8 [label="GPU8: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_8 [label="GPU8: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_9 [label="GPU9: Embedding (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_9 [label="GPU9: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_9 [label="GPU9: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_9 [label="GPU9: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_9 [label="GPU9: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_9 [label="GPU9: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_9 [label="GPU9: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_9 [label="GPU9: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_9 [label="GPU9: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_9 [label="GPU9: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_9 [label="GPU9: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_9 [label="GPU9: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_9 [label="GPU9: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_10 [label="GPU10: Embedding (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_10 [label="GPU10: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_10 [label="GPU10: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_10 [label="GPU10: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_10 [label="GPU10: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_10 [label="GPU10: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_10 [label="GPU10: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_10 [label="GPU10: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_10 [label="GPU10: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_10 [label="GPU10: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_10 [label="GPU10: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_10 [label="GPU10: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_10 [label="GPU10: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_11 [label="GPU11: Embedding (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_11 [label="GPU11: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_11 [label="GPU11: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_11 [label="GPU11: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_11 [label="GPU11: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_11 [label="GPU11: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_11 [label="GPU11: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_11 [label="GPU11: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_11 [label="GPU11: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_11 [label="GPU11: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_11 [label="GPU11: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_11 [label="GPU11: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_11 [label="GPU11: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_12 [label="GPU12: Embedding (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_12 [label="GPU12: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_12 [label="GPU12: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_12 [label="GPU12: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_12 [label="GPU12: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_12 [label="GPU12: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_12 [label="GPU12: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_12 [label="GPU12: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_12 [label="GPU12: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_12 [label="GPU12: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_12 [label="GPU12: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_12 [label="GPU12: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_12 [label="GPU12: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_13 [label="GPU13: Embedding (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_13 [label="GPU13: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_13 [label="GPU13: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_13 [label="GPU13: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_13 [label="GPU13: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_13 [label="GPU13: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_13 [label="GPU13: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_13 [label="GPU13: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_13 [label="GPU13: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_13 [label="GPU13: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_13 [label="GPU13: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_13 [label="GPU13: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_13 [label="GPU13: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_14 [label="GPU14: Embedding (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_14 [label="GPU14: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_14 [label="GPU14: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_14 [label="GPU14: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_14 [label="GPU14: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_14 [label="GPU14: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_14 [label="GPU14: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_14 [label="GPU14: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_14 [label="GPU14: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_14 [label="GPU14: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_14 [label="GPU14: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_14 [label="GPU14: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_14 [label="GPU14: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_emb_15 [label="GPU15: Embedding (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_ln0_15 [label="GPU15: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_q_15 [label="GPU15: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_k_15 [label="GPU15: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_v_15 [label="GPU15: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightblue]
		s0_allgather_q_15 [label="GPU15: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_k_15 [label="GPU15: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_allgather_v_15 [label="GPU15: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_attn_15 [label="GPU15: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_gate_15 [label="GPU15: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s0_expert_15 [label="GPU15: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
		s0_alltoall_15 [label="GPU15: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s0_ln1_15 [label="GPU15: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightblue]
	}
	subgraph stage1 {
		rank=same
		s1_ln0_16 [label="GPU16: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_16 [label="GPU16: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_16 [label="GPU16: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_16 [label="GPU16: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_16 [label="GPU16: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_16 [label="GPU16: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_16 [label="GPU16: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_16 [label="GPU16: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_16 [label="GPU16: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_16 [label="GPU16: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_16 [label="GPU16: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_16 [label="GPU16: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_17 [label="GPU17: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_17 [label="GPU17: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_17 [label="GPU17: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_17 [label="GPU17: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_17 [label="GPU17: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_17 [label="GPU17: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_17 [label="GPU17: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_17 [label="GPU17: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_17 [label="GPU17: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_17 [label="GPU17: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_17 [label="GPU17: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_17 [label="GPU17: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_18 [label="GPU18: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_18 [label="GPU18: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_18 [label="GPU18: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_18 [label="GPU18: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_18 [label="GPU18: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_18 [label="GPU18: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_18 [label="GPU18: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_18 [label="GPU18: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_18 [label="GPU18: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_18 [label="GPU18: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_18 [label="GPU18: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_18 [label="GPU18: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_19 [label="GPU19: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_19 [label="GPU19: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_19 [label="GPU19: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_19 [label="GPU19: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_19 [label="GPU19: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_19 [label="GPU19: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_19 [label="GPU19: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_19 [label="GPU19: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_19 [label="GPU19: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_19 [label="GPU19: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_19 [label="GPU19: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_19 [label="GPU19: LayerNorm (DP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_20 [label="GPU20: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_20 [label="GPU20: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_20 [label="GPU20: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_20 [label="GPU20: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_20 [label="GPU20: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_20 [label="GPU20: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_20 [label="GPU20: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_20 [label="GPU20: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_20 [label="GPU20: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_20 [label="GPU20: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_20 [label="GPU20: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_20 [label="GPU20: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_21 [label="GPU21: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_21 [label="GPU21: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_21 [label="GPU21: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_21 [label="GPU21: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_21 [label="GPU21: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_21 [label="GPU21: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_21 [label="GPU21: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_21 [label="GPU21: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_21 [label="GPU21: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_21 [label="GPU21: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_21 [label="GPU21: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_21 [label="GPU21: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_22 [label="GPU22: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_22 [label="GPU22: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_22 [label="GPU22: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_22 [label="GPU22: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_22 [label="GPU22: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_22 [label="GPU22: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_22 [label="GPU22: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_22 [label="GPU22: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_22 [label="GPU22: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_22 [label="GPU22: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_22 [label="GPU22: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_22 [label="GPU22: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_23 [label="GPU23: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_23 [label="GPU23: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_23 [label="GPU23: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_23 [label="GPU23: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_23 [label="GPU23: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_23 [label="GPU23: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_23 [label="GPU23: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_23 [label="GPU23: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_23 [label="GPU23: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_23 [label="GPU23: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_23 [label="GPU23: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_23 [label="GPU23: LayerNorm (DP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_24 [label="GPU24: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_24 [label="GPU24: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_24 [label="GPU24: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_24 [label="GPU24: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_24 [label="GPU24: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_24 [label="GPU24: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_24 [label="GPU24: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_24 [label="GPU24: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_24 [label="GPU24: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_24 [label="GPU24: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_24 [label="GPU24: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_24 [label="GPU24: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_25 [label="GPU25: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_25 [label="GPU25: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_25 [label="GPU25: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_25 [label="GPU25: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_25 [label="GPU25: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_25 [label="GPU25: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_25 [label="GPU25: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_25 [label="GPU25: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_25 [label="GPU25: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_25 [label="GPU25: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_25 [label="GPU25: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_25 [label="GPU25: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_26 [label="GPU26: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_26 [label="GPU26: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_26 [label="GPU26: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_26 [label="GPU26: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_26 [label="GPU26: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_26 [label="GPU26: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_26 [label="GPU26: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_26 [label="GPU26: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_26 [label="GPU26: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_26 [label="GPU26: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_26 [label="GPU26: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_26 [label="GPU26: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_27 [label="GPU27: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_27 [label="GPU27: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_27 [label="GPU27: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_27 [label="GPU27: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_27 [label="GPU27: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_27 [label="GPU27: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_27 [label="GPU27: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_27 [label="GPU27: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_27 [label="GPU27: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_27 [label="GPU27: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_27 [label="GPU27: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_27 [label="GPU27: LayerNorm (DP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_28 [label="GPU28: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_28 [label="GPU28: Q Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_28 [label="GPU28: K Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_28 [label="GPU28: V Linear (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_28 [label="GPU28: AllGather Q (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_28 [label="GPU28: AllGather K (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_28 [label="GPU28: AllGather V (TP0)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_28 [label="GPU28: Attention (TP0)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_28 [label="GPU28: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_28 [label="GPU28: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_28 [label="GPU28: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_28 [label="GPU28: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_29 [label="GPU29: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_29 [label="GPU29: Q Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_29 [label="GPU29: K Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_29 [label="GPU29: V Linear (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_29 [label="GPU29: AllGather Q (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_29 [label="GPU29: AllGather K (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_29 [label="GPU29: AllGather V (TP1)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_29 [label="GPU29: Attention (TP1)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_29 [label="GPU29: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_29 [label="GPU29: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_29 [label="GPU29: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_29 [label="GPU29: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_30 [label="GPU30: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_30 [label="GPU30: Q Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_30 [label="GPU30: K Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_30 [label="GPU30: V Linear (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_30 [label="GPU30: AllGather Q (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_30 [label="GPU30: AllGather K (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_30 [label="GPU30: AllGather V (TP2)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_30 [label="GPU30: Attention (TP2)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_30 [label="GPU30: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_30 [label="GPU30: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_30 [label="GPU30: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_30 [label="GPU30: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_ln0_31 [label="GPU31: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_q_31 [label="GPU31: Q Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_k_31 [label="GPU31: K Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_v_31 [label="GPU31: V Linear (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=1536]" fillcolor=lightgreen]
		s1_allgather_q_31 [label="GPU31: AllGather Q (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_k_31 [label="GPU31: AllGather K (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_allgather_v_31 [label="GPU31: AllGather V (TP3)\nInput: [batch_size=1, seq_len=1, hidden=1536]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_attn_31 [label="GPU31: Attention (TP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_gate_31 [label="GPU31: MoE Gate\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, experts=8]" fillcolor=orange shape=parallelogram]
		s1_expert_31 [label="GPU31: Expert FFN\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
		s1_alltoall_31 [label="GPU31: AllToAll\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
		s1_ln1_31 [label="GPU31: LayerNorm (DP3)\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=lightgreen]
	}
	output [label="OUTPUT\nInput: [batch_size=1, seq_len=1, hidden=6144]\nOutput: [batch_size=1, seq_len=1, hidden=6144]" fillcolor=white shape=ellipse]
	input -> s0_emb_0
	s0_emb_0 -> s0_ln0_0
	s0_ln0_0 -> s0_q_0
	s0_ln0_0 -> s0_k_0
	s0_ln0_0 -> s0_v_0
	s0_q_0 -> s0_allgather_q_0
	s0_k_0 -> s0_allgather_k_0
	s0_v_0 -> s0_allgather_v_0
	s0_allgather_q_0 -> s0_attn_0
	s0_allgather_k_0 -> s0_attn_0
	s0_allgather_v_0 -> s0_attn_0
	s0_attn_0 -> s0_gate_0
	s0_gate_0 -> s0_expert_0 [style=dashed]
	s0_expert_0 -> s0_alltoall_0
	s0_alltoall_0 -> s0_ln1_0
	input -> s0_emb_1
	s0_emb_1 -> s0_ln0_1
	s0_ln0_1 -> s0_q_1
	s0_ln0_1 -> s0_k_1
	s0_ln0_1 -> s0_v_1
	s0_q_1 -> s0_allgather_q_1
	s0_k_1 -> s0_allgather_k_1
	s0_v_1 -> s0_allgather_v_1
	s0_allgather_q_1 -> s0_attn_1
	s0_allgather_k_1 -> s0_attn_1
	s0_allgather_v_1 -> s0_attn_1
	s0_attn_1 -> s0_gate_1
	s0_gate_1 -> s0_expert_1 [style=dashed]
	s0_expert_1 -> s0_alltoall_1
	s0_alltoall_1 -> s0_ln1_1
	input -> s0_emb_2
	s0_emb_2 -> s0_ln0_2
	s0_ln0_2 -> s0_q_2
	s0_ln0_2 -> s0_k_2
	s0_ln0_2 -> s0_v_2
	s0_q_2 -> s0_allgather_q_2
	s0_k_2 -> s0_allgather_k_2
	s0_v_2 -> s0_allgather_v_2
	s0_allgather_q_2 -> s0_attn_2
	s0_allgather_k_2 -> s0_attn_2
	s0_allgather_v_2 -> s0_attn_2
	s0_attn_2 -> s0_gate_2
	s0_gate_2 -> s0_expert_2 [style=dashed]
	s0_expert_2 -> s0_alltoall_2
	s0_alltoall_2 -> s0_ln1_2
	input -> s0_emb_3
	s0_emb_3 -> s0_ln0_3
	s0_ln0_3 -> s0_q_3
	s0_ln0_3 -> s0_k_3
	s0_ln0_3 -> s0_v_3
	s0_q_3 -> s0_allgather_q_3
	s0_k_3 -> s0_allgather_k_3
	s0_v_3 -> s0_allgather_v_3
	s0_allgather_q_3 -> s0_attn_3
	s0_allgather_k_3 -> s0_attn_3
	s0_allgather_v_3 -> s0_attn_3
	s0_attn_3 -> s0_gate_3
	s0_gate_3 -> s0_expert_3 [style=dashed]
	s0_expert_3 -> s0_alltoall_3
	s0_alltoall_3 -> s0_ln1_3
	input -> s0_emb_4
	s0_emb_4 -> s0_ln0_4
	s0_ln0_4 -> s0_q_4
	s0_ln0_4 -> s0_k_4
	s0_ln0_4 -> s0_v_4
	s0_q_4 -> s0_allgather_q_4
	s0_k_4 -> s0_allgather_k_4
	s0_v_4 -> s0_allgather_v_4
	s0_allgather_q_4 -> s0_attn_4
	s0_allgather_k_4 -> s0_attn_4
	s0_allgather_v_4 -> s0_attn_4
	s0_attn_4 -> s0_gate_4
	s0_gate_4 -> s0_expert_4 [style=dashed]
	s0_expert_4 -> s0_alltoall_4
	s0_alltoall_4 -> s0_ln1_4
	input -> s0_emb_5
	s0_emb_5 -> s0_ln0_5
	s0_ln0_5 -> s0_q_5
	s0_ln0_5 -> s0_k_5
	s0_ln0_5 -> s0_v_5
	s0_q_5 -> s0_allgather_q_5
	s0_k_5 -> s0_allgather_k_5
	s0_v_5 -> s0_allgather_v_5
	s0_allgather_q_5 -> s0_attn_5
	s0_allgather_k_5 -> s0_attn_5
	s0_allgather_v_5 -> s0_attn_5
	s0_attn_5 -> s0_gate_5
	s0_gate_5 -> s0_expert_5 [style=dashed]
	s0_expert_5 -> s0_alltoall_5
	s0_alltoall_5 -> s0_ln1_5
	input -> s0_emb_6
	s0_emb_6 -> s0_ln0_6
	s0_ln0_6 -> s0_q_6
	s0_ln0_6 -> s0_k_6
	s0_ln0_6 -> s0_v_6
	s0_q_6 -> s0_allgather_q_6
	s0_k_6 -> s0_allgather_k_6
	s0_v_6 -> s0_allgather_v_6
	s0_allgather_q_6 -> s0_attn_6
	s0_allgather_k_6 -> s0_attn_6
	s0_allgather_v_6 -> s0_attn_6
	s0_attn_6 -> s0_gate_6
	s0_gate_6 -> s0_expert_6 [style=dashed]
	s0_expert_6 -> s0_alltoall_6
	s0_alltoall_6 -> s0_ln1_6
	input -> s0_emb_7
	s0_emb_7 -> s0_ln0_7
	s0_ln0_7 -> s0_q_7
	s0_ln0_7 -> s0_k_7
	s0_ln0_7 -> s0_v_7
	s0_q_7 -> s0_allgather_q_7
	s0_k_7 -> s0_allgather_k_7
	s0_v_7 -> s0_allgather_v_7
	s0_allgather_q_7 -> s0_attn_7
	s0_allgather_k_7 -> s0_attn_7
	s0_allgather_v_7 -> s0_attn_7
	s0_attn_7 -> s0_gate_7
	s0_gate_7 -> s0_expert_7 [style=dashed]
	s0_expert_7 -> s0_alltoall_7
	s0_alltoall_7 -> s0_ln1_7
	input -> s0_emb_8
	s0_emb_8 -> s0_ln0_8
	s0_ln0_8 -> s0_q_8
	s0_ln0_8 -> s0_k_8
	s0_ln0_8 -> s0_v_8
	s0_q_8 -> s0_allgather_q_8
	s0_k_8 -> s0_allgather_k_8
	s0_v_8 -> s0_allgather_v_8
	s0_allgather_q_8 -> s0_attn_8
	s0_allgather_k_8 -> s0_attn_8
	s0_allgather_v_8 -> s0_attn_8
	s0_attn_8 -> s0_gate_8
	s0_gate_8 -> s0_expert_8 [style=dashed]
	s0_expert_8 -> s0_alltoall_8
	s0_alltoall_8 -> s0_ln1_8
	input -> s0_emb_9
	s0_emb_9 -> s0_ln0_9
	s0_ln0_9 -> s0_q_9
	s0_ln0_9 -> s0_k_9
	s0_ln0_9 -> s0_v_9
	s0_q_9 -> s0_allgather_q_9
	s0_k_9 -> s0_allgather_k_9
	s0_v_9 -> s0_allgather_v_9
	s0_allgather_q_9 -> s0_attn_9
	s0_allgather_k_9 -> s0_attn_9
	s0_allgather_v_9 -> s0_attn_9
	s0_attn_9 -> s0_gate_9
	s0_gate_9 -> s0_expert_9 [style=dashed]
	s0_expert_9 -> s0_alltoall_9
	s0_alltoall_9 -> s0_ln1_9
	input -> s0_emb_10
	s0_emb_10 -> s0_ln0_10
	s0_ln0_10 -> s0_q_10
	s0_ln0_10 -> s0_k_10
	s0_ln0_10 -> s0_v_10
	s0_q_10 -> s0_allgather_q_10
	s0_k_10 -> s0_allgather_k_10
	s0_v_10 -> s0_allgather_v_10
	s0_allgather_q_10 -> s0_attn_10
	s0_allgather_k_10 -> s0_attn_10
	s0_allgather_v_10 -> s0_attn_10
	s0_attn_10 -> s0_gate_10
	s0_gate_10 -> s0_expert_10 [style=dashed]
	s0_expert_10 -> s0_alltoall_10
	s0_alltoall_10 -> s0_ln1_10
	input -> s0_emb_11
	s0_emb_11 -> s0_ln0_11
	s0_ln0_11 -> s0_q_11
	s0_ln0_11 -> s0_k_11
	s0_ln0_11 -> s0_v_11
	s0_q_11 -> s0_allgather_q_11
	s0_k_11 -> s0_allgather_k_11
	s0_v_11 -> s0_allgather_v_11
	s0_allgather_q_11 -> s0_attn_11
	s0_allgather_k_11 -> s0_attn_11
	s0_allgather_v_11 -> s0_attn_11
	s0_attn_11 -> s0_gate_11
	s0_gate_11 -> s0_expert_11 [style=dashed]
	s0_expert_11 -> s0_alltoall_11
	s0_alltoall_11 -> s0_ln1_11
	input -> s0_emb_12
	s0_emb_12 -> s0_ln0_12
	s0_ln0_12 -> s0_q_12
	s0_ln0_12 -> s0_k_12
	s0_ln0_12 -> s0_v_12
	s0_q_12 -> s0_allgather_q_12
	s0_k_12 -> s0_allgather_k_12
	s0_v_12 -> s0_allgather_v_12
	s0_allgather_q_12 -> s0_attn_12
	s0_allgather_k_12 -> s0_attn_12
	s0_allgather_v_12 -> s0_attn_12
	s0_attn_12 -> s0_gate_12
	s0_gate_12 -> s0_expert_12 [style=dashed]
	s0_expert_12 -> s0_alltoall_12
	s0_alltoall_12 -> s0_ln1_12
	input -> s0_emb_13
	s0_emb_13 -> s0_ln0_13
	s0_ln0_13 -> s0_q_13
	s0_ln0_13 -> s0_k_13
	s0_ln0_13 -> s0_v_13
	s0_q_13 -> s0_allgather_q_13
	s0_k_13 -> s0_allgather_k_13
	s0_v_13 -> s0_allgather_v_13
	s0_allgather_q_13 -> s0_attn_13
	s0_allgather_k_13 -> s0_attn_13
	s0_allgather_v_13 -> s0_attn_13
	s0_attn_13 -> s0_gate_13
	s0_gate_13 -> s0_expert_13 [style=dashed]
	s0_expert_13 -> s0_alltoall_13
	s0_alltoall_13 -> s0_ln1_13
	input -> s0_emb_14
	s0_emb_14 -> s0_ln0_14
	s0_ln0_14 -> s0_q_14
	s0_ln0_14 -> s0_k_14
	s0_ln0_14 -> s0_v_14
	s0_q_14 -> s0_allgather_q_14
	s0_k_14 -> s0_allgather_k_14
	s0_v_14 -> s0_allgather_v_14
	s0_allgather_q_14 -> s0_attn_14
	s0_allgather_k_14 -> s0_attn_14
	s0_allgather_v_14 -> s0_attn_14
	s0_attn_14 -> s0_gate_14
	s0_gate_14 -> s0_expert_14 [style=dashed]
	s0_expert_14 -> s0_alltoall_14
	s0_alltoall_14 -> s0_ln1_14
	input -> s0_emb_15
	s0_emb_15 -> s0_ln0_15
	s0_ln0_15 -> s0_q_15
	s0_ln0_15 -> s0_k_15
	s0_ln0_15 -> s0_v_15
	s0_q_15 -> s0_allgather_q_15
	s0_k_15 -> s0_allgather_k_15
	s0_v_15 -> s0_allgather_v_15
	s0_allgather_q_15 -> s0_attn_15
	s0_allgather_k_15 -> s0_attn_15
	s0_allgather_v_15 -> s0_attn_15
	s0_attn_15 -> s0_gate_15
	s0_gate_15 -> s0_expert_15 [style=dashed]
	s0_expert_15 -> s0_alltoall_15
	s0_alltoall_15 -> s0_ln1_15
	s0_ln1_0 -> s1_ln0_16
	s0_ln1_1 -> s1_ln0_17
	s0_ln1_2 -> s1_ln0_18
	s0_ln1_3 -> s1_ln0_19
	s0_ln1_4 -> s1_ln0_20
	s0_ln1_5 -> s1_ln0_21
	s0_ln1_6 -> s1_ln0_22
	s0_ln1_7 -> s1_ln0_23
	s0_ln1_8 -> s1_ln0_24
	s0_ln1_9 -> s1_ln0_25
	s0_ln1_10 -> s1_ln0_26
	s0_ln1_11 -> s1_ln0_27
	s0_ln1_12 -> s1_ln0_28
	s0_ln1_13 -> s1_ln0_29
	s0_ln1_14 -> s1_ln0_30
	s0_ln1_15 -> s1_ln0_31
	s1_ln1_16 -> output
	s1_ln1_17 -> output
	s1_ln1_18 -> output
	s1_ln1_19 -> output
	s1_ln1_20 -> output
	s1_ln1_21 -> output
	s1_ln1_22 -> output
	s1_ln1_23 -> output
	s1_ln1_24 -> output
	s1_ln1_25 -> output
	s1_ln1_26 -> output
	s1_ln1_27 -> output
	s1_ln1_28 -> output
	s1_ln1_29 -> output
	s1_ln1_30 -> output
	s1_ln1_31 -> output
}
