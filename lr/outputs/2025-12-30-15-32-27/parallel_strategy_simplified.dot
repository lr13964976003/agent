digraph "Parallel Strategy Deployment DAG - Simplified" {
	comment="Parallel Strategy Deployment DAG - Simplified"
	rankdir=TB
	size="50,50"
	dpi="300"
	node [fontname=Arial, fontsize=10]
	
	// Input
	input [fillcolor=lightgreen, label="Input\n[batch_size=128, seq_len=?, heads=16, d_k=32]", shape=rectangle, style=filled]
	
	// Layer 0 - GPUs 0-15
	mha_0 [fillcolor=lightgreen, label="MHA Layer 0\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_0 [fillcolor=lightyellow, label="Gate Layer 0\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_0 [fillcolor=lightblue, label="Expert Routing\nGPUs: 0-15\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_0 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 0-15\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_0 [fillcolor=lightyellow, label="Aggregate Layer 0\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 1 - GPUs 16-31
	mha_1 [fillcolor=lightgreen, label="MHA Layer 1\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_1 [fillcolor=lightyellow, label="Gate Layer 1\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_1 [fillcolor=lightblue, label="Expert Routing\nGPUs: 16-31\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_1 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 16-31\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_1 [fillcolor=lightyellow, label="Aggregate Layer 1\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 2 - GPUs 32-47
	mha_2 [fillcolor=lightgreen, label="MHA Layer 2\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_2 [fillcolor=lightyellow, label="Gate Layer 2\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_2 [fillcolor=lightblue, label="Expert Routing\nGPUs: 32-47\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_2 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 32-47\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_2 [fillcolor=lightyellow, label="Aggregate Layer 2\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 3 - GPUs 48-63
	mha_3 [fillcolor=lightgreen, label="MHA Layer 3\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_3 [fillcolor=lightyellow, label="Gate Layer 3\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_3 [fillcolor=lightblue, label="Expert Routing\nGPUs: 48-63\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_3 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 48-63\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_3 [fillcolor=lightyellow, label="Aggregate Layer 3\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 4 - GPUs 64-79
	mha_4 [fillcolor=lightgreen, label="MHA Layer 4\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_4 [fillcolor=lightyellow, label="Gate Layer 4\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_4 [fillcolor=lightblue, label="Expert Routing\nGPUs: 64-79\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_4 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 64-79\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_4 [fillcolor=lightyellow, label="Aggregate Layer 4\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 5 - GPUs 80-95
	mha_5 [fillcolor=lightgreen, label="MHA Layer 5\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_5 [fillcolor=lightyellow, label="Gate Layer 5\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_5 [fillcolor=lightblue, label="Expert Routing\nGPUs: 80-95\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_5 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 80-95\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_5 [fillcolor=lightyellow, label="Aggregate Layer 5\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 6 - GPUs 96-111
	mha_6 [fillcolor=lightgreen, label="MHA Layer 6\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_6 [fillcolor=lightyellow, label="Gate Layer 6\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_6 [fillcolor=lightblue, label="Expert Routing\nGPUs: 96-111\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_6 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 96-111\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_6 [fillcolor=lightyellow, label="Aggregate Layer 6\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 7 - GPUs 112-127
	mha_7 [fillcolor=lightgreen, label="MHA Layer 7\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_7 [fillcolor=lightyellow, label="Gate Layer 7\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_7 [fillcolor=lightblue, label="Expert Routing\nGPUs: 112-127\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_7 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 112-127\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_7 [fillcolor=lightyellow, label="Aggregate Layer 7\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 8 - GPUs 128-143
	mha_8 [fillcolor=lightgreen, label="MHA Layer 8\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_8 [fillcolor=lightyellow, label="Gate Layer 8\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_8 [fillcolor=lightblue, label="Expert Routing\nGPUs: 128-143\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_8 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 128-143\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_8 [fillcolor=lightyellow, label="Aggregate Layer 8\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 9 - GPUs 144-159
	mha_9 [fillcolor=lightgreen, label="MHA Layer 9\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_9 [fillcolor=lightyellow, label="Gate Layer 9\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_9 [fillcolor=lightblue, label="Expert Routing\nGPUs: 144-159\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_9 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 144-159\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_9 [fillcolor=lightyellow, label="Aggregate Layer 9\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 10 - GPUs 160-175
	mha_10 [fillcolor=lightgreen, label="MHA Layer 10\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_10 [fillcolor=lightyellow, label="Gate Layer 10\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_10 [fillcolor=lightblue, label="Expert Routing\nGPUs: 160-175\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_10 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 160-175\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_10 [fillcolor=lightyellow, label="Aggregate Layer 10\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 11 - GPUs 176-191
	mha_11 [fillcolor=lightgreen, label="MHA Layer 11\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_11 [fillcolor=lightyellow, label="Gate Layer 11\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_11 [fillcolor=lightblue, label="Expert Routing\nGPUs: 176-191\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_11 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 176-191\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_11 [fillcolor=lightyellow, label="Aggregate Layer 11\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 12 - GPUs 192-207
	mha_12 [fillcolor=lightgreen, label="MHA Layer 12\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_12 [fillcolor=lightyellow, label="Gate Layer 12\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_12 [fillcolor=lightblue, label="Expert Routing\nGPUs: 192-207\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_12 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 192-207\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_12 [fillcolor=lightyellow, label="Aggregate Layer 12\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 13 - GPUs 208-223
	mha_13 [fillcolor=lightgreen, label="MHA Layer 13\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_13 [fillcolor=lightyellow, label="Gate Layer 13\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_13 [fillcolor=lightblue, label="Expert Routing\nGPUs: 208-223\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_13 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 208-223\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_13 [fillcolor=lightyellow, label="Aggregate Layer 13\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 14 - GPUs 224-239
	mha_14 [fillcolor=lightgreen, label="MHA Layer 14\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_14 [fillcolor=lightyellow, label="Gate Layer 14\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_14 [fillcolor=lightblue, label="Expert Routing\nGPUs: 224-239\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_14 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 224-239\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_14 [fillcolor=lightyellow, label="Aggregate Layer 14\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Layer 15 - GPUs 240-255
	mha_15 [fillcolor=lightgreen, label="MHA Layer 15\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	gate_15 [fillcolor=lightyellow, label="Gate Layer 15\nGPU: All GPUs\n[batch_size=128, seq_len=?, top_k=2, experts=16]", shape=parallelogram, style=filled]
	comm_15 [fillcolor=lightblue, label="Expert Routing\nGPUs: 240-255\n[batch_size=?, seq_len=?, top_k=2]", shape=ellipse, style=filled]
	experts_15 [fillcolor=lightgreen, label="Experts 0-15\nGPUs: 240-255\n[batch_size=?, seq_len=?, heads=16, d_k=512]", shape=rectangle, style=filled]
	agg_15 [fillcolor=lightyellow, label="Aggregate Layer 15\nGPU: All GPUs\n[batch_size=128, seq_len=?, heads=16, d_k=512]", shape=parallelogram, style=filled]
	
	// Output
	output [fillcolor=lightgreen, label="Output\n[batch_size=128, seq_len=?, vocab_size=?]", shape=rectangle, style=filled]
	
	// Connections
	input -> mha_0
	
	// Layer 0
	mha_0 -> gate_0
	gate_0 -> comm_0 [color=red, style=dashed]
	comm_0 -> experts_0
	experts_0 -> agg_0
	
	// Layer 1
	agg_0 -> mha_1 [color=blue, penwidth=2]
	mha_1 -> gate_1
	gate_1 -> comm_1 [color=red, style=dashed]
	comm_1 -> experts_1
	experts_1 -> agg_1
	
	// Layer 2
	agg_1 -> mha_2 [color=blue, penwidth=2]
	mha_2 -> gate_2
	gate_2 -> comm_2 [color=red, style=dashed]
	comm_2 -> experts_2
	experts_2 -> agg_2
	
	// Layer 3
	agg_2 -> mha_3 [color=blue, penwidth=2]
	mha_3 -> gate_3
	gate_3 -> comm_3 [color=red, style=dashed]
	comm_3 -> experts_3
	experts_3 -> agg_3
	
	// Layer 4
	agg_3 -> mha_4 [color=blue, penwidth=2]
	mha_4 -> gate_4
	gate_4 -> comm_4 [color=red, style=dashed]
	comm_4 -> experts_4
	experts_4 -> agg_4
	
	// Layer 5
	agg_4 -> mha_5 [color=blue, penwidth=2]
	mha_5 -> gate_5
	gate_5 -> comm_5 [color=red, style=dashed]
	comm_5 -> experts_5
	experts_5 -> agg_5
	
	// Layer 6
	agg_5 -> mha_6 [color=blue, penwidth=2]
	mha_6 -> gate_6
	gate_6 -> comm_6 [color=red, style=dashed]
	comm_6 -> experts_6
	experts_6 -> agg_6
	
	// Layer 7
	agg_6 -> mha_7 [color=blue, penwidth=2]
	mha_7 -> gate_7
	gate_7 -> comm_7 [color=red, style=dashed]
	comm_7 -> experts_7
	experts_7 -> agg_7
	
	// Layer 8
	agg_7 -> mha_8 [color=blue, penwidth=2]
	mha_8 -> gate_8
	gate_8 -> comm_8 [color=red, style=dashed]
	comm_8 -> experts_8
	experts_8 -> agg_8
	
	// Layer 9
	agg_8 -> mha_9 [color=blue, penwidth=2]
	mha_9 -> gate_9
	gate_9 -> comm_9 [color=red, style=dashed]
	comm_9 -> experts_9
	experts_9 -> agg_9
	
	// Layer 10
	agg_9 -> mha_10 [color=blue, penwidth=2]
	mha_10 -> gate_10
	gate_10 -> comm_10 [color=red, style=dashed]
	comm_10 -> experts_10
	experts_10 -> agg_10
	
	// Layer 11
	agg_10 -> mha_11 [color=blue, penwidth=2]
	mha_11 -> gate_11
	gate_11 -> comm_11 [color=red, style=dashed]
	comm_11 -> experts_11
	experts_11 -> agg_11
	
	// Layer 12
	agg_11 -> mha_12 [color=blue, penwidth=2]
	mha_12 -> gate_12
	gate_12 -> comm_12 [color=red, style=dashed]
	comm_12 -> experts_12
	experts_12 -> agg_12
	
	// Layer 13
	agg_12 -> mha_13 [color=blue, penwidth=2]
	mha_13 -> gate_13
	gate_13 -> comm_13 [color=red, style=dashed]
	comm_13 -> experts_13
	experts_13 -> agg_13
	
	// Layer 14
	agg_13 -> mha_14 [color=blue, penwidth=2]
	mha_14 -> gate_14
	gate_14 -> comm_14 [color=red, style=dashed]
	comm_14 -> experts_14
	experts_14 -> agg_14
	
	// Layer 15
	agg_14 -> mha_15 [color=blue, penwidth=2]
	mha_15 -> gate_15
	gate_15 -> comm_15 [color=red, style=dashed]
	comm_15 -> experts_15
	experts_15 -> agg_15
	
	// Final output
	agg_15 -> output
}