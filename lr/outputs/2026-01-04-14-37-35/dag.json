{
  "dag_content": "// Qwen3-235B Parallel Strategy DAG - Corrected\ndigraph {\n\trankdir=TB size=\"20,30\"\n\tnode [fillcolor=lightblue shape=ellipse style=filled]\n\tinput [label=\"Input\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=ellipse]\n\tsubgraph cluster_stage1 {\n\t\tfillcolor=lightyellow label=\"Pipeline Stage 1 (GPUs 0-7)\" style=rounded\n\t\tembed [label=\"Embedding (GPU-0)\\nInput: [batch_size=128, seq_len=2048]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightcoral shape=rectangle]\n\t\tnorm1_1 [label=\"LayerNorm (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_qkv_1 [label=\"QKV Projection (GPU-0,1)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_score_1 [label=\"Attention Scores (GPU-0,1)\\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_out_1 [label=\"Attention Output (GPU-0,1)\\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightblue shape=rectangle]\n\t\tgate_1 [label=\"Expert Gate (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=16]\" fillcolor=orange shape=parallelogram style=dashed]\n\t\texpert_1_0 [label=\"Expert 0 (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_1 [label=\"Expert 1 (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_2 [label=\"Expert 2 (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_3 [label=\"Expert 3 (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_4 [label=\"Expert 4 (GPU-1)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_5 [label=\"Expert 5 (GPU-1)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_6 [label=\"Expert 6 (GPU-1)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_1_7 [label=\"Expert 7 (GPU-1)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_agg_1 [label=\"Expert Aggregation (GPU-0)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=orange shape=parallelogram]\n\t}\n\tsubgraph cluster_stage2 {\n\t\tfillcolor=lightyellow label=\"Pipeline Stage 2 (GPUs 8-15)\" style=rounded\n\t\tcomm_s1_s2 [label=\"Pipeline Communication\\nGPU-7 → GPU-8\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=red shape=ellipse]\n\t\tnorm2_1 [label=\"LayerNorm (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_qkv_2 [label=\"QKV Projection (GPU-8,9)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_score_2 [label=\"Attention Scores (GPU-8,9)\\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\" fillcolor=lightblue shape=rectangle]\n\t\tattn_out_2 [label=\"Attention Output (GPU-8,9)\\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=32]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightblue shape=rectangle]\n\t\tgate_2 [label=\"Expert Gate (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=16]\" fillcolor=orange shape=parallelogram style=dashed]\n\t\texpert_2_0 [label=\"Expert 0 (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_1 [label=\"Expert 1 (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_2 [label=\"Expert 2 (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_3 [label=\"Expert 3 (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_4 [label=\"Expert 4 (GPU-9)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_5 [label=\"Expert 5 (GPU-9)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_6 [label=\"Expert 6 (GPU-9)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_2_7 [label=\"Expert 7 (GPU-9)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=rectangle]\n\t\texpert_agg_2 [label=\"Expert Aggregation (GPU-8)\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=orange shape=parallelogram]\n\t\toutput [label=\"Output\\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]\" fillcolor=lightgreen shape=ellipse]\n\t}\n\tinput -> embed\n\tembed -> norm1_1\n\tnorm1_1 -> attn_qkv_1\n\tattn_qkv_1 -> attn_score_1\n\tattn_score_1 -> attn_out_1\n\tattn_out_1 -> gate_1\n\tgate_1 -> expert_1_0 [style=dashed]\n\texpert_1_0 -> expert_agg_1\n\tgate_1 -> expert_1_1 [style=dashed]\n\texpert_1_1 -> expert_agg_1\n\tgate_1 -> expert_1_2 [style=dashed]\n\texpert_1_2 -> expert_agg_1\n\tgate_1 -> expert_1_3 [style=dashed]\n\texpert_1_3 -> expert_agg_1\n\tgate_1 -> expert_1_4 [style=dashed]\n\texpert_1_4 -> expert_agg_1\n\tgate_1 -> expert_1_5 [style=dashed]\n\texpert_1_5 -> expert_agg_1\n\tgate_1 -> expert_1_6 [style=dashed]\n\texpert_1_6 -> expert_agg_1\n\tgate_1 -> expert_1_7 [style=dashed]\n\texpert_1_7 -> expert_agg_1\n\texpert_agg_1 -> comm_s1_s2\n\tcomm_s1_s2 -> norm2_1\n\tnorm2_1 -> attn_qkv_2\n\tattn_qkv_2 -> attn_score_2\n\tattn_score_2 -> attn_out_2\n\tattn_out_2 -> gate_2\n\tgate_2 -> expert_2_0 [style=dashed]\n\texpert_2_0 -> expert_agg_2\n\tgate_2 -> expert_2_1 [style=dashed]\n\texpert_2_1 -> expert_agg_2\n\tgate_2 -> expert_2_2 [style=dashed]\n\texpert_2_2 -> expert_agg_2\n\tgate_2 -> expert_2_3 [style=dashed]\n\texpert_2_3 -> expert_agg_2\n\tgate_2 -> expert_2_4 [style=dashed]\n\texpert_2_4 -> expert_agg_2\n\tgate_2 -> expert_2_5 [style=dashed]\n\texpert_2_5 -> expert_agg_2\n\tgate_2 -> expert_2_6 [style=dashed]\n\texpert_2_6 -> expert_agg_2\n\tgate_2 -> expert_2_7 [style=dashed]\n\texpert_2_7 -> expert_agg_2\n\texpert_agg_2 -> output\n}",
  "analysis_results": {
    "has_cycle": false,
    "nodes_with_only_in": ["output"],
    "nodes_with_only_out": ["input"],
    "expert_parallelism": "8 experts per group (16 total experts per stage)",
    "tensor_parallelism": "2 GPUs per attention operation (32 heads per GPU)",
    "pipeline_parallelism": "2 stages with 47 layers each",
    "communication_identified": ["GPU-7 → GPU-8 pipeline communication"],
    "attention_breakdown": "Complete: QKV Projection → Attention Scores → Attention Output",
    "node_connectivity": "All nodes properly connected except input/output terminals"
  },
  "corrections_made": [
    "Fixed parallel strategy to match deployment JSON (2 PP stages instead of 4)",
    "Added complete attention blocks in all stages",
    "Connected all expert nodes to aggregation nodes",
    "Updated GPU allocations to match 2×2×4=16 GPU configuration",
    "Added tensor parallelism notation for attention operations"
  ]
}