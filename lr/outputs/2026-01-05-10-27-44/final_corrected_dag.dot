digraph "Final Corrected DAG - PP4_TP4_DP2_EP1" {
	graph [bgcolor=white, fontname=Arial, fontsize=12, nodesep=0.5, ranksep=0.8, size="40,50"];
	node [fillcolor=lightblue, fontname=Arial, fontsize=10, shape=ellipse, style=filled];
	edge [fontname=Arial, fontsize=10];

	// Input node
	input [fillcolor=lightcoral, fontsize=12, label="Input\n[batch_size=512, seq_len=2048, heads=64, d_k=64]\nGPU: Host", shape=ellipse];
	dp_split [fillcolor=lightyellow, fontsize=12, label="Data Parallel Split\n[batch_size=256, seq_len=2048, heads=64, d_k=64]\nGPU: Host", shape=parallelogram];
	input -> dp_split [label="DP=2"];

	// DP0 Pipeline Stage 0 (GPUs 0-7) - Complete attention breakdown
	dp0_q_proj_s0_l0_tp0 [fillcolor=lightgreen, label="Q Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp0_k_proj_s0_l0_tp0 [fillcolor=lightgreen, label="K Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp0_v_proj_s0_l0_tp0 [fillcolor=lightgreen, label="V Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp_split -> dp0_q_proj_s0_l0_tp0;
	dp_split -> dp0_k_proj_s0_l0_tp0;
	dp_split -> dp0_v_proj_s0_l0_tp0;

	// QKV cache and attention computation
	dp0_qkha_s0_l0_tp0 [fillcolor=lightgreen, label="QK Matrix Multiply L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp0_softmax_s0_l0_tp0 [fillcolor=lightgreen, label="Softmax L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp0_attn_out_s0_l0_tp0 [fillcolor=lightgreen, label="Attention Output L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0", shape=rectangle];
	dp0_q_proj_s0_l0_tp0 -> dp0_qk_s0_l0_tp0;
	dp0_k_proj_s0_l0_tp0 -> dp0_qk_s0_l0_tp0;
	dp0_qk_s0_l0_tp0 -> dp0_softmax_s0_l0_tp0;
	dp0_softmax_s0_l0_tp0 -> dp0_attn_out_s0_l0_tp0;
	dp0_v_proj_s0_l0_tp0 -> dp0_attn_out_s0_l0_tp0;

	// TP communications between GPUs 0-3
	dp0_attn_out_s0_l0_tp1 [fillcolor=lightgreen, label="Attention Output L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU1", shape=rectangle];
	dp0_attn_out_s0_l0_tp2 [fillcolor=lightgreen, label="Attention Output L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU2", shape=rectangle];
	dp0_attn_out_s0_l0_tp3 [fillcolor=lightgreen, label="Attention Output L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU3", shape=rectangle];
	
	dp0_attn_ar0_s0_l0 [fillcolor=lightblue, label="TP All-Reduce L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU0→GPU1", shape=ellipse];
	dp0_attn_ar1_s0_l0 [fillcolor=lightblue, label="TP All-Reduce L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU1→GPU2", shape=ellipse];
	dp0_attn_ar2_s0_l0 [fillcolor=lightblue, label="TP All-Reduce L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU2→GPU3", shape=ellipse];
	
	dp0_attn_out_s0_l0_tp0 -> dp0_attn_ar0_s0_l0;
	dp0_attn_ar0_s0_l0 -> dp0_attn_out_s0_l0_tp1;
	dp0_attn_out_s0_l0_tp1 -> dp0_attn_ar1_s0_l0;
	dp0_attn_ar1_s0_l0 -> dp0_attn_out_s0_l0_tp2;
	dp0_attn_out_s0_l0_tp2 -> dp0_attn_ar2_s0_l0;
	dp0_attn_ar2_s0_l0 -> dp0_attn_out_s0_l0_tp3;

	// Complete MoE layer with expert parallelism
	dp0_gate_s0_l0 [fillcolor=lightyellow, label="Gate Selection L0\n[batch_size=256, seq_len=2048, experts=128, top_k=8]\nGPU: GPU0", shape=parallelogram];
	dp0_expert0_s0_l0 [fillcolor=lightgreen, label="Expert 0-31 L0\n[batch_size=256, seq_len=2048, experts=32]\nGPU: GPU0", shape=rectangle];
	dp0_expert1_s0_l0 [fillcolor=lightgreen, label="Expert 32-63 L0\n[batch_size=256, seq_len=2048, experts=32]\nGPU: GPU1", shape=rectangle];
	dp0_expert2_s0_l0 [fillcolor=lightgreen, label="Expert 64-95 L0\n[batch_size=256, seq_len=2048, experts=32]\nGPU: GPU2", shape=rectangle];
	dp0_expert3_s0_l0 [fillcolor=lightgreen, label="Expert 96-127 L0\n[batch_size=256, seq_len=2048, experts=32]\nGPU: GPU3", shape=rectangle];
	dp0_attn_out_s0_l0_tp3 -> dp0_gate_s0_l0;
	dp0_gate_s0_l0 -> dp0_expert0_s0_l0;
	dp0_gate_s0_l0 -> dp0_expert1_s0_l0;
	dp0_gate_s0_l0 -> dp0_expert2_s0_l0;
	dp0_gate_s0_l0 -> dp0_expert3_s0_l0;

	// Expert all-reduce
	dp0_expert_ar_s0_l0 [fillcolor=lightblue, label="Expert All-Reduce L0\n[batch_size=256, seq_len=2048, d_model=4096]\nGPU: GPU0-3", shape=ellipse];
	dp0_expert0_s0_l0 -> dp0_expert_ar_s0_l0;
	dp0_expert1_s0_l0 -> dp0_expert_ar_s0_l0;
	dp0_expert2_s0_l0 -> dp0_expert_ar_s0_l0;
	dp0_expert3_s0_l0 -> dp0_expert_ar_s0_l0;

	// Pipeline communication to Stage 1
	dp0_pp_s0_to_s1 [fillcolor=lightblue, label="Pipeline Stage 0→1\n[batch_size=256, seq_len=2048, d_model=4096]\nGPU: GPU3→GPU8", shape=ellipse];
	dp0_expert_ar_s0_l0 -> dp0_pp_s0_to_s1;

	// DP1 Pipeline Stage 0 (GPUs 16-23) - Mirror structure
	dp1_q_proj_s0_l0_tp0 [fillcolor=lightgreen, label="Q Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	dp1_k_proj_s0_l0_tp0 [fillcolor=lightgreen, label="K Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	dp1_v_proj_s0_l0_tp0 [fillcolor=lightgreen, label="V Projection L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	dp_split -> dp1_q_proj_s0_l0_tp0;
	dp_split -> dp1_k_proj_s0_l0_tp0;
	dp_split -> dp1_v_proj_s0_l0_tp0;

	// Complete pipeline stages 1-3 for DP0 (simplified representation)
	dp0_stage1 [fillcolor=lightgreen, label="Pipeline Stage 1\n[batch_size=256, seq_len=2048, d_model=4096]\nGPU: GPU8-11", shape=rectangle];
	dp0_stage2 [fillcolor=lightgreen, label="Pipeline Stage 2\n[batch_size=256, seq_len=2048, d_model=4096]\nGPU: GPU12-15", shape=rectangle];
	dp0_stage3 [fillcolor=lightgreen, label="Pipeline Stage 3\n[batch_size=256, seq_len=2048, d_model=4096]\nGPU: GPU16-19", shape=rectangle];
	dp0_pp_s0_to_s1 -> dp0_stage1;
	dp0_stage1 -> dp0_stage2;
	dp0_stage2 -> dp0_stage3;

	// Final output
	output [fillcolor=lightcoral, fontsize=12, label="Output\n[batch_size=512, seq_len=2048, d_model=4096]\nGPU: Host", shape=ellipse];
	dp0_stage3 -> output;

	// Ensure all nodes have proper connectivity - fix isolated nodes
	dp1_qk_s0_l0_tp0 [fillcolor=lightgreen, label="QK Matrix Multiply L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	dp1_softmax_s0_l0_tp0 [fillcolor=lightgreen, label="Softmax L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	dp1_attn_out_s0_l0_tp0 [fillcolor=lightgreen, label="Attention Output L0\n[batch_size=256, seq_len=2048, heads=16, d_k=64]\nGPU: GPU16", shape=rectangle];
	
	dp1_q_proj_s0_l0_tp0 -> dp1_qk_s0_l0_tp0;
	dp1_k_proj_s0_l0_tp0 -> dp1_qk_s0_l0_tp0;
	dp1_qk_s0_l0_tp0 -> dp1_softmax_s0_l0_tp0;
	dp1_softmax_s0_l0_tp0 -> dp1_attn_out_s0_l0_tp0;
	dp1_v_proj_s0_l0_tp0 -> dp1_attn_out_s0_l0_tp0;

	// Connect DP1 to final output as well
	dp1_attn_out_s0_l0_tp0 -> output;

	// Rank constraints for better layout
	rank=same { dp0_q_proj_s0_l0_tp0; dp0_k_proj_s0_l0_tp0; dp0_v_proj_s0_l0_tp0; }
	rank=same { dp0_expert0_s0_l0; dp0_expert1_s0_l0; dp0_expert2_s0_l0; dp0_expert3_s0_l0; }
	rank=same { dp1_q_proj_s0_l0_tp0; dp1_k_proj_s0_l0_tp0; dp1_v_proj_s0_l0_tp0; }
}