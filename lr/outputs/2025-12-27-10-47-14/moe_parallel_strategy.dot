// MoE Parallel Strategy DAG
digraph {
	fontsize=12 rankdir=TB size="20,30"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=9]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input\nGPU Input\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightblue shape=ellipse]
	input -> mha_layer_0
	mha_layer_0 -> gate_layer_0
	gate_layer_0 -> route_comm_layer_0_expert_0_tp_0 [style=dashed]
	route_comm_layer_0_expert_0_tp_0 -> expert_layer_0_expert_0_tp_0
	gate_layer_0 -> route_comm_layer_0_expert_0_tp_1 [style=dashed]
	route_comm_layer_0_expert_0_tp_1 -> expert_layer_0_expert_0_tp_1
	expert_layer_0_expert_0_tp_0 -> tp_comm_layer_0_expert_0
	expert_layer_0_expert_0_tp_1 -> tp_comm_layer_0_expert_0
	gate_layer_0 -> route_comm_layer_0_expert_1_tp_0 [style=dashed]
	route_comm_layer_0_expert_1_tp_0 -> expert_layer_0_expert_1_tp_0
	gate_layer_0 -> route_comm_layer_0_expert_1_tp_1 [style=dashed]
	route_comm_layer_0_expert_1_tp_1 -> expert_layer_0_expert_1_tp_1
	expert_layer_0_expert_1_tp_0 -> tp_comm_layer_0_expert_1
	expert_layer_0_expert_1_tp_1 -> tp_comm_layer_0_expert_1
	gate_layer_0 -> route_comm_layer_0_expert_8_tp_0 [style=dashed]
	route_comm_layer_0_expert_8_tp_0 -> expert_layer_0_expert_8_tp_0
	gate_layer_0 -> route_comm_layer_0_expert_8_tp_1 [style=dashed]
	route_comm_layer_0_expert_8_tp_1 -> expert_layer_0_expert_8_tp_1
	expert_layer_0_expert_8_tp_0 -> tp_comm_layer_0_expert_8
	expert_layer_0_expert_8_tp_1 -> tp_comm_layer_0_expert_8
	gate_layer_0 -> route_comm_layer_0_expert_15_tp_0 [style=dashed]
	route_comm_layer_0_expert_15_tp_0 -> expert_layer_0_expert_15_tp_0
	gate_layer_0 -> route_comm_layer_0_expert_15_tp_1 [style=dashed]
	route_comm_layer_0_expert_15_tp_1 -> expert_layer_0_expert_15_tp_1
	expert_layer_0_expert_15_tp_0 -> tp_comm_layer_0_expert_15
	expert_layer_0_expert_15_tp_1 -> tp_comm_layer_0_expert_15
	tp_comm_layer_0_expert_0 -> agg_layer_0
	tp_comm_layer_0_expert_1 -> agg_layer_0
	tp_comm_layer_0_expert_8 -> agg_layer_0
	tp_comm_layer_0_expert_15 -> agg_layer_0
	agg_layer_0 -> mha_layer_1
	mha_layer_1 -> gate_layer_1
	gate_layer_1 -> route_comm_layer_1_expert_0_tp_0 [style=dashed]
	route_comm_layer_1_expert_0_tp_0 -> expert_layer_1_expert_0_tp_0
	gate_layer_1 -> route_comm_layer_1_expert_0_tp_1 [style=dashed]
	route_comm_layer_1_expert_0_tp_1 -> expert_layer_1_expert_0_tp_1
	expert_layer_1_expert_0_tp_0 -> tp_comm_layer_1_expert_0
	expert_layer_1_expert_0_tp_1 -> tp_comm_layer_1_expert_0
	gate_layer_1 -> route_comm_layer_1_expert_1_tp_0 [style=dashed]
	route_comm_layer_1_expert_1_tp_0 -> expert_layer_1_expert_1_tp_0
	gate_layer_1 -> route_comm_layer_1_expert_1_tp_1 [style=dashed]
	route_comm_layer_1_expert_1_tp_1 -> expert_layer_1_expert_1_tp_1
	expert_layer_1_expert_1_tp_0 -> tp_comm_layer_1_expert_1
	expert_layer_1_expert_1_tp_1 -> tp_comm_layer_1_expert_1
	gate_layer_1 -> route_comm_layer_1_expert_8_tp_0 [style=dashed]
	route_comm_layer_1_expert_8_tp_0 -> expert_layer_1_expert_8_tp_0
	gate_layer_1 -> route_comm_layer_1_expert_8_tp_1 [style=dashed]
	route_comm_layer_1_expert_8_tp_1 -> expert_layer_1_expert_8_tp_1
	expert_layer_1_expert_8_tp_0 -> tp_comm_layer_1_expert_8
	expert_layer_1_expert_8_tp_1 -> tp_comm_layer_1_expert_8
	gate_layer_1 -> route_comm_layer_1_expert_15_tp_0 [style=dashed]
	route_comm_layer_1_expert_15_tp_0 -> expert_layer_1_expert_15_tp_0
	gate_layer_1 -> route_comm_layer_1_expert_15_tp_1 [style=dashed]
	route_comm_layer_1_expert_15_tp_1 -> expert_layer_1_expert_15_tp_1
	expert_layer_1_expert_15_tp_0 -> tp_comm_layer_1_expert_15
	expert_layer_1_expert_15_tp_1 -> tp_comm_layer_1_expert_15
	tp_comm_layer_1_expert_0 -> agg_layer_1
	tp_comm_layer_1_expert_1 -> agg_layer_1
	tp_comm_layer_1_expert_8 -> agg_layer_1
	tp_comm_layer_1_expert_15 -> agg_layer_1
	agg_layer_1 -> mha_layer_2
	mha_layer_2 -> gate_layer_2
	gate_layer_2 -> route_comm_layer_2_expert_0_tp_0 [style=dashed]
	route_comm_layer_2_expert_0_tp_0 -> expert_layer_2_expert_0_tp_0
	gate_layer_2 -> route_comm_layer_2_expert_0_tp_1 [style=dashed]
	route_comm_layer_2_expert_0_tp_1 -> expert_layer_2_expert_0_tp_1
	expert_layer_2_expert_0_tp_0 -> tp_comm_layer_2_expert_0
	expert_layer_2_expert_0_tp_1 -> tp_comm_layer_2_expert_0
	gate_layer_2 -> route_comm_layer_2_expert_1_tp_0 [style=dashed]
	route_comm_layer_2_expert_1_tp_0 -> expert_layer_2_expert_1_tp_0
	gate_layer_2 -> route_comm_layer_2_expert_1_tp_1 [style=dashed]
	route_comm_layer_2_expert_1_tp_1 -> expert_layer_2_expert_1_tp_1
	expert_layer_2_expert_1_tp_0 -> tp_comm_layer_2_expert_1
	expert_layer_2_expert_1_tp_1 -> tp_comm_layer_2_expert_1
	gate_layer_2 -> route_comm_layer_2_expert_8_tp_0 [style=dashed]
	route_comm_layer_2_expert_8_tp_0 -> expert_layer_2_expert_8_tp_0
	gate_layer_2 -> route_comm_layer_2_expert_8_tp_1 [style=dashed]
	route_comm_layer_2_expert_8_tp_1 -> expert_layer_2_expert_8_tp_1
	expert_layer_2_expert_8_tp_0 -> tp_comm_layer_2_expert_8
	expert_layer_2_expert_8_tp_1 -> tp_comm_layer_2_expert_8
	gate_layer_2 -> route_comm_layer_2_expert_15_tp_0 [style=dashed]
	route_comm_layer_2_expert_15_tp_0 -> expert_layer_2_expert_15_tp_0
	gate_layer_2 -> route_comm_layer_2_expert_15_tp_1 [style=dashed]
	route_comm_layer_2_expert_15_tp_1 -> expert_layer_2_expert_15_tp_1
	expert_layer_2_expert_15_tp_0 -> tp_comm_layer_2_expert_15
	expert_layer_2_expert_15_tp_1 -> tp_comm_layer_2_expert_15
	tp_comm_layer_2_expert_0 -> agg_layer_2
	tp_comm_layer_2_expert_1 -> agg_layer_2
	tp_comm_layer_2_expert_8 -> agg_layer_2
	tp_comm_layer_2_expert_15 -> agg_layer_2
	agg_layer_2 -> mha_layer_3
	mha_layer_3 -> gate_layer_3
	gate_layer_3 -> route_comm_layer_3_expert_0_tp_0 [style=dashed]
	route_comm_layer_3_expert_0_tp_0 -> expert_layer_3_expert_0_tp_0
	gate_layer_3 -> route_comm_layer_3_expert_0_tp_1 [style=dashed]
	route_comm_layer_3_expert_0_tp_1 -> expert_layer_3_expert_0_tp_1
	expert_layer_3_expert_0_tp_0 -> tp_comm_layer_3_expert_0
	expert_layer_3_expert_0_tp_1 -> tp_comm_layer_3_expert_0
	gate_layer_3 -> route_comm_layer_3_expert_1_tp_0 [style=dashed]
	route_comm_layer_3_expert_1_tp_0 -> expert_layer_3_expert_1_tp_0
	gate_layer_3 -> route_comm_layer_3_expert_1_tp_1 [style=dashed]
	route_comm_layer_3_expert_1_tp_1 -> expert_layer_3_expert_1_tp_1
	expert_layer_3_expert_1_tp_0 -> tp_comm_layer_3_expert_1
	expert_layer_3_expert_1_tp_1 -> tp_comm_layer_3_expert_1
	gate_layer_3 -> route_comm_layer_3_expert_8_tp_0 [style=dashed]
	route_comm_layer_3_expert_8_tp_0 -> expert_layer_3_expert_8_tp_0
	gate_layer_3 -> route_comm_layer_3_expert_8_tp_1 [style=dashed]
	route_comm_layer_3_expert_8_tp_1 -> expert_layer_3_expert_8_tp_1
	expert_layer_3_expert_8_tp_0 -> tp_comm_layer_3_expert_8
	expert_layer_3_expert_8_tp_1 -> tp_comm_layer_3_expert_8
	gate_layer_3 -> route_comm_layer_3_expert_15_tp_0 [style=dashed]
	route_comm_layer_3_expert_15_tp_0 -> expert_layer_3_expert_15_tp_0
	gate_layer_3 -> route_comm_layer_3_expert_15_tp_1 [style=dashed]
	route_comm_layer_3_expert_15_tp_1 -> expert_layer_3_expert_15_tp_1
	expert_layer_3_expert_15_tp_0 -> tp_comm_layer_3_expert_15
	expert_layer_3_expert_15_tp_1 -> tp_comm_layer_3_expert_15
	tp_comm_layer_3_expert_0 -> agg_layer_3
	tp_comm_layer_3_expert_1 -> agg_layer_3
	tp_comm_layer_3_expert_8 -> agg_layer_3
	tp_comm_layer_3_expert_15 -> agg_layer_3
	subgraph cluster_pipeline_0 {
		fillcolor=lightgray fontname="Arial Bold" label="Pipeline Stage 0 (Layers 0-3)\nGPUs 0-31" style="rounded,filled"
		mha_layer_0 [label="MHA Layer 0\nGPU 0\nInput: [batch_size=64, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		gate_layer_0 [label="Gate Layer 0\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, num_experts=16]" fillcolor=lightyellow shape=parallelogram]
		expert_layer_0_expert_0_tp_0 [label="Expert 0 TP0\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_0_tp_0 [label="Route Comm\nGPU Gate -> GPU 0" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_0_tp_1 [label="Expert 0 TP1\nGPU 1\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_0_tp_1 [label="Route Comm\nGPU Gate -> GPU 1" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_0_expert_0 [label="TP AllReduce\nGPU 0 <-> GPU 1" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_1_tp_0 [label="Expert 1 TP0\nGPU 2\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_1_tp_0 [label="Route Comm\nGPU Gate -> GPU 2" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_1_tp_1 [label="Expert 1 TP1\nGPU 3\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_1_tp_1 [label="Route Comm\nGPU Gate -> GPU 3" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_0_expert_1 [label="TP AllReduce\nGPU 2 <-> GPU 3" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_8_tp_0 [label="Expert 8 TP0\nGPU 16\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_8_tp_0 [label="Route Comm\nGPU Gate -> GPU 16" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_8_tp_1 [label="Expert 8 TP1\nGPU 17\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_8_tp_1 [label="Route Comm\nGPU Gate -> GPU 17" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_0_expert_8 [label="TP AllReduce\nGPU 16 <-> GPU 17" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_15_tp_0 [label="Expert 15 TP0\nGPU 30\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_15_tp_0 [label="Route Comm\nGPU Gate -> GPU 30" fillcolor=lightblue shape=ellipse]
		expert_layer_0_expert_15_tp_1 [label="Expert 15 TP1\nGPU 31\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_0_expert_15_tp_1 [label="Route Comm\nGPU Gate -> GPU 31" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_0_expert_15 [label="TP AllReduce\nGPU 30 <-> GPU 31" fillcolor=lightblue shape=ellipse]
		agg_layer_0 [label="Expert Aggregation Layer 0\nGPU 0\nInput: [batch_size=64, seq_len=1024, num_experts=16, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightyellow shape=parallelogram]
		mha_layer_1 [label="MHA Layer 1\nGPU 0\nInput: [batch_size=64, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		gate_layer_1 [label="Gate Layer 1\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, num_experts=16]" fillcolor=lightyellow shape=parallelogram]
		expert_layer_1_expert_0_tp_0 [label="Expert 0 TP0\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_0_tp_0 [label="Route Comm\nGPU Gate -> GPU 0" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_0_tp_1 [label="Expert 0 TP1\nGPU 1\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_0_tp_1 [label="Route Comm\nGPU Gate -> GPU 1" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_1_expert_0 [label="TP AllReduce\nGPU 0 <-> GPU 1" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_1_tp_0 [label="Expert 1 TP0\nGPU 2\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_1_tp_0 [label="Route Comm\nGPU Gate -> GPU 2" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_1_tp_1 [label="Expert 1 TP1\nGPU 3\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_1_tp_1 [label="Route Comm\nGPU Gate -> GPU 3" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_1_expert_1 [label="TP AllReduce\nGPU 2 <-> GPU 3" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_8_tp_0 [label="Expert 8 TP0\nGPU 16\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_8_tp_0 [label="Route Comm\nGPU Gate -> GPU 16" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_8_tp_1 [label="Expert 8 TP1\nGPU 17\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_8_tp_1 [label="Route Comm\nGPU Gate -> GPU 17" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_1_expert_8 [label="TP AllReduce\nGPU 16 <-> GPU 17" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_15_tp_0 [label="Expert 15 TP0\nGPU 30\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_15_tp_0 [label="Route Comm\nGPU Gate -> GPU 30" fillcolor=lightblue shape=ellipse]
		expert_layer_1_expert_15_tp_1 [label="Expert 15 TP1\nGPU 31\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_1_expert_15_tp_1 [label="Route Comm\nGPU Gate -> GPU 31" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_1_expert_15 [label="TP AllReduce\nGPU 30 <-> GPU 31" fillcolor=lightblue shape=ellipse]
		agg_layer_1 [label="Expert Aggregation Layer 1\nGPU 0\nInput: [batch_size=64, seq_len=1024, num_experts=16, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightyellow shape=parallelogram]
		mha_layer_2 [label="MHA Layer 2\nGPU 0\nInput: [batch_size=64, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		gate_layer_2 [label="Gate Layer 2\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, num_experts=16]" fillcolor=lightyellow shape=parallelogram]
		expert_layer_2_expert_0_tp_0 [label="Expert 0 TP0\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_0_tp_0 [label="Route Comm\nGPU Gate -> GPU 0" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_0_tp_1 [label="Expert 0 TP1\nGPU 1\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_0_tp_1 [label="Route Comm\nGPU Gate -> GPU 1" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_2_expert_0 [label="TP AllReduce\nGPU 0 <-> GPU 1" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_1_tp_0 [label="Expert 1 TP0\nGPU 2\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_1_tp_0 [label="Route Comm\nGPU Gate -> GPU 2" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_1_tp_1 [label="Expert 1 TP1\nGPU 3\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_1_tp_1 [label="Route Comm\nGPU Gate -> GPU 3" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_2_expert_1 [label="TP AllReduce\nGPU 2 <-> GPU 3" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_8_tp_0 [label="Expert 8 TP0\nGPU 16\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_8_tp_0 [label="Route Comm\nGPU Gate -> GPU 16" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_8_tp_1 [label="Expert 8 TP1\nGPU 17\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_8_tp_1 [label="Route Comm\nGPU Gate -> GPU 17" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_2_expert_8 [label="TP AllReduce\nGPU 16 <-> GPU 17" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_15_tp_0 [label="Expert 15 TP0\nGPU 30\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_15_tp_0 [label="Route Comm\nGPU Gate -> GPU 30" fillcolor=lightblue shape=ellipse]
		expert_layer_2_expert_15_tp_1 [label="Expert 15 TP1\nGPU 31\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_2_expert_15_tp_1 [label="Route Comm\nGPU Gate -> GPU 31" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_2_expert_15 [label="TP AllReduce\nGPU 30 <-> GPU 31" fillcolor=lightblue shape=ellipse]
		agg_layer_2 [label="Expert Aggregation Layer 2\nGPU 0\nInput: [batch_size=64, seq_len=1024, num_experts=16, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightyellow shape=parallelogram]
		mha_layer_3 [label="MHA Layer 3\nGPU 0\nInput: [batch_size=64, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		gate_layer_3 [label="Gate Layer 3\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, num_experts=16]" fillcolor=lightyellow shape=parallelogram]
		expert_layer_3_expert_0_tp_0 [label="Expert 0 TP0\nGPU 0\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_0_tp_0 [label="Route Comm\nGPU Gate -> GPU 0" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_0_tp_1 [label="Expert 0 TP1\nGPU 1\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_0_tp_1 [label="Route Comm\nGPU Gate -> GPU 1" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_3_expert_0 [label="TP AllReduce\nGPU 0 <-> GPU 1" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_1_tp_0 [label="Expert 1 TP0\nGPU 2\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_1_tp_0 [label="Route Comm\nGPU Gate -> GPU 2" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_1_tp_1 [label="Expert 1 TP1\nGPU 3\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_1_tp_1 [label="Route Comm\nGPU Gate -> GPU 3" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_3_expert_1 [label="TP AllReduce\nGPU 2 <-> GPU 3" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_8_tp_0 [label="Expert 8 TP0\nGPU 16\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_8_tp_0 [label="Route Comm\nGPU Gate -> GPU 16" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_8_tp_1 [label="Expert 8 TP1\nGPU 17\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_8_tp_1 [label="Route Comm\nGPU Gate -> GPU 17" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_3_expert_8 [label="TP AllReduce\nGPU 16 <-> GPU 17" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_15_tp_0 [label="Expert 15 TP0\nGPU 30\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_15_tp_0 [label="Route Comm\nGPU Gate -> GPU 30" fillcolor=lightblue shape=ellipse]
		expert_layer_3_expert_15_tp_1 [label="Expert 15 TP1\nGPU 31\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightgreen shape=rectangle]
		route_comm_layer_3_expert_15_tp_1 [label="Route Comm\nGPU Gate -> GPU 31" fillcolor=lightblue shape=ellipse]
		tp_comm_layer_3_expert_15 [label="TP AllReduce\nGPU 30 <-> GPU 31" fillcolor=lightblue shape=ellipse]
		agg_layer_3 [label="Expert Aggregation Layer 3\nGPU 0\nInput: [batch_size=64, seq_len=1024, num_experts=16, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, hidden_dim=512]" fillcolor=lightyellow shape=parallelogram]
	}
	pp_comm_stage0_to_stage1 [label="PP Comm Stage0->Stage1\nGPU 0 -> GPU 32" fillcolor=lightblue shape=ellipse]
	agg_layer_3 -> pp_comm_stage0_to_stage1
	output [label="Output\nGPU Output\nInput: [batch_size=64, seq_len=1024, hidden_dim=512]\nOutput: [batch_size=64, seq_len=1024, vocab_size=?]" fillcolor=lightblue shape=ellipse]
	agg_layer_3 -> output [label="after all stages" style=dashed]
}