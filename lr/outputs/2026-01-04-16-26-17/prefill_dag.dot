// Prefill Phase DAG - Qwen3-235B
digraph {
	rankdir=TB size="20,30"
	node [shape=rectangle style=filled]
	input [label="Input Embedding
GPU: 0-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightgray shape=ellipse]
	subgraph cluster_stage0 {
		fillcolor=lightblue label="Stage 0: Layers 0-23
GPUs 0-8" rank=same style=rounded
		stage0_layer0_attn [label="Layer 0 Attention
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightblue]
		stage0_layer0_gate [label="Gate Selection (Top-8)
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage0_layer0_moe [label="Layer 0 MoE (128 experts)
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightblue]
		stage0_layer23_attn [label="Layer 23 Attention
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightblue]
		stage0_layer23_gate [label="Gate Selection (Top-8)
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage0_layer23_moe [label="Layer 23 MoE (128 experts)
GPU: 0-8
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightblue]
	}
	subgraph cluster_stage1 {
		fillcolor=lightgreen label="Stage 1: Layers 24-47
GPUs 9-17" rank=same style=rounded
		stage1_layer24_attn [label="Layer 24 Attention
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightgreen]
		stage1_layer24_gate [label="Gate Selection (Top-8)
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage1_layer24_moe [label="Layer 24 MoE (128 experts)
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightgreen]
		stage1_layer47_attn [label="Layer 47 Attention
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightgreen]
		stage1_layer47_gate [label="Gate Selection (Top-8)
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage1_layer47_moe [label="Layer 47 MoE (128 experts)
GPU: 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightgreen]
	}
	subgraph cluster_stage2 {
		fillcolor=lightyellow label="Stage 2: Layers 48-71
GPUs 18-26" rank=same style=rounded
		stage2_layer48_attn [label="Layer 48 Attention
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightyellow]
		stage2_layer48_gate [label="Gate Selection (Top-8)
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage2_layer48_moe [label="Layer 48 MoE (128 experts)
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightyellow]
		stage2_layer71_attn [label="Layer 71 Attention
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightyellow]
		stage2_layer71_gate [label="Gate Selection (Top-8)
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage2_layer71_moe [label="Layer 71 MoE (128 experts)
GPU: 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightyellow]
	}
	subgraph cluster_stage3 {
		fillcolor=lightcoral label="Stage 3: Layers 72-93
GPUs 27-34" rank=same style=rounded
		stage3_layer72_attn [label="Layer 72 Attention
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightcoral]
		stage3_layer72_gate [label="Gate Selection (Top-8)
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage3_layer72_moe [label="Layer 72 MoE (128 experts)
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightcoral]
		stage3_layer93_attn [label="Layer 93 Attention
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightcoral]
		stage3_layer93_gate [label="Gate Selection (Top-8)
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, top_k=8]" fillcolor=orange shape=parallelogram]
		stage3_layer93_moe [label="Layer 93 MoE (128 experts)
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=lightcoral]
	}
	output [label="Output Projection
GPU: 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, vocab_size=151936]" fillcolor=lightgray shape=ellipse]
	comm_0_1 [label="Pipeline Communication
GPUs: 0-8 → 9-17
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
	comm_1_2 [label="Pipeline Communication
GPUs: 9-17 → 18-26
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
	comm_2_3 [label="Pipeline Communication
GPUs: 18-26 → 27-34
Input: [batch_size=128, seq_len=2048, d_model=4096]
Output: [batch_size=128, seq_len=2048, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
	input -> stage0_layer0_attn
	stage0_layer0_attn -> stage0_layer0_gate
	stage0_layer0_gate -> stage0_layer0_moe [label="Top-8 expert selection" style=dashed]
	stage0_layer0_moe -> stage0_layer23_attn [label="Layers 1-22"]
	stage0_layer23_attn -> stage0_layer23_gate
	stage0_layer23_gate -> stage0_layer23_moe [label="Top-8 expert selection" style=dashed]
	stage0_layer23_moe -> comm_0_1
	comm_0_1 -> stage1_layer24_attn
	stage1_layer24_attn -> stage1_layer24_gate
	stage1_layer24_gate -> stage1_layer24_moe [label="Top-8 expert selection" style=dashed]
	stage1_layer47_attn -> stage1_layer47_gate
	stage1_layer47_gate -> stage1_layer47_moe [label="Top-8 expert selection" style=dashed]
	stage1_layer47_moe -> comm_1_2
	comm_1_2 -> stage2_layer48_attn
	stage2_layer48_attn -> stage2_layer48_gate
	stage2_layer48_gate -> stage2_layer48_moe [label="Top-8 expert selection" style=dashed]
	stage2_layer71_attn -> stage2_layer71_gate
	stage2_layer71_gate -> stage2_layer71_moe [label="Top-8 expert selection" style=dashed]
	stage2_layer71_moe -> comm_2_3
	comm_2_3 -> stage3_layer72_attn
	stage3_layer72_attn -> stage3_layer72_gate
	stage3_layer72_gate -> stage3_layer72_moe [label="Top-8 expert selection" style=dashed]
	stage3_layer93_attn -> stage3_layer93_gate
	stage3_layer93_gate -> stage3_layer93_moe [label="Top-8 expert selection" style=dashed]
	stage3_layer93_moe -> output
}
