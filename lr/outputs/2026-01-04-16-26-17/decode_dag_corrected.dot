// Decode Phase DAG - Qwen3-235B (Corrected)
digraph {
    rankdir=TB size="30,40"
    node [shape=rectangle style=filled]
    
    // Input node
    input [label="Input Token Embedding\nGPU: 0-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgray shape=ellipse]
    
    // KV Cache node
    kv_cache [label="KV Cache Storage\nGPU: 0-34\nCache: [batch_size=128, seq_len=2048, heads=4, d_k=64]\nHit Rate: 95%+" fillcolor=lightpink shape=parallelogram]
    
    // Stage 0: Layers 0-23 (GPUs 0-8)
    subgraph cluster_stage0 {
        fillcolor=lightblue label="Stage 0: Layers 0-23\nGPUs 0-8" style=rounded
        
        // Layer 0
        layer0_qkv_proj [label="Layer 0 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer0_attn_comp [label="Layer 0 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer0_out_proj [label="Layer 0 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer0_gate [label="Layer 0 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer0_moe [label="Layer 0 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 1
        layer1_qkv_proj [label="Layer 1 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer1_attn_comp [label="Layer 1 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer1_out_proj [label="Layer 1 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer1_gate [label="Layer 1 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer1_moe [label="Layer 1 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 2
        layer2_qkv_proj [label="Layer 2 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer2_attn_comp [label="Layer 2 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer2_out_proj [label="Layer 2 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer2_gate [label="Layer 2 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer2_moe [label="Layer 2 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 3
        layer3_qkv_proj [label="Layer 3 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer3_attn_comp [label="Layer 3 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer3_out_proj [label="Layer 3 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer3_gate [label="Layer 3 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer3_moe [label="Layer 3 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 4
        layer4_qkv_proj [label="Layer 4 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer4_attn_comp [label="Layer 4 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer4_out_proj [label="Layer 4 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer4_gate [label="Layer 4 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer4_moe [label="Layer 4 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 5
        layer5_qkv_proj [label="Layer 5 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer5_attn_comp [label="Layer 5 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer5_out_proj [label="Layer 5 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer5_gate [label="Layer 5 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer5_moe [label="Layer 5 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 6
        layer6_qkv_proj [label="Layer 6 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer6_attn_comp [label="Layer 6 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer6_out_proj [label="Layer 6 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer6_gate [label="Layer 6 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer6_moe [label="Layer 6 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 7
        layer7_qkv_proj [label="Layer 7 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer7_attn_comp [label="Layer 7 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer7_out_proj [label="Layer 7 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer7_gate [label="Layer 7 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer7_moe [label="Layer 7 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 8
        layer8_qkv_proj [label="Layer 8 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer8_attn_comp [label="Layer 8 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer8_out_proj [label="Layer 8 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer8_gate [label="Layer 8 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer8_moe [label="Layer 8 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 9
        layer9_qkv_proj [label="Layer 9 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer9_attn_comp [label="Layer 9 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer9_out_proj [label="Layer 9 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer9_gate [label="Layer 9 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer9_moe [label="Layer 9 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 10
        layer10_qkv_proj [label="Layer 10 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer10_attn_comp [label="Layer 10 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer10_out_proj [label="Layer 10 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer10_gate [label="Layer 10 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer10_moe [label="Layer 10 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 11
        layer11_qkv_proj [label="Layer 11 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer11_attn_comp [label="Layer 11 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer11_out_proj [label="Layer 11 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer11_gate [label="Layer 11 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer11_moe [label="Layer 11 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 12
        layer12_qkv_proj [label="Layer 12 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer12_attn_comp [label="Layer 12 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer12_out_proj [label="Layer 12 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer12_gate [label="Layer 12 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer12_moe [label="Layer 12 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 13
        layer13_qkv_proj [label="Layer 13 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer13_attn_comp [label="Layer 13 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer13_out_proj [label="Layer 13 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer13_gate [label="Layer 13 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer13_moe [label="Layer 13 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 14
        layer14_qkv_proj [label="Layer 14 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer14_attn_comp [label="Layer 14 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer14_out_proj [label="Layer 14 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer14_gate [label="Layer 14 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer14_moe [label="Layer 14 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 15
        layer15_qkv_proj [label="Layer 15 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer15_attn_comp [label="Layer 15 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer15_out_proj [label="Layer 15 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer15_gate [label="Layer 15 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer15_moe [label="Layer 15 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 16
        layer16_qkv_proj [label="Layer 16 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer16_attn_comp [label="Layer 16 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer16_out_proj [label="Layer 16 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer16_gate [label="Layer 16 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer16_moe [label="Layer 16 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 17
        layer17_qkv_proj [label="Layer 17 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer17_attn_comp [label="Layer 17 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer17_out_proj [label="Layer 17 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer17_gate [label="Layer 17 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer17_moe [label="Layer 17 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 18
        layer18_qkv_proj [label="Layer 18 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer18_attn_comp [label="Layer 18 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer18_out_proj [label="Layer 18 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer18_gate [label="Layer 18 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer18_moe [label="Layer 18 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 19
        layer19_qkv_proj [label="Layer 19 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer19_attn_comp [label="Layer 19 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer19_out_proj [label="Layer 19 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer19_gate [label="Layer 19 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer19_moe [label="Layer 19 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 20
        layer20_qkv_proj [label="Layer 20 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer20_attn_comp [label="Layer 20 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer20_out_proj [label="Layer 20 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer20_gate [label="Layer 20 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer20_moe [label="Layer 20 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 21
        layer21_qkv_proj [label="Layer 21 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer21_attn_comp [label="Layer 21 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer21_out_proj [label="Layer 21 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer21_gate [label="Layer 21 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer21_moe [label="Layer 21 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 22
        layer22_qkv_proj [label="Layer 22 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer22_attn_comp [label="Layer 22 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer22_out_proj [label="Layer 22 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer22_gate [label="Layer 22 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer22_moe [label="Layer 22 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        
        // Layer 23
        layer23_qkv_proj [label="Layer 23 QKV Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer23_attn_comp [label="Layer 23 Attention Computation + KV Cache\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightblue]
        layer23_out_proj [label="Layer 23 Output Projection\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
        layer23_gate [label="Layer 23 Gate Selection (Top-8)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer23_moe [label="Layer 23 MoE (128 experts)\nGPU: 0-8\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightblue]
    }
    
    // KV Cache access nodes
    kv_access_0 [label="KV Cache Access\nGPU: 0-8\nCache: [batch_size=128, seq_len=2048, heads=4, d_k=64]" fillcolor=lightcyan shape=parallelogram]
    
    // Communication between stages
    comm_0_1 [label="Pipeline Communication (Single Token)\nGPUs: 0-8 â†’ 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
    
    // Connections for Stage 0
    input -> kv_cache
    kv_cache -> kv_access_0
    kv_access_0 -> layer0_qkv_proj
    layer0_qkv_proj -> layer0_attn_comp
    layer0_attn_comp -> layer0_out_proj
    layer0_out_proj -> layer0_gate
    layer0_gate -> layer0_moe [style=dashed]
    
    layer0_moe -> layer1_qkv_proj
    layer1_qkv_proj -> layer1_attn_comp
    layer1_attn_comp -> layer1_out_proj
    layer1_out_proj -> layer1_gate
    layer1_gate -> layer1_moe [style=dashed]
        
    layer1_moe -> layer2_qkv_proj
    layer2_qkv_proj -> layer2_attn_comp
    layer2_attn_comp -> layer2_out_proj
    layer2_out_proj -> layer2_gate
    layer2_gate -> layer2_moe [style=dashed]
        
    layer2_moe -> layer3_qkv_proj
    layer3_qkv_proj -> layer3_attn_comp
    layer3_attn_comp -> layer3_out_proj
    layer3_out_proj -> layer3_gate
    layer3_gate -> layer3_moe [style=dashed]
        
    layer3_moe -> layer4_qkv_proj
    layer4_qkv_proj -> layer4_attn_comp
    layer4_attn_comp -> layer4_out_proj
    layer4_out_proj -> layer4_gate
    layer4_gate -> layer4_moe [style=dashed]
        
    layer4_moe -> layer5_qkv_proj
    layer5_qkv_proj -> layer5_attn_comp
    layer5_attn_comp -> layer5_out_proj
    layer5_out_proj -> layer5_gate
    layer5_gate -> layer5_moe [style=dashed]
        
    layer5_moe -> layer6_qkv_proj
    layer6_qkv_proj -> layer6_attn_comp
    layer6_attn_comp -> layer6_out_proj
    layer6_out_proj -> layer6_gate
    layer6_gate -> layer6_moe [style=dashed]
        
    layer6_moe -> layer7_qkv_proj
    layer7_qkv_proj -> layer7_attn_comp
    layer7_attn_comp -> layer7_out_proj
    layer7_out_proj -> layer7_gate
    layer7_gate -> layer7_moe [style=dashed]
        
    layer7_moe -> layer8_qkv_proj
    layer8_qkv_proj -> layer8_attn_comp
    layer8_attn_comp -> layer8_out_proj
    layer8_out_proj -> layer8_gate
    layer8_gate -> layer8_moe [style=dashed]
        
    layer8_moe -> layer9_qkv_proj
    layer9_qkv_proj -> layer9_attn_comp
    layer9_attn_comp -> layer9_out_proj
    layer9_out_proj -> layer9_gate
    layer9_gate -> layer9_moe [style=dashed]
        
    layer9_moe -> layer10_qkv_proj
    layer10_qkv_proj -> layer10_attn_comp
    layer10_attn_comp -> layer10_out_proj
    layer10_out_proj -> layer10_gate
    layer10_gate -> layer10_moe [style=dashed]
        
    layer10_moe -> layer11_qkv_proj
    layer11_qkv_proj -> layer11_attn_comp
    layer11_attn_comp -> layer11_out_proj
    layer11_out_proj -> layer11_gate
    layer11_gate -> layer11_moe [style=dashed]
        
    layer11_moe -> layer12_qkv_proj
    layer12_qkv_proj -> layer12_attn_comp
    layer12_attn_comp -> layer12_out_proj
    layer12_out_proj -> layer12_gate
    layer12_gate -> layer12_moe [style=dashed]
        
    layer12_moe -> layer13_qkv_proj
    layer13_qkv_proj -> layer13_attn_comp
    layer13_attn_comp -> layer13_out_proj
    layer13_out_proj -> layer13_gate
    layer13_gate -> layer13_moe [style=dashed]
        
    layer13_moe -> layer14_qkv_proj
    layer14_qkv_proj -> layer14_attn_comp
    layer14_attn_comp -> layer14_out_proj
    layer14_out_proj -> layer14_gate
    layer14_gate -> layer14_moe [style=dashed]
        
    layer14_moe -> layer15_qkv_proj
    layer15_qkv_proj -> layer15_attn_comp
    layer15_attn_comp -> layer15_out_proj
    layer15_out_proj -> layer15_gate
    layer15_gate -> layer15_moe [style=dashed]
        
    layer15_moe -> layer16_qkv_proj
    layer16_qkv_proj -> layer16_attn_comp
    layer16_attn_comp -> layer16_out_proj
    layer16_out_proj -> layer16_gate
    layer16_gate -> layer16_moe [style=dashed]
        
    layer16_moe -> layer17_qkv_proj
    layer17_qkv_proj -> layer17_attn_comp
    layer17_attn_comp -> layer17_out_proj
    layer17_out_proj -> layer17_gate
    layer17_gate -> layer17_moe [style=dashed]
        
    layer17_moe -> layer18_qkv_proj
    layer18_qkv_proj -> layer18_attn_comp
    layer18_attn_comp -> layer18_out_proj
    layer18_out_proj -> layer18_gate
    layer18_gate -> layer18_moe [style=dashed]
        
    layer18_moe -> layer19_qkv_proj
    layer19_qkv_proj -> layer19_attn_comp
    layer19_attn_comp -> layer19_out_proj
    layer19_out_proj -> layer19_gate
    layer19_gate -> layer19_moe [style=dashed]
        
    layer19_moe -> layer20_qkv_proj
    layer20_qkv_proj -> layer20_attn_comp
    layer20_attn_comp -> layer20_out_proj
    layer20_out_proj -> layer20_gate
    layer20_gate -> layer20_moe [style=dashed]
        
    layer20_moe -> layer21_qkv_proj
    layer21_qkv_proj -> layer21_attn_comp
    layer21_attn_comp -> layer21_out_proj
    layer21_out_proj -> layer21_gate
    layer21_gate -> layer21_moe [style=dashed]
        
    layer21_moe -> layer22_qkv_proj
    layer22_qkv_proj -> layer22_attn_comp
    layer22_attn_comp -> layer22_out_proj
    layer22_out_proj -> layer22_gate
    layer22_gate -> layer22_moe [style=dashed]
        
    layer22_moe -> layer23_qkv_proj
    layer23_qkv_proj -> layer23_attn_comp
    layer23_attn_comp -> layer23_out_proj
    layer23_out_proj -> layer23_gate
    layer23_gate -> layer23_moe [style=dashed]
    layer23_moe -> comm_0_1
    
    // Stage 1: Layers 24-47 (GPUs 9-17)
    subgraph cluster_stage1 {
        fillcolor=lightgreen label="Stage 1: Layers 24-47\nGPUs 9-17" style=rounded
        
        // Layer 24
        layer24_qkv_proj [label="Layer 24 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer24_attn_comp [label="Layer 24 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer24_out_proj [label="Layer 24 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer24_gate [label="Layer 24 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer24_moe [label="Layer 24 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 25
        layer25_qkv_proj [label="Layer 25 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer25_attn_comp [label="Layer 25 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer25_out_proj [label="Layer 25 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer25_gate [label="Layer 25 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer25_moe [label="Layer 25 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 26
        layer26_qkv_proj [label="Layer 26 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer26_attn_comp [label="Layer 26 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer26_out_proj [label="Layer 26 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer26_gate [label="Layer 26 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer26_moe [label="Layer 26 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 27
        layer27_qkv_proj [label="Layer 27 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer27_attn_comp [label="Layer 27 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer27_out_proj [label="Layer 27 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer27_gate [label="Layer 27 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer27_moe [label="Layer 27 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 28
        layer28_qkv_proj [label="Layer 28 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer28_attn_comp [label="Layer 28 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer28_out_proj [label="Layer 28 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer28_gate [label="Layer 28 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer28_moe [label="Layer 28 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 29
        layer29_qkv_proj [label="Layer 29 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer29_attn_comp [label="Layer 29 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer29_out_proj [label="Layer 29 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer29_gate [label="Layer 29 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer29_moe [label="Layer 29 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 30
        layer30_qkv_proj [label="Layer 30 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer30_attn_comp [label="Layer 30 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer30_out_proj [label="Layer 30 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer30_gate [label="Layer 30 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer30_moe [label="Layer 30 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 31
        layer31_qkv_proj [label="Layer 31 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer31_attn_comp [label="Layer 31 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer31_out_proj [label="Layer 31 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer31_gate [label="Layer 31 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer31_moe [label="Layer 31 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 32
        layer32_qkv_proj [label="Layer 32 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer32_attn_comp [label="Layer 32 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer32_out_proj [label="Layer 32 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer32_gate [label="Layer 32 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer32_moe [label="Layer 32 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 33
        layer33_qkv_proj [label="Layer 33 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer33_attn_comp [label="Layer 33 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer33_out_proj [label="Layer 33 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer33_gate [label="Layer 33 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer33_moe [label="Layer 33 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 34
        layer34_qkv_proj [label="Layer 34 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer34_attn_comp [label="Layer 34 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer34_out_proj [label="Layer 34 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer34_gate [label="Layer 34 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer34_moe [label="Layer 34 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 35
        layer35_qkv_proj [label="Layer 35 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer35_attn_comp [label="Layer 35 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer35_out_proj [label="Layer 35 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer35_gate [label="Layer 35 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer35_moe [label="Layer 35 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 36
        layer36_qkv_proj [label="Layer 36 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer36_attn_comp [label="Layer 36 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer36_out_proj [label="Layer 36 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer36_gate [label="Layer 36 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer36_moe [label="Layer 36 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 37
        layer37_qkv_proj [label="Layer 37 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer37_attn_comp [label="Layer 37 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer37_out_proj [label="Layer 37 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer37_gate [label="Layer 37 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer37_moe [label="Layer 37 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 38
        layer38_qkv_proj [label="Layer 38 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer38_attn_comp [label="Layer 38 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer38_out_proj [label="Layer 38 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer38_gate [label="Layer 38 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer38_moe [label="Layer 38 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 39
        layer39_qkv_proj [label="Layer 39 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer39_attn_comp [label="Layer 39 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer39_out_proj [label="Layer 39 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer39_gate [label="Layer 39 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer39_moe [label="Layer 39 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 40
        layer40_qkv_proj [label="Layer 40 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer40_attn_comp [label="Layer 40 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer40_out_proj [label="Layer 40 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer40_gate [label="Layer 40 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer40_moe [label="Layer 40 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 41
        layer41_qkv_proj [label="Layer 41 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer41_attn_comp [label="Layer 41 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer41_out_proj [label="Layer 41 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer41_gate [label="Layer 41 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer41_moe [label="Layer 41 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 42
        layer42_qkv_proj [label="Layer 42 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer42_attn_comp [label="Layer 42 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer42_out_proj [label="Layer 42 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer42_gate [label="Layer 42 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer42_moe [label="Layer 42 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 43
        layer43_qkv_proj [label="Layer 43 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer43_attn_comp [label="Layer 43 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer43_out_proj [label="Layer 43 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer43_gate [label="Layer 43 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer43_moe [label="Layer 43 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 44
        layer44_qkv_proj [label="Layer 44 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer44_attn_comp [label="Layer 44 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer44_out_proj [label="Layer 44 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer44_gate [label="Layer 44 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer44_moe [label="Layer 44 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 45
        layer45_qkv_proj [label="Layer 45 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer45_attn_comp [label="Layer 45 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer45_out_proj [label="Layer 45 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer45_gate [label="Layer 45 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer45_moe [label="Layer 45 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 46
        layer46_qkv_proj [label="Layer 46 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer46_attn_comp [label="Layer 46 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer46_out_proj [label="Layer 46 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer46_gate [label="Layer 46 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer46_moe [label="Layer 46 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        
        // Layer 47
        layer47_qkv_proj [label="Layer 47 QKV Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer47_attn_comp [label="Layer 47 Attention Computation + KV Cache\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightgreen]
        layer47_out_proj [label="Layer 47 Output Projection\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
        layer47_gate [label="Layer 47 Gate Selection (Top-8)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer47_moe [label="Layer 47 MoE (128 experts)\nGPU: 9-17\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightgreen]
    }
    
    // KV Cache access nodes
    kv_access_1 [label="KV Cache Access\nGPU: 9-17\nCache: [batch_size=128, seq_len=2048, heads=4, d_k=64]" fillcolor=lightcyan shape=parallelogram]
    
    // Communication between stages
    comm_1_2 [label="Pipeline Communication (Single Token)\nGPUs: 9-17 â†’ 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
    
    // Connections for Stage 1
    comm_0_1 -> kv_access_1
    kv_access_1 -> layer24_qkv_proj
    layer24_qkv_proj -> layer24_attn_comp
    layer24_attn_comp -> layer24_out_proj
    layer24_out_proj -> layer24_gate
    layer24_gate -> layer24_moe [style=dashed]
    
    layer24_moe -> layer25_qkv_proj
    layer25_qkv_proj -> layer25_attn_comp
    layer25_attn_comp -> layer25_out_proj
    layer25_out_proj -> layer25_gate
    layer25_gate -> layer25_moe [style=dashed]
        
    layer25_moe -> layer26_qkv_proj
    layer26_qkv_proj -> layer26_attn_comp
    layer26_attn_comp -> layer26_out_proj
    layer26_out_proj -> layer26_gate
    layer26_gate -> layer26_moe [style=dashed]
        
    layer26_moe -> layer27_qkv_proj
    layer27_qkv_proj -> layer27_attn_comp
    layer27_attn_comp -> layer27_out_proj
    layer27_out_proj -> layer27_gate
    layer27_gate -> layer27_moe [style=dashed]
        
    layer27_moe -> layer28_qkv_proj
    layer28_qkv_proj -> layer28_attn_comp
    layer28_attn_comp -> layer28_out_proj
    layer28_out_proj -> layer28_gate
    layer28_gate -> layer28_moe [style=dashed]
        
    layer28_moe -> layer29_qkv_proj
    layer29_qkv_proj -> layer29_attn_comp
    layer29_attn_comp -> layer29_out_proj
    layer29_out_proj -> layer29_gate
    layer29_gate -> layer29_moe [style=dashed]
        
    layer29_moe -> layer30_qkv_proj
    layer30_qkv_proj -> layer30_attn_comp
    layer30_attn_comp -> layer30_out_proj
    layer30_out_proj -> layer30_gate
    layer30_gate -> layer30_moe [style=dashed]
        
    layer30_moe -> layer31_qkv_proj
    layer31_qkv_proj -> layer31_attn_comp
    layer31_attn_comp -> layer31_out_proj
    layer31_out_proj -> layer31_gate
    layer31_gate -> layer31_moe [style=dashed]
        
    layer31_moe -> layer32_qkv_proj
    layer32_qkv_proj -> layer32_attn_comp
    layer32_attn_comp -> layer32_out_proj
    layer32_out_proj -> layer32_gate
    layer32_gate -> layer32_moe [style=dashed]
        
    layer32_moe -> layer33_qkv_proj
    layer33_qkv_proj -> layer33_attn_comp
    layer33_attn_comp -> layer33_out_proj
    layer33_out_proj -> layer33_gate
    layer33_gate -> layer33_moe [style=dashed]
        
    layer33_moe -> layer34_qkv_proj
    layer34_qkv_proj -> layer34_attn_comp
    layer34_attn_comp -> layer34_out_proj
    layer34_out_proj -> layer34_gate
    layer34_gate -> layer34_moe [style=dashed]
        
    layer34_moe -> layer35_qkv_proj
    layer35_qkv_proj -> layer35_attn_comp
    layer35_attn_comp -> layer35_out_proj
    layer35_out_proj -> layer35_gate
    layer35_gate -> layer35_moe [style=dashed]
        
    layer35_moe -> layer36_qkv_proj
    layer36_qkv_proj -> layer36_attn_comp
    layer36_attn_comp -> layer36_out_proj
    layer36_out_proj -> layer36_gate
    layer36_gate -> layer36_moe [style=dashed]
        
    layer36_moe -> layer37_qkv_proj
    layer37_qkv_proj -> layer37_attn_comp
    layer37_attn_comp -> layer37_out_proj
    layer37_out_proj -> layer37_gate
    layer37_gate -> layer37_moe [style=dashed]
        
    layer37_moe -> layer38_qkv_proj
    layer38_qkv_proj -> layer38_attn_comp
    layer38_attn_comp -> layer38_out_proj
    layer38_out_proj -> layer38_gate
    layer38_gate -> layer38_moe [style=dashed]
        
    layer38_moe -> layer39_qkv_proj
    layer39_qkv_proj -> layer39_attn_comp
    layer39_attn_comp -> layer39_out_proj
    layer39_out_proj -> layer39_gate
    layer39_gate -> layer39_moe [style=dashed]
        
    layer39_moe -> layer40_qkv_proj
    layer40_qkv_proj -> layer40_attn_comp
    layer40_attn_comp -> layer40_out_proj
    layer40_out_proj -> layer40_gate
    layer40_gate -> layer40_moe [style=dashed]
        
    layer40_moe -> layer41_qkv_proj
    layer41_qkv_proj -> layer41_attn_comp
    layer41_attn_comp -> layer41_out_proj
    layer41_out_proj -> layer41_gate
    layer41_gate -> layer41_moe [style=dashed]
        
    layer41_moe -> layer42_qkv_proj
    layer42_qkv_proj -> layer42_attn_comp
    layer42_attn_comp -> layer42_out_proj
    layer42_out_proj -> layer42_gate
    layer42_gate -> layer42_moe [style=dashed]
        
    layer42_moe -> layer43_qkv_proj
    layer43_qkv_proj -> layer43_attn_comp
    layer43_attn_comp -> layer43_out_proj
    layer43_out_proj -> layer43_gate
    layer43_gate -> layer43_moe [style=dashed]
        
    layer43_moe -> layer44_qkv_proj
    layer44_qkv_proj -> layer44_attn_comp
    layer44_attn_comp -> layer44_out_proj
    layer44_out_proj -> layer44_gate
    layer44_gate -> layer44_moe [style=dashed]
        
    layer44_moe -> layer45_qkv_proj
    layer45_qkv_proj -> layer45_attn_comp
    layer45_attn_comp -> layer45_out_proj
    layer45_out_proj -> layer45_gate
    layer45_gate -> layer45_moe [style=dashed]
        
    layer45_moe -> layer46_qkv_proj
    layer46_qkv_proj -> layer46_attn_comp
    layer46_attn_comp -> layer46_out_proj
    layer46_out_proj -> layer46_gate
    layer46_gate -> layer46_moe [style=dashed]
        
    layer46_moe -> layer47_qkv_proj
    layer47_qkv_proj -> layer47_attn_comp
    layer47_attn_comp -> layer47_out_proj
    layer47_out_proj -> layer47_gate
    layer47_gate -> layer47_moe [style=dashed]
    layer47_moe -> comm_1_2
    
    // Stage 2: Layers 48-71 (GPUs 18-26)
    subgraph cluster_stage2 {
        fillcolor=lightyellow label="Stage 2: Layers 48-71\nGPUs 18-26" style=rounded
        
        // Layer 48
        layer48_qkv_proj [label="Layer 48 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer48_attn_comp [label="Layer 48 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer48_out_proj [label="Layer 48 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer48_gate [label="Layer 48 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer48_moe [label="Layer 48 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 49
        layer49_qkv_proj [label="Layer 49 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer49_attn_comp [label="Layer 49 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer49_out_proj [label="Layer 49 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer49_gate [label="Layer 49 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer49_moe [label="Layer 49 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 50
        layer50_qkv_proj [label="Layer 50 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer50_attn_comp [label="Layer 50 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer50_out_proj [label="Layer 50 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer50_gate [label="Layer 50 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer50_moe [label="Layer 50 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 51
        layer51_qkv_proj [label="Layer 51 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer51_attn_comp [label="Layer 51 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer51_out_proj [label="Layer 51 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer51_gate [label="Layer 51 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer51_moe [label="Layer 51 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 52
        layer52_qkv_proj [label="Layer 52 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer52_attn_comp [label="Layer 52 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer52_out_proj [label="Layer 52 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer52_gate [label="Layer 52 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer52_moe [label="Layer 52 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 53
        layer53_qkv_proj [label="Layer 53 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer53_attn_comp [label="Layer 53 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer53_out_proj [label="Layer 53 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer53_gate [label="Layer 53 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer53_moe [label="Layer 53 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 54
        layer54_qkv_proj [label="Layer 54 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer54_attn_comp [label="Layer 54 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer54_out_proj [label="Layer 54 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer54_gate [label="Layer 54 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer54_moe [label="Layer 54 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 55
        layer55_qkv_proj [label="Layer 55 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer55_attn_comp [label="Layer 55 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer55_out_proj [label="Layer 55 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer55_gate [label="Layer 55 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer55_moe [label="Layer 55 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 56
        layer56_qkv_proj [label="Layer 56 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer56_attn_comp [label="Layer 56 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer56_out_proj [label="Layer 56 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer56_gate [label="Layer 56 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer56_moe [label="Layer 56 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 57
        layer57_qkv_proj [label="Layer 57 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer57_attn_comp [label="Layer 57 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer57_out_proj [label="Layer 57 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer57_gate [label="Layer 57 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer57_moe [label="Layer 57 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 58
        layer58_qkv_proj [label="Layer 58 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer58_attn_comp [label="Layer 58 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer58_out_proj [label="Layer 58 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer58_gate [label="Layer 58 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer58_moe [label="Layer 58 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 59
        layer59_qkv_proj [label="Layer 59 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer59_attn_comp [label="Layer 59 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer59_out_proj [label="Layer 59 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer59_gate [label="Layer 59 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer59_moe [label="Layer 59 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 60
        layer60_qkv_proj [label="Layer 60 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer60_attn_comp [label="Layer 60 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer60_out_proj [label="Layer 60 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer60_gate [label="Layer 60 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer60_moe [label="Layer 60 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 61
        layer61_qkv_proj [label="Layer 61 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer61_attn_comp [label="Layer 61 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer61_out_proj [label="Layer 61 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer61_gate [label="Layer 61 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer61_moe [label="Layer 61 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 62
        layer62_qkv_proj [label="Layer 62 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer62_attn_comp [label="Layer 62 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer62_out_proj [label="Layer 62 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer62_gate [label="Layer 62 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer62_moe [label="Layer 62 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 63
        layer63_qkv_proj [label="Layer 63 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer63_attn_comp [label="Layer 63 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer63_out_proj [label="Layer 63 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer63_gate [label="Layer 63 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer63_moe [label="Layer 63 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 64
        layer64_qkv_proj [label="Layer 64 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer64_attn_comp [label="Layer 64 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer64_out_proj [label="Layer 64 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer64_gate [label="Layer 64 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer64_moe [label="Layer 64 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 65
        layer65_qkv_proj [label="Layer 65 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer65_attn_comp [label="Layer 65 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer65_out_proj [label="Layer 65 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer65_gate [label="Layer 65 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer65_moe [label="Layer 65 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 66
        layer66_qkv_proj [label="Layer 66 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer66_attn_comp [label="Layer 66 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer66_out_proj [label="Layer 66 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer66_gate [label="Layer 66 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer66_moe [label="Layer 66 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 67
        layer67_qkv_proj [label="Layer 67 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer67_attn_comp [label="Layer 67 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer67_out_proj [label="Layer 67 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer67_gate [label="Layer 67 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer67_moe [label="Layer 67 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 68
        layer68_qkv_proj [label="Layer 68 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer68_attn_comp [label="Layer 68 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer68_out_proj [label="Layer 68 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer68_gate [label="Layer 68 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer68_moe [label="Layer 68 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 69
        layer69_qkv_proj [label="Layer 69 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer69_attn_comp [label="Layer 69 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer69_out_proj [label="Layer 69 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer69_gate [label="Layer 69 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer69_moe [label="Layer 69 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 70
        layer70_qkv_proj [label="Layer 70 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer70_attn_comp [label="Layer 70 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer70_out_proj [label="Layer 70 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer70_gate [label="Layer 70 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer70_moe [label="Layer 70 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        
        // Layer 71
        layer71_qkv_proj [label="Layer 71 QKV Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer71_attn_comp [label="Layer 71 Attention Computation + KV Cache\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightyellow]
        layer71_out_proj [label="Layer 71 Output Projection\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
        layer71_gate [label="Layer 71 Gate Selection (Top-8)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer71_moe [label="Layer 71 MoE (128 experts)\nGPU: 18-26\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightyellow]
    }
    
    // KV Cache access nodes
    kv_access_2 [label="KV Cache Access\nGPU: 18-26\nCache: [batch_size=128, seq_len=2048, heads=4, d_k=64]" fillcolor=lightcyan shape=parallelogram]
    
    // Communication between stages
    comm_2_3 [label="Pipeline Communication (Single Token)\nGPUs: 18-26 â†’ 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=pink shape=ellipse style=dashed]
    
    // Connections for Stage 2
    comm_1_2 -> kv_access_2
    kv_access_2 -> layer48_qkv_proj
    layer48_qkv_proj -> layer48_attn_comp
    layer48_attn_comp -> layer48_out_proj
    layer48_out_proj -> layer48_gate
    layer48_gate -> layer48_moe [style=dashed]
    
    layer48_moe -> layer49_qkv_proj
    layer49_qkv_proj -> layer49_attn_comp
    layer49_attn_comp -> layer49_out_proj
    layer49_out_proj -> layer49_gate
    layer49_gate -> layer49_moe [style=dashed]
        
    layer49_moe -> layer50_qkv_proj
    layer50_qkv_proj -> layer50_attn_comp
    layer50_attn_comp -> layer50_out_proj
    layer50_out_proj -> layer50_gate
    layer50_gate -> layer50_moe [style=dashed]
        
    layer50_moe -> layer51_qkv_proj
    layer51_qkv_proj -> layer51_attn_comp
    layer51_attn_comp -> layer51_out_proj
    layer51_out_proj -> layer51_gate
    layer51_gate -> layer51_moe [style=dashed]
        
    layer51_moe -> layer52_qkv_proj
    layer52_qkv_proj -> layer52_attn_comp
    layer52_attn_comp -> layer52_out_proj
    layer52_out_proj -> layer52_gate
    layer52_gate -> layer52_moe [style=dashed]
        
    layer52_moe -> layer53_qkv_proj
    layer53_qkv_proj -> layer53_attn_comp
    layer53_attn_comp -> layer53_out_proj
    layer53_out_proj -> layer53_gate
    layer53_gate -> layer53_moe [style=dashed]
        
    layer53_moe -> layer54_qkv_proj
    layer54_qkv_proj -> layer54_attn_comp
    layer54_attn_comp -> layer54_out_proj
    layer54_out_proj -> layer54_gate
    layer54_gate -> layer54_moe [style=dashed]
        
    layer54_moe -> layer55_qkv_proj
    layer55_qkv_proj -> layer55_attn_comp
    layer55_attn_comp -> layer55_out_proj
    layer55_out_proj -> layer55_gate
    layer55_gate -> layer55_moe [style=dashed]
        
    layer55_moe -> layer56_qkv_proj
    layer56_qkv_proj -> layer56_attn_comp
    layer56_attn_comp -> layer56_out_proj
    layer56_out_proj -> layer56_gate
    layer56_gate -> layer56_moe [style=dashed]
        
    layer56_moe -> layer57_qkv_proj
    layer57_qkv_proj -> layer57_attn_comp
    layer57_attn_comp -> layer57_out_proj
    layer57_out_proj -> layer57_gate
    layer57_gate -> layer57_moe [style=dashed]
        
    layer57_moe -> layer58_qkv_proj
    layer58_qkv_proj -> layer58_attn_comp
    layer58_attn_comp -> layer58_out_proj
    layer58_out_proj -> layer58_gate
    layer58_gate -> layer58_moe [style=dashed]
        
    layer58_moe -> layer59_qkv_proj
    layer59_qkv_proj -> layer59_attn_comp
    layer59_attn_comp -> layer59_out_proj
    layer59_out_proj -> layer59_gate
    layer59_gate -> layer59_moe [style=dashed]
        
    layer59_moe -> layer60_qkv_proj
    layer60_qkv_proj -> layer60_attn_comp
    layer60_attn_comp -> layer60_out_proj
    layer60_out_proj -> layer60_gate
    layer60_gate -> layer60_moe [style=dashed]
        
    layer60_moe -> layer61_qkv_proj
    layer61_qkv_proj -> layer61_attn_comp
    layer61_attn_comp -> layer61_out_proj
    layer61_out_proj -> layer61_gate
    layer61_gate -> layer61_moe [style=dashed]
        
    layer61_moe -> layer62_qkv_proj
    layer62_qkv_proj -> layer62_attn_comp
    layer62_attn_comp -> layer62_out_proj
    layer62_out_proj -> layer62_gate
    layer62_gate -> layer62_moe [style=dashed]
        
    layer62_moe -> layer63_qkv_proj
    layer63_qkv_proj -> layer63_attn_comp
    layer63_attn_comp -> layer63_out_proj
    layer63_out_proj -> layer63_gate
    layer63_gate -> layer63_moe [style=dashed]
        
    layer63_moe -> layer64_qkv_proj
    layer64_qkv_proj -> layer64_attn_comp
    layer64_attn_comp -> layer64_out_proj
    layer64_out_proj -> layer64_gate
    layer64_gate -> layer64_moe [style=dashed]
        
    layer64_moe -> layer65_qkv_proj
    layer65_qkv_proj -> layer65_attn_comp
    layer65_attn_comp -> layer65_out_proj
    layer65_out_proj -> layer65_gate
    layer65_gate -> layer65_moe [style=dashed]
        
    layer65_moe -> layer66_qkv_proj
    layer66_qkv_proj -> layer66_attn_comp
    layer66_attn_comp -> layer66_out_proj
    layer66_out_proj -> layer66_gate
    layer66_gate -> layer66_moe [style=dashed]
        
    layer66_moe -> layer67_qkv_proj
    layer67_qkv_proj -> layer67_attn_comp
    layer67_attn_comp -> layer67_out_proj
    layer67_out_proj -> layer67_gate
    layer67_gate -> layer67_moe [style=dashed]
        
    layer67_moe -> layer68_qkv_proj
    layer68_qkv_proj -> layer68_attn_comp
    layer68_attn_comp -> layer68_out_proj
    layer68_out_proj -> layer68_gate
    layer68_gate -> layer68_moe [style=dashed]
        
    layer68_moe -> layer69_qkv_proj
    layer69_qkv_proj -> layer69_attn_comp
    layer69_attn_comp -> layer69_out_proj
    layer69_out_proj -> layer69_gate
    layer69_gate -> layer69_moe [style=dashed]
        
    layer69_moe -> layer70_qkv_proj
    layer70_qkv_proj -> layer70_attn_comp
    layer70_attn_comp -> layer70_out_proj
    layer70_out_proj -> layer70_gate
    layer70_gate -> layer70_moe [style=dashed]
        
    layer70_moe -> layer71_qkv_proj
    layer71_qkv_proj -> layer71_attn_comp
    layer71_attn_comp -> layer71_out_proj
    layer71_out_proj -> layer71_gate
    layer71_gate -> layer71_moe [style=dashed]
    layer71_moe -> comm_2_3
    
    // Stage 3: Layers 72-93 (GPUs 27-34)
    subgraph cluster_stage3 {
        fillcolor=lightcoral label="Stage 3: Layers 72-93\nGPUs 27-34" style=rounded
        
        // Layer 72
        layer72_qkv_proj [label="Layer 72 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer72_attn_comp [label="Layer 72 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer72_out_proj [label="Layer 72 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer72_gate [label="Layer 72 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer72_moe [label="Layer 72 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 73
        layer73_qkv_proj [label="Layer 73 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer73_attn_comp [label="Layer 73 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer73_out_proj [label="Layer 73 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer73_gate [label="Layer 73 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer73_moe [label="Layer 73 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 74
        layer74_qkv_proj [label="Layer 74 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer74_attn_comp [label="Layer 74 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer74_out_proj [label="Layer 74 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer74_gate [label="Layer 74 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer74_moe [label="Layer 74 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 75
        layer75_qkv_proj [label="Layer 75 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer75_attn_comp [label="Layer 75 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer75_out_proj [label="Layer 75 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer75_gate [label="Layer 75 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer75_moe [label="Layer 75 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 76
        layer76_qkv_proj [label="Layer 76 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer76_attn_comp [label="Layer 76 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer76_out_proj [label="Layer 76 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer76_gate [label="Layer 76 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer76_moe [label="Layer 76 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 77
        layer77_qkv_proj [label="Layer 77 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer77_attn_comp [label="Layer 77 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer77_out_proj [label="Layer 77 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer77_gate [label="Layer 77 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer77_moe [label="Layer 77 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 78
        layer78_qkv_proj [label="Layer 78 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer78_attn_comp [label="Layer 78 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer78_out_proj [label="Layer 78 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer78_gate [label="Layer 78 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer78_moe [label="Layer 78 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 79
        layer79_qkv_proj [label="Layer 79 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer79_attn_comp [label="Layer 79 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer79_out_proj [label="Layer 79 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer79_gate [label="Layer 79 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer79_moe [label="Layer 79 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 80
        layer80_qkv_proj [label="Layer 80 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer80_attn_comp [label="Layer 80 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer80_out_proj [label="Layer 80 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer80_gate [label="Layer 80 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer80_moe [label="Layer 80 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 81
        layer81_qkv_proj [label="Layer 81 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer81_attn_comp [label="Layer 81 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer81_out_proj [label="Layer 81 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer81_gate [label="Layer 81 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer81_moe [label="Layer 81 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 82
        layer82_qkv_proj [label="Layer 82 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer82_attn_comp [label="Layer 82 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer82_out_proj [label="Layer 82 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer82_gate [label="Layer 82 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer82_moe [label="Layer 82 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 83
        layer83_qkv_proj [label="Layer 83 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer83_attn_comp [label="Layer 83 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer83_out_proj [label="Layer 83 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer83_gate [label="Layer 83 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer83_moe [label="Layer 83 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 84
        layer84_qkv_proj [label="Layer 84 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer84_attn_comp [label="Layer 84 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer84_out_proj [label="Layer 84 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer84_gate [label="Layer 84 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer84_moe [label="Layer 84 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 85
        layer85_qkv_proj [label="Layer 85 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer85_attn_comp [label="Layer 85 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer85_out_proj [label="Layer 85 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer85_gate [label="Layer 85 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer85_moe [label="Layer 85 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 86
        layer86_qkv_proj [label="Layer 86 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer86_attn_comp [label="Layer 86 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer86_out_proj [label="Layer 86 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer86_gate [label="Layer 86 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer86_moe [label="Layer 86 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 87
        layer87_qkv_proj [label="Layer 87 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer87_attn_comp [label="Layer 87 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer87_out_proj [label="Layer 87 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer87_gate [label="Layer 87 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer87_moe [label="Layer 87 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 88
        layer88_qkv_proj [label="Layer 88 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer88_attn_comp [label="Layer 88 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer88_out_proj [label="Layer 88 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer88_gate [label="Layer 88 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer88_moe [label="Layer 88 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 89
        layer89_qkv_proj [label="Layer 89 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer89_attn_comp [label="Layer 89 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer89_out_proj [label="Layer 89 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer89_gate [label="Layer 89 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer89_moe [label="Layer 89 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 90
        layer90_qkv_proj [label="Layer 90 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer90_attn_comp [label="Layer 90 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer90_out_proj [label="Layer 90 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer90_gate [label="Layer 90 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer90_moe [label="Layer 90 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 91
        layer91_qkv_proj [label="Layer 91 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer91_attn_comp [label="Layer 91 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer91_out_proj [label="Layer 91 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer91_gate [label="Layer 91 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer91_moe [label="Layer 91 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 92
        layer92_qkv_proj [label="Layer 92 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer92_attn_comp [label="Layer 92 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer92_out_proj [label="Layer 92 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer92_gate [label="Layer 92 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer92_moe [label="Layer 92 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        
        // Layer 93
        layer93_qkv_proj [label="Layer 93 QKV Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer93_attn_comp [label="Layer 93 Attention Computation + KV Cache\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, heads=64, d_k=64]" fillcolor=lightcoral]
        layer93_out_proj [label="Layer 93 Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
        layer93_gate [label="Layer 93 Gate Selection (Top-8)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, top_k=8]" fillcolor=orange shape=parallelogram]
        layer93_moe [label="Layer 93 MoE (128 experts)\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, d_model=4096]" fillcolor=lightcoral]
    }
    
    // KV Cache access nodes
    kv_access_3 [label="KV Cache Access\nGPU: 27-34\nCache: [batch_size=128, seq_len=2048, heads=4, d_k=64]" fillcolor=lightcyan shape=parallelogram]
    
    // Output node
    output [label="Output Projection\nGPU: 27-34\nInput: [batch_size=128, seq_len=1, d_model=4096]\nOutput: [batch_size=128, seq_len=1, vocab_size=151936]" fillcolor=lightgray shape=ellipse]
    
    // Connections for Stage 3
    comm_2_3 -> kv_access_3
    kv_access_3 -> layer72_qkv_proj
    layer72_qkv_proj -> layer72_attn_comp
    layer72_attn_comp -> layer72_out_proj
    layer72_out_proj -> layer72_gate
    layer72_gate -> layer72_moe [style=dashed]
    
    layer72_moe -> layer73_qkv_proj
    layer73_qkv_proj -> layer73_attn_comp
    layer73_attn_comp -> layer73_out_proj
    layer73_out_proj -> layer73_gate
    layer73_gate -> layer73_moe [style=dashed]
        
    layer73_moe -> layer74_qkv_proj
    layer74_qkv_proj -> layer74_attn_comp
    layer74_attn_comp -> layer74_out_proj
    layer74_out_proj -> layer74_gate
    layer74_gate -> layer74_moe [style=dashed]
        
    layer74_moe -> layer75_qkv_proj
    layer75_qkv_proj -> layer75_attn_comp
    layer75_attn_comp -> layer75_out_proj
    layer75_out_proj -> layer75_gate
    layer75_gate -> layer75_moe [style=dashed]
        
    layer75_moe -> layer76_qkv_proj
    layer76_qkv_proj -> layer76_attn_comp
    layer76_attn_comp -> layer76_out_proj
    layer76_out_proj -> layer76_gate
    layer76_gate -> layer76_moe [style=dashed]
        
    layer76_moe -> layer77_qkv_proj
    layer77_qkv_proj -> layer77_attn_comp
    layer77_attn_comp -> layer77_out_proj
    layer77_out_proj -> layer77_gate
    layer77_gate -> layer77_moe [style=dashed]
        
    layer77_moe -> layer78_qkv_proj
    layer78_qkv_proj -> layer78_attn_comp
    layer78_attn_comp -> layer78_out_proj
    layer78_out_proj -> layer78_gate
    layer78_gate -> layer78_moe [style=dashed]
        
    layer78_moe -> layer79_qkv_proj
    layer79_qkv_proj -> layer79_attn_comp
    layer79_attn_comp -> layer79_out_proj
    layer79_out_proj -> layer79_gate
    layer79_gate -> layer79_moe [style=dashed]
        
    layer79_moe -> layer80_qkv_proj
    layer80_qkv_proj -> layer80_attn_comp
    layer80_attn_comp -> layer80_out_proj
    layer80_out_proj -> layer80_gate
    layer80_gate -> layer80_moe [style=dashed]
        
    layer80_moe -> layer81_qkv_proj
    layer81_qkv_proj -> layer81_attn_comp
    layer81_attn_comp -> layer81_out_proj
    layer81_out_proj -> layer81_gate
    layer81_gate -> layer81_moe [style=dashed]
        
    layer81_moe -> layer82_qkv_proj
    layer82_qkv_proj -> layer82_attn_comp
    layer82_attn_comp -> layer82_out_proj
    layer82_out_proj -> layer82_gate
    layer82_gate -> layer82_moe [style=dashed]
        
    layer82_moe -> layer83_qkv_proj
    layer83_qkv_proj -> layer83_attn_comp
    layer83_attn_comp -> layer83_out_proj
    layer83_out_proj -> layer83_gate
    layer83_gate -> layer83_moe [style=dashed]
        
    layer83_moe -> layer84_qkv_proj
    layer84_qkv_proj -> layer84_attn_comp
    layer84_attn_comp -> layer84_out_proj
    layer84_out_proj -> layer84_gate
    layer84_gate -> layer84_moe [style=dashed]
        
    layer84_moe -> layer85_qkv_proj
    layer85_qkv_proj -> layer85_attn_comp
    layer85_attn_comp -> layer85_out_proj
    layer85_out_proj -> layer85_gate
    layer85_gate -> layer85_moe [style=dashed]
        
    layer85_moe -> layer86_qkv_proj
    layer86_qkv_proj -> layer86_attn_comp
    layer86_attn_comp -> layer86_out_proj
    layer86_out_proj -> layer86_gate
    layer86_gate -> layer86_moe [style=dashed]
        
    layer86_moe -> layer87_qkv_proj
    layer87_qkv_proj -> layer87_attn_comp
    layer87_attn_comp -> layer87_out_proj
    layer87_out_proj -> layer87_gate
    layer87_gate -> layer87_moe [style=dashed]
        
    layer87_moe -> layer88_qkv_proj
    layer88_qkv_proj -> layer88_attn_comp
    layer88_attn_comp -> layer88_out_proj
    layer88_out_proj -> layer88_gate
    layer88_gate -> layer88_moe [style=dashed]
        
    layer88_moe -> layer89_qkv_proj
    layer89_qkv_proj -> layer89_attn_comp
    layer89_attn_comp -> layer89_out_proj
    layer89_out_proj -> layer89_gate
    layer89_gate -> layer89_moe [style=dashed]
        
    layer89_moe -> layer90_qkv_proj
    layer90_qkv_proj -> layer90_attn_comp
    layer90_attn_comp -> layer90_out_proj
    layer90_out_proj -> layer90_gate
    layer90_gate -> layer90_moe [style=dashed]
        
    layer90_moe -> layer91_qkv_proj
    layer91_qkv_proj -> layer91_attn_comp
    layer91_attn_comp -> layer91_out_proj
    layer91_out_proj -> layer91_gate
    layer91_gate -> layer91_moe [style=dashed]
        
    layer91_moe -> layer92_qkv_proj
    layer92_qkv_proj -> layer92_attn_comp
    layer92_attn_comp -> layer92_out_proj
    layer92_out_proj -> layer92_gate
    layer92_gate -> layer92_moe [style=dashed]
        
    layer92_moe -> layer93_qkv_proj
    layer93_qkv_proj -> layer93_attn_comp
    layer93_attn_comp -> layer93_out_proj
    layer93_out_proj -> layer93_gate
    layer93_gate -> layer93_moe [style=dashed]
    layer93_moe -> output
}
    