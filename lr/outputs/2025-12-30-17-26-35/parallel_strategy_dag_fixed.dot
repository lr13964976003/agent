// Qwen3-235B Parallel Strategy Deployment DAG - Corrected
digraph {
	graph [comment="Qwen3-235B Parallel Strategy Deployment DAG - Corrected" dpi=300 rankdir=TB size="30,40"]
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=8]
	subgraph cluster_stage0 {
		fillcolor=lightcoral label="Pipeline Stage 0: Layers 0-23\nGPUs: 0-31" style=rounded
		embed_0 [label="Token Embedding\nGPU: 0-7\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=rectangle style=filled]
		gate_0 [label="Gate Selection\nGPU: 0-7\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=32]" fillcolor=lightyellow shape=parallelogram style=filled]
		route_0 [label="Expert Routing\nGPU: 0-31\nInput: [batch_size=128, seq_len=2048, experts=32]\nOutput: [batch_size=128, seq_len=2048, experts=1]" fillcolor=lightgreen shape=ellipse style=filled]
		expert_0 [label="Expert 0 Computation\nGPU: 0\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_1 [label="Expert 1 Computation\nGPU: 1\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_2 [label="Expert 2 Computation\nGPU: 2\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_3 [label="Expert 3 Computation\nGPU: 3\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_4 [label="Expert 4 Computation\nGPU: 4\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_5 [label="Expert 5 Computation\nGPU: 5\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_6 [label="Expert 6 Computation\nGPU: 6\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_7 [label="Expert 7 Computation\nGPU: 7\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_proj_0 [label="QKV Projection (TP=8)\nGPU: 0-7\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=16, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_comm_0 [label="QKV AllGather (TP)\nGPU: 0-7\nInput: [batch_size=16, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		attn_scores_0 [label="Attention Score Computation (SP=2)\nGPU: 0-7\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=64, seq_len=1024, seq_len=1024]" fillcolor=lightblue shape=rectangle style=filled]
		attn_apply_0 [label="Attention Weight Application (SP=2)\nGPU: 0-7\nInput: [batch_size=64, seq_len=1024, seq_len=1024]\nOutput: [batch_size=64, seq_len=1024, heads=64, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		attn_sp_comm_0 [label="Attention SP AllReduce\nGPU: 0-7\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		out_proj_0 [label="Output Projection (TP=8)\nGPU: 0-7\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		out_proj_comm_0 [label="Output AllReduce (TP)\nGPU: 0-7\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
		agg_0 [label="Expert Aggregation\nGPU: 0-7\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	}
	subgraph cluster_stage1 {
		fillcolor=lightsteelblue label="Pipeline Stage 1: Layers 24-47\nGPUs: 32-63" style=rounded
		embed_1 [label="Token Embedding\nGPU: 32-39\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=rectangle style=filled]
		gate_1 [label="Gate Selection\nGPU: 32-39\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=32]" fillcolor=lightyellow shape=parallelogram style=filled]
		route_1 [label="Expert Routing\nGPU: 32-63\nInput: [batch_size=128, seq_len=2048, experts=32]\nOutput: [batch_size=128, seq_len=2048, experts=1]" fillcolor=lightgreen shape=ellipse style=filled]
		expert_32 [label="Expert 32 Computation\nGPU: 32\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_33 [label="Expert 33 Computation\nGPU: 33\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_34 [label="Expert 34 Computation\nGPU: 34\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_35 [label="Expert 35 Computation\nGPU: 35\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_36 [label="Expert 36 Computation\nGPU: 36\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_37 [label="Expert 37 Computation\nGPU: 37\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_38 [label="Expert 38 Computation\nGPU: 38\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_39 [label="Expert 39 Computation\nGPU: 39\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_proj_1 [label="QKV Projection (TP=8)\nGPU: 32-39\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=16, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_comm_1 [label="QKV AllGather (TP)\nGPU: 32-39\nInput: [batch_size=16, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		attn_scores_1 [label="Attention Score Computation (SP=2)\nGPU: 32-39\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=64, seq_len=1024, seq_len=1024]" fillcolor=lightblue shape=rectangle style=filled]
		attn_apply_1 [label="Attention Weight Application (SP=2)\nGPU: 32-39\nInput: [batch_size=64, seq_len=1024, seq_len=1024]\nOutput: [batch_size=64, seq_len=1024, heads=64, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		attn_sp_comm_1 [label="Attention SP AllReduce\nGPU: 32-39\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		out_proj_1 [label="Output Projection (TP=8)\nGPU: 32-39\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		out_proj_comm_1 [label="Output AllReduce (TP)\nGPU: 32-39\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
		agg_1 [label="Expert Aggregation\nGPU: 32-39\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	}
	subgraph cluster_stage2 {
		fillcolor=lightseagreen label="Pipeline Stage 2: Layers 48-71\nGPUs: 64-95" style=rounded
		embed_2 [label="Token Embedding\nGPU: 64-71\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=rectangle style=filled]
		gate_2 [label="Gate Selection\nGPU: 64-71\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=32]" fillcolor=lightyellow shape=parallelogram style=filled]
		route_2 [label="Expert Routing\nGPU: 64-95\nInput: [batch_size=128, seq_len=2048, experts=32]\nOutput: [batch_size=128, seq_len=2048, experts=1]" fillcolor=lightgreen shape=ellipse style=filled]
		expert_64 [label="Expert 64 Computation\nGPU: 64\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_65 [label="Expert 65 Computation\nGPU: 65\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_66 [label="Expert 66 Computation\nGPU: 66\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_67 [label="Expert 67 Computation\nGPU: 67\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_68 [label="Expert 68 Computation\nGPU: 68\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_69 [label="Expert 69 Computation\nGPU: 69\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_70 [label="Expert 70 Computation\nGPU: 70\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_71 [label="Expert 71 Computation\nGPU: 71\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_proj_2 [label="QKV Projection (TP=8)\nGPU: 64-71\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=16, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_comm_2 [label="QKV AllGather (TP)\nGPU: 64-71\nInput: [batch_size=16, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		attn_scores_2 [label="Attention Score Computation (SP=2)\nGPU: 64-71\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=64, seq_len=1024, seq_len=1024]" fillcolor=lightblue shape=rectangle style=filled]
		attn_apply_2 [label="Attention Weight Application (SP=2)\nGPU: 64-71\nInput: [batch_size=64, seq_len=1024, seq_len=1024]\nOutput: [batch_size=64, seq_len=1024, heads=64, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		attn_sp_comm_2 [label="Attention SP AllReduce\nGPU: 64-71\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		out_proj_2 [label="Output Projection (TP=8)\nGPU: 64-71\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		out_proj_comm_2 [label="Output AllReduce (TP)\nGPU: 64-71\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
		agg_2 [label="Expert Aggregation\nGPU: 64-71\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	}
	subgraph cluster_stage3 {
		fillcolor=lightsalmon label="Pipeline Stage 3: Layers 72-93\nGPUs: 96-127" style=rounded
		embed_3 [label="Token Embedding\nGPU: 96-103\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=rectangle style=filled]
		gate_3 [label="Gate Selection\nGPU: 96-103\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=32]" fillcolor=lightyellow shape=parallelogram style=filled]
		route_3 [label="Expert Routing\nGPU: 96-127\nInput: [batch_size=128, seq_len=2048, experts=32]\nOutput: [batch_size=128, seq_len=2048, experts=1]" fillcolor=lightgreen shape=ellipse style=filled]
		expert_96 [label="Expert 96 Computation\nGPU: 96\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_97 [label="Expert 97 Computation\nGPU: 97\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_98 [label="Expert 98 Computation\nGPU: 98\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_99 [label="Expert 99 Computation\nGPU: 99\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_100 [label="Expert 100 Computation\nGPU: 100\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_101 [label="Expert 101 Computation\nGPU: 101\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_102 [label="Expert 102 Computation\nGPU: 102\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		expert_103 [label="Expert 103 Computation\nGPU: 103\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_proj_3 [label="QKV Projection (TP=8)\nGPU: 96-103\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=16, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		qkv_comm_3 [label="QKV AllGather (TP)\nGPU: 96-103\nInput: [batch_size=16, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		attn_scores_3 [label="Attention Score Computation (SP=2)\nGPU: 96-103\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=64, seq_len=1024, seq_len=1024]" fillcolor=lightblue shape=rectangle style=filled]
		attn_apply_3 [label="Attention Weight Application (SP=2)\nGPU: 96-103\nInput: [batch_size=64, seq_len=1024, seq_len=1024]\nOutput: [batch_size=64, seq_len=1024, heads=64, d_k=64]" fillcolor=lightblue shape=rectangle style=filled]
		attn_sp_comm_3 [label="Attention SP AllReduce\nGPU: 96-103\nInput: [batch_size=64, seq_len=1024, heads=64, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]" fillcolor=lightgreen shape=ellipse style=filled]
		out_proj_3 [label="Output Projection (TP=8)\nGPU: 96-103\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\nOutput: [batch_size=16, seq_len=2048, hidden=512]" fillcolor=lightblue shape=rectangle style=filled]
		out_proj_comm_3 [label="Output AllReduce (TP)\nGPU: 96-103\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
		agg_3 [label="Expert Aggregation\nGPU: 96-103\nInput: [batch_size=16, seq_len=2048, hidden=512]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	}
	input [label="Input\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgray shape=rectangle style=filled]
	output [label="Output\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, vocab_size=128000]" fillcolor=lightgray shape=rectangle style=filled]
	pipe_comm_0_1 [label="Pipeline Communication\nGPU: 31->32\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
	pipe_comm_1_2 [label="Pipeline Communication\nGPU: 63->64\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
	pipe_comm_2_3 [label="Pipeline Communication\nGPU: 95->96\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=ellipse style=filled]
	input -> embed_0
	embed_0 -> gate_0
	gate_0 -> route_0 [style=dashed]
	route_0 -> expert_0
	route_0 -> expert_1
	route_0 -> expert_2
	route_0 -> expert_3
	route_0 -> expert_4
	route_0 -> expert_5
	route_0 -> expert_6
	route_0 -> expert_7
	expert_0 -> qkv_proj_0
	expert_1 -> qkv_proj_0
	expert_2 -> qkv_proj_0
	expert_3 -> qkv_proj_0
	expert_4 -> qkv_proj_0
	expert_5 -> qkv_proj_0
	expert_6 -> qkv_proj_0
	expert_7 -> qkv_proj_0
	qkv_proj_0 -> qkv_comm_0
	qkv_comm_0 -> attn_scores_0
	attn_scores_0 -> attn_apply_0
	attn_apply_0 -> attn_sp_comm_0
	attn_sp_comm_0 -> out_proj_0
	out_proj_0 -> out_proj_comm_0
	expert_0 -> agg_0
	expert_1 -> agg_0
	expert_2 -> agg_0
	expert_3 -> agg_0
	expert_4 -> agg_0
	expert_5 -> agg_0
	expert_6 -> agg_0
	expert_7 -> agg_0
	out_proj_comm_0 -> agg_0
	agg_0 -> pipe_comm_0_1
	pipe_comm_0_1 -> embed_1
	embed_1 -> gate_1
	gate_1 -> route_1 [style=dashed]
	route_1 -> expert_32
	route_1 -> expert_33
	route_1 -> expert_34
	route_1 -> expert_35
	route_1 -> expert_36
	route_1 -> expert_37
	route_1 -> expert_38
	route_1 -> expert_39
	expert_32 -> qkv_proj_1
	expert_33 -> qkv_proj_1
	expert_34 -> qkv_proj_1
	expert_35 -> qkv_proj_1
	expert_36 -> qkv_proj_1
	expert_37 -> qkv_proj_1
	expert_38 -> qkv_proj_1
	expert_39 -> qkv_proj_1
	qkv_proj_1 -> qkv_comm_1
	qkv_comm_1 -> attn_scores_1
	attn_scores_1 -> attn_apply_1
	attn_apply_1 -> attn_sp_comm_1
	attn_sp_comm_1 -> out_proj_1
	out_proj_1 -> out_proj_comm_1
	expert_32 -> agg_1
	expert_33 -> agg_1
	expert_34 -> agg_1
	expert_35 -> agg_1
	expert_36 -> agg_1
	expert_37 -> agg_1
	expert_38 -> agg_1
	expert_39 -> agg_1
	out_proj_comm_1 -> agg_1
	agg_1 -> pipe_comm_1_2
	pipe_comm_1_2 -> embed_2
	embed_2 -> gate_2
	gate_2 -> route_2 [style=dashed]
	route_2 -> expert_64
	route_2 -> expert_65
	route_2 -> expert_66
	route_2 -> expert_67
	route_2 -> expert_68
	route_2 -> expert_69
	route_2 -> expert_70
	route_2 -> expert_71
	expert_64 -> qkv_proj_2
	expert_65 -> qkv_proj_2
	expert_66 -> qkv_proj_2
	expert_67 -> qkv_proj_2
	expert_68 -> qkv_proj_2
	expert_69 -> qkv_proj_2
	expert_70 -> qkv_proj_2
	expert_71 -> qkv_proj_2
	qkv_proj_2 -> qkv_comm_2
	qkv_comm_2 -> attn_scores_2
	attn_scores_2 -> attn_apply_2
	attn_apply_2 -> attn_sp_comm_2
	attn_sp_comm_2 -> out_proj_2
	out_proj_2 -> out_proj_comm_2
	expert_64 -> agg_2
	expert_65 -> agg_2
	expert_66 -> agg_2
	expert_67 -> agg_2
	expert_68 -> agg_2
	expert_69 -> agg_2
	expert_70 -> agg_2
	expert_71 -> agg_2
	out_proj_comm_2 -> agg_2
	agg_2 -> pipe_comm_2_3
	pipe_comm_2_3 -> embed_3
	embed_3 -> gate_3
	gate_3 -> route_3 [style=dashed]
	route_3 -> expert_96
	route_3 -> expert_97
	route_3 -> expert_98
	route_3 -> expert_99
	route_3 -> expert_100
	route_3 -> expert_101
	route_3 -> expert_102
	route_3 -> expert_103
	expert_96 -> qkv_proj_3
	expert_97 -> qkv_proj_3
	expert_98 -> qkv_proj_3
	expert_99 -> qkv_proj_3
	expert_100 -> qkv_proj_3
	expert_101 -> qkv_proj_3
	expert_102 -> qkv_proj_3
	expert_103 -> qkv_proj_3
	qkv_proj_3 -> qkv_comm_3
	qkv_comm_3 -> attn_scores_3
	attn_scores_3 -> attn_apply_3
	attn_apply_3 -> attn_sp_comm_3
	attn_sp_comm_3 -> out_proj_3
	out_proj_3 -> out_proj_comm_3
	expert_96 -> agg_3
	expert_97 -> agg_3
	expert_98 -> agg_3
	expert_99 -> agg_3
	expert_100 -> agg_3
	expert_101 -> agg_3
	expert_102 -> agg_3
	expert_103 -> agg_3
	out_proj_comm_3 -> agg_3
	agg_3 -> output
}
