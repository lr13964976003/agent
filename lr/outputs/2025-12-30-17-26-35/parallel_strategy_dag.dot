digraph "Qwen3-235B Parallel Strategy Deployment DAG" {
	graph [comment="Qwen3-235B Parallel Strategy Deployment DAG", dpi=300, rankdir=TB, size="30,40"]
	node [fontname=Arial, fontsize=10]
	edge [fontname=Arial, fontsize=8]
	subgraph cluster_stage0 {
		graph [fillcolor=lightcoral, label="Pipeline Stage 0: Layers 0-23\\nGPUs: 0-31", style=rounded]
		embed_0 [fillcolor=lightblue, label="Token Embedding\\nGPU: 0-7\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=rectangle, style=filled]
		gate_0 [fillcolor=lightyellow, label="Gate Selection\\nGPU: 0-7\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=32]", shape=parallelogram, style=filled]
		route_0 [fillcolor=lightgreen, label="Expert Routing\\nGPU: 0-31\\nInput: [batch_size=128, seq_len=2048, experts=32]\\nOutput: [batch_size=128, seq_len=2048, experts=1]", shape=ellipse, style=filled]
		expert_0 [fillcolor=lightblue, label="Expert 0 Computation\\nGPU: 0\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_1 [fillcolor=lightblue, label="Expert 1 Computation\\nGPU: 1\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_2 [fillcolor=lightblue, label="Expert 2 Computation\\nGPU: 2\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_3 [fillcolor=lightblue, label="Expert 3 Computation\\nGPU: 3\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_4 [fillcolor=lightblue, label="Expert 4 Computation\\nGPU: 4\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_5 [fillcolor=lightblue, label="Expert 5 Computation\\nGPU: 5\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_6 [fillcolor=lightblue, label="Expert 6 Computation\\nGPU: 6\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_7 [fillcolor=lightblue, label="Expert 7 Computation\\nGPU: 7\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		attn_0 [fillcolor=lightblue, label="Attention (TP+SP)\\nGPU: 0-7\\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=rectangle, style=filled]
		attn_comm_0 [fillcolor=lightgreen, label="Attention All-Reduce\\nGPU: 0-7\\nInput: [batch_size=16, seq_len=1024, heads=8, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=ellipse, style=filled]
		agg_0 [fillcolor=lightyellow, label="Expert Aggregation\\nGPU: 0-7\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=parallelogram, style=filled]
	}
	subgraph cluster_stage1 {
		graph [fillcolor=lightsteelblue, label="Pipeline Stage 1: Layers 24-47\\nGPUs: 32-63", style=rounded]
		embed_1 [fillcolor=lightblue, label="Token Embedding\\nGPU: 32-39\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=rectangle, style=filled]
		gate_1 [fillcolor=lightyellow, label="Gate Selection\\nGPU: 32-39\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=32]", shape=parallelogram, style=filled]
		route_1 [fillcolor=lightgreen, label="Expert Routing\\nGPU: 32-63\\nInput: [batch_size=128, seq_len=2048, experts=32]\\nOutput: [batch_size=128, seq_len=2048, experts=1]", shape=ellipse, style=filled]
		expert_32 [fillcolor=lightblue, label="Expert 32 Computation\\nGPU: 32\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_33 [fillcolor=lightblue, label="Expert 33 Computation\\nGPU: 33\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_34 [fillcolor=lightblue, label="Expert 34 Computation\\nGPU: 34\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_35 [fillcolor=lightblue, label="Expert 35 Computation\\nGPU: 35\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_36 [fillcolor=lightblue, label="Expert 36 Computation\\nGPU: 36\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_37 [fillcolor=lightblue, label="Expert 37 Computation\\nGPU: 37\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_38 [fillcolor=lightblue, label="Expert 38 Computation\\nGPU: 38\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_39 [fillcolor=lightblue, label="Expert 39 Computation\\nGPU: 39\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		attn_1 [fillcolor=lightblue, label="Attention (TP+SP)\\nGPU: 32-39\\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=rectangle, style=filled]
		attn_comm_1 [fillcolor=lightgreen, label="Attention All-Reduce\\nGPU: 32-39\\nInput: [batch_size=16, seq_len=1024, heads=8, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=ellipse, style=filled]
		agg_1 [fillcolor=lightyellow, label="Expert Aggregation\\nGPU: 32-39\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=parallelogram, style=filled]
	}
	subgraph cluster_stage2 {
		graph [fillcolor=lightseagreen, label="Pipeline Stage 2: Layers 48-71\\nGPUs: 64-95", style=rounded]
		embed_2 [fillcolor=lightblue, label="Token Embedding\\nGPU: 64-71\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=rectangle, style=filled]
		gate_2 [fillcolor=lightyellow, label="Gate Selection\\nGPU: 64-71\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=32]", shape=parallelogram, style=filled]
		route_2 [fillcolor=lightgreen, label="Expert Routing\\nGPU: 64-95\\nInput: [batch_size=128, seq_len=2048, experts=32]\\nOutput: [batch_size=128, seq_len=2048, experts=1]", shape=ellipse, style=filled]
		expert_64 [fillcolor=lightblue, label="Expert 64 Computation\\nGPU: 64\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_65 [fillcolor=lightblue, label="Expert 65 Computation\\nGPU: 65\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_66 [fillcolor=lightblue, label="Expert 66 Computation\\nGPU: 66\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_67 [fillcolor=lightblue, label="Expert 67 Computation\\nGPU: 67\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_68 [fillcolor=lightblue, label="Expert 68 Computation\\nGPU: 68\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_69 [fillcolor=lightblue, label="Expert 69 Computation\\nGPU: 69\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_70 [fillcolor=lightblue, label="Expert 70 Computation\\nGPU: 70\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_71 [fillcolor=lightblue, label="Expert 71 Computation\\nGPU: 71\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		attn_2 [fillcolor=lightblue, label="Attention (TP+SP)\\nGPU: 64-71\\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=rectangle, style=filled]
		attn_comm_2 [fillcolor=lightgreen, label="Attention All-Reduce\\nGPU: 64-71\\nInput: [batch_size=16, seq_len=1024, heads=8, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=ellipse, style=filled]
		agg_2 [fillcolor=lightyellow, label="Expert Aggregation\\nGPU: 64-71\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=parallelogram, style=filled]
	}
	subgraph cluster_stage3 {
		graph [fillcolor=lightsalmon, label="Pipeline Stage 3: Layers 72-93\\nGPUs: 96-127", style=rounded]
		embed_3 [fillcolor=lightblue, label="Token Embedding\\nGPU: 96-103\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=rectangle, style=filled]
		gate_3 [fillcolor=lightyellow, label="Gate Selection\\nGPU: 96-103\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, experts=32]", shape=parallelogram, style=filled]
		route_3 [fillcolor=lightgreen, label="Expert Routing\\nGPU: 96-127\\nInput: [batch_size=128, seq_len=2048, experts=32]\\nOutput: [batch_size=128, seq_len=2048, experts=1]", shape=ellipse, style=filled]
		expert_96 [fillcolor=lightblue, label="Expert 96 Computation\\nGPU: 96\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_97 [fillcolor=lightblue, label="Expert 97 Computation\\nGPU: 97\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_98 [fillcolor=lightblue, label="Expert 98 Computation\\nGPU: 98\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_99 [fillcolor=lightblue, label="Expert 99 Computation\\nGPU: 99\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_100 [fillcolor=lightblue, label="Expert 100 Computation\\nGPU: 100\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_101 [fillcolor=lightblue, label="Expert 101 Computation\\nGPU: 101\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_102 [fillcolor=lightblue, label="Expert 102 Computation\\nGPU: 102\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		expert_103 [fillcolor=lightblue, label="Expert 103 Computation\\nGPU: 103\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=16, seq_len=2048, hidden=512]", shape=rectangle, style=filled]
		attn_3 [fillcolor=lightblue, label="Attention (TP+SP)\\nGPU: 96-103\\nInput: [batch_size=128, seq_len=2048, heads=64, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=rectangle, style=filled]
		attn_comm_3 [fillcolor=lightgreen, label="Attention All-Reduce\\nGPU: 96-103\\nInput: [batch_size=16, seq_len=1024, heads=8, d_k=64]\\nOutput: [batch_size=128, seq_len=2048, heads=64, d_k=64]", shape=ellipse, style=filled]
		agg_3 [fillcolor=lightyellow, label="Expert Aggregation\\nGPU: 96-103\\nInput: [batch_size=16, seq_len=2048, hidden=512]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=parallelogram, style=filled]
	}
	input [fillcolor=lightgray, label="Input\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=rectangle, style=filled]
	output [fillcolor=lightgray, label="Output\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, vocab_size=128000]", shape=rectangle, style=filled]
	pipe_comm_0_1 [fillcolor=lightgreen, label="Pipeline Communication\\nGPU: 31->32\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=ellipse, style=filled]
	pipe_comm_1_2 [fillcolor=lightgreen, label="Pipeline Communication\\nGPU: 63->64\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=ellipse, style=filled]
	pipe_comm_2_3 [fillcolor=lightgreen, label="Pipeline Communication\\nGPU: 95->96\\nInput: [batch_size=128, seq_len=2048, hidden=4096]\\nOutput: [batch_size=128, seq_len=2048, hidden=4096]", shape=ellipse, style=filled]
	input -> embed_0
	embed_0 -> gate_0
	gate_0 -> route_0 [style=dashed]
	route_0 -> expert_0
	route_0 -> expert_1
	route_0 -> expert_2
	route_0 -> expert_3
	route_0 -> expert_4
	route_0 -> expert_5
	route_0 -> expert_6
	route_0 -> expert_7
	expert_0 -> attn_0
	expert_1 -> attn_0
	expert_2 -> attn_0
	expert_3 -> attn_0
	expert_4 -> attn_0
	expert_5 -> attn_0
	expert_6 -> attn_0
	expert_7 -> attn_0
	attn_0 -> attn_comm_0
	attn_comm_0 -> agg_0
	expert_0 -> agg_0
	expert_1 -> agg_0
	expert_2 -> agg_0
	expert_3 -> agg_0
	expert_4 -> agg_0
	expert_5 -> agg_0
	expert_6 -> agg_0
	expert_7 -> agg_0
	agg_0 -> pipe_comm_0_1
	pipe_comm_0_1 -> embed_1
	embed_1 -> gate_1
	gate_1 -> route_1 [style=dashed]
	route_1 -> expert_32
	route_1 -> expert_33
	route_1 -> expert_34
	route_1 -> expert_35
	route_1 -> expert_36
	route_1 -> expert_37
	route_1 -> expert_38
	route_1 -> expert_39
	expert_32 -> attn_1
	expert_33 -> attn_1
	expert_34 -> attn_1
	expert_35 -> attn_1
	expert_36 -> attn_1
	expert_37 -> attn_1
	expert_38 -> attn_1
	expert_39 -> attn_1
	attn_1 -> attn_comm_1
	attn_comm_1 -> agg_1
	expert_32 -> agg_1
	expert_33 -> agg_1
	expert_34 -> agg_1
	expert_35 -> agg_1
	expert_36 -> agg_1
	expert_37 -> agg_1
	expert_38 -> agg_1
	expert_39 -> agg_1
	agg_1 -> pipe_comm_1_2
	pipe_comm_1_2 -> embed_2
	embed_2 -> gate_2
	gate_2 -> route_2 [style=dashed]
	route_2 -> expert_64
	route_2 -> expert_65
	route_2 -> expert_66
	route_2 -> expert_67
	route_2 -> expert_68
	route_2 -> expert_69
	route_2 -> expert_70
	route_2 -> expert_71
	expert_64 -> attn_2
	expert_65 -> attn_2
	expert_66 -> attn_2
	expert_67 -> attn_2
	expert_68 -> attn_2
	expert_69 -> attn_2
	expert_70 -> attn_2
	expert_71 -> attn_2
	attn_2 -> attn_comm_2
	attn_comm_2 -> agg_2
	expert_64 -> agg_2
	expert_65 -> agg_2
	expert_66 -> agg_2
	expert_67 -> agg_2
	expert_68 -> agg_2
	expert_69 -> agg_2
	expert_70 -> agg_2
	expert_71 -> agg_2
	agg_2 -> pipe_comm_2_3
	pipe_comm_2_3 -> embed_3
	embed_3 -> gate_3
	gate_3 -> route_3 [style=dashed]
	route_3 -> expert_96
	route_3 -> expert_97
	route_3 -> expert_98
	route_3 -> expert_99
	route_3 -> expert_100
	route_3 -> expert_101
	route_3 -> expert_102
	route_3 -> expert_103
	expert_96 -> attn_3
	expert_97 -> attn_3
	expert_98 -> attn_3
	expert_99 -> attn_3
	expert_100 -> attn_3
	expert_101 -> attn_3
	expert_102 -> attn_3
	expert_103 -> attn_3
	attn_3 -> attn_comm_3
	attn_comm_3 -> agg_3
	expert_96 -> agg_3
	expert_97 -> agg_3
	expert_98 -> agg_3
	expert_99 -> agg_3
	expert_100 -> agg_3
	expert_101 -> agg_3
	expert_102 -> agg_3
	expert_103 -> agg_3
	agg_3 -> output
}