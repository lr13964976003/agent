digraph "Qwen3-235B Parallel Strategy Deployment DAG之象，digraph "Qwen3-235B Parallel Strategy Deployment DAG" {
	graph [rankdir=TB]
	node [fillcolor=lightblue, shape=rectangle, style=filled]
	subgraph cluster_stage0 {
		graph [fillcolor=lightblue, label="Pipeline Stage 0: GPUs 0-7 (Layers 0-22)", style="rounded,filled"]
		embed_0 [label="Embedding (GPU-0)\nInput: [batch_size=128, seq_len=2048]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		attn_qkv_0_0 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-0)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_qkv_0_1 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-1)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_0_0 [fillcolor=lightgreen, label="Attn Scores (GPU-0)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_0_1 [fillcolor=lightgreen, label="Attn Scores (GPU-1)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_out_0_0 [fillcolor=lightgreen, label="Attn Output Proj (GPU-0)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		attn_out_0_1 [fillcolor=lightgreen, label="Attn Output Proj (GPU-1)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		gate_0 [fillcolor=lightyellow, label="Gate Routing (GPU-2)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, num_experts=128]", shape=parallelogram, style=dashed]
		expert_0_0 [fillcolor=lightcoral, label="Expert 0 (GPU-0)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_0_1 [fillcolor=lightcoral, label="Expert 1 (GPU-1)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_0_2 [fillcolor=lightcoral, label="Expert 2 (GPU-2)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_0_3 [fillcolor=lightcoral, label="Expert 3 (GPU-3)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_agg_0 [fillcolor=lightyellow, label="Expert Aggregation (GPU-4)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=parallelogram]
	}
	subgraph cluster_stage1 {
		graph [fillcolor=lightgreen, label="Pipeline Stage 1: GPUs 8-15 (Layers 23-46)", style="rounded,filled"]
		attn_qkv_23_0 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-8)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_qkv_23_1 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-9)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_23_0 [fillcolor=lightgreen, label="Attn Scores (GPU-8)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_23_1 [fillcolor=lightgreen, label="Attn Scores (GPU-9)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_out_23_0 [fillcolor=lightgreen, label="Attn Output Proj (GPU-8)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		attn_out_23_1 [fillcolor=lightgreen, label="Attn Output Proj (GPU-9)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		gate_23 [fillcolor=lightyellow, label="Gate Routing (GPU-10)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, num_experts=128]", shape=parallelogram, style=dashed]
		expert_23_0 [fillcolor=lightcoral, label="Expert 0 (GPU-8)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_23_1 [fillcolor=lightcoral, label="Expert 1 (GPU-9)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_23_2 [fillcolor=lightcoral, label="Expert 2 (GPU-10)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_23_3 [fillcolor=lightcoral, label="Expert 3 (GPU-11)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_agg_23 [fillcolor=lightyellow, label="Expert Aggregation (GPU-12)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=parallelogram]
	}
	subgraph cluster_stage2 {
		graph [fillcolor=lightyellow, label="Pipeline Stage 2: GPUs 16-23 (Layers 47-69)", style="rounded,filled"]
		attn_qkv_47_0 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-16)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_qkv_47_1 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-17)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_47_0 [fillcolor=lightgreen, label="Attn Scores (GPU-16)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_47_1 [fillcolor=lightgreen, label="Attn Scores (GPU-17)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_out_47_0 [fillcolor=lightgreen, label="Attn Output Proj (GPU-16)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		attn_out_47_1 [fillcolor=lightgreen, label="Attn Output Proj (GPU-17)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		gate_47 [fillcolor=lightyellow, label="Gate Routing (GPU-18)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, num_experts=128]", shape=parallelogram, style=dashed]
		expert_47_0 [fillcolor=lightcoral, label="Expert 0 (GPU-16)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_47_1 [fillcolor=lightcoral, label="Expert 1 (GPU-17)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_47_2 [fillcolor=lightcoral, label="Expert 2 (GPU-18)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_47_3 [fillcolor=lightcoral, label="Expert 3 (GPU-19)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_agg_47 [fillcolor=lightyellow, label="Expert Aggregation (GPU-20)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=parallelogram]
	}
	subgraph cluster_stage3 {
		graph [fillcolor=lightcoral, label="Pipeline Stage 3: GPUs 24-31 (Layers 70-93)", style="rounded,filled"]
		attn_qkv_70_0 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-24)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_qkv_70_1 [fillcolor=lightgreen, label="Attn QKV Proj (GPU-25)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_70_0 [fillcolor=lightgreen, label="Attn Scores (GPU-24)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_score_70_1 [fillcolor=lightgreen, label="Attn Scores (GPU-25)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=32, d_k=64]"]
		attn_out_70_0 [fillcolor=lightgreen, label="Attn Output Proj (GPU-24)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		attn_out_70_1 [fillcolor=lightgreen, label="Attn Output Proj (GPU-25)\nInput: [batch_size=128, seq_len=2048, heads=32, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		gate_70 [fillcolor=lightyellow, label="Gate Routing (GPU-26)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, num_experts=128]", shape=parallelogram, style=dashed]
		expert_70_0 [fillcolor=lightcoral, label="Expert 0 (GPU-24)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_70_1 [fillcolor=lightcoral, label="Expert 1 (GPU-25)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_70_2 [fillcolor=lightcoral, label="Expert 2 (GPU-26)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_70_3 [fillcolor=lightcoral, label="Expert 3 (GPU-27)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]"]
		expert_agg_70 [fillcolor=lightyellow, label="Expert Aggregation (GPU-28)\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=parallelogram]
	}
	input [fillcolor=white, label="Input\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=ellipse]
	output [fillcolor=white, label="Output\nInput: [batch_size=128, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=2048, hidden_dim=4096]", shape=ellipse]
	input -> embed_0
	embed_0 -> attn_qkv_0_0
	embed_0 -> attn_qkv_0_1
	attn_qkv_0_0 -> attn_score_0_0
	attn_qkv_0_1 -> attn_score_0_1
	attn_score_0_0 -> attn_out_0_0
	attn_score_0_1 -> attn_out_0_1
	attn_qkv_0_0 -> attn_score_0_1 [color=red, label="TP Comm", style=dashed]
	attn_qkv_0_1 -> attn_score_0_0 [color=red, label="TP Comm", style=dashed]
	attn_out_0_0 -> gate_0 [style=dashed]
	attn_out_0_1 -> gate_0 [style=dashed]
	gate_0 -> expert_0_0 [color=blue, label=routing, style=dashed]
	gate_0 -> expert_0_1 [color=blue, label=routing, style=dashed]
	gate_0 -> expert_0_2 [color=blue, label=routing, style=dashed]
	gate_0 -> expert_0_3 [color=blue, label=routing, style=dashed]
	expert_0_0 -> expert_agg_0
	expert_0_1 -> expert_agg_0
	expert_0_2 -> expert_agg_0
	expert_0_3 -> expert_agg_0
	expert_agg_0 -> attn_qkv_23_0
	expert_agg_0 -> attn_qkv_23_1
	attn_qkv_23_0 -> attn_score_23_0
	attn_qkv_23_1 -> attn_score_23_1
	attn_score_23_0 -> attn_out_23_0
	attn_score_23_1 -> attn_out_23_1
	attn_qkv_23_0 -> attn_score_23_1 [color=red, label="TP Comm", style=dashed]
	attn_qkv_23_1 -> attn_score_23_0 [color=red, label="TP Comm", style=dashed]
	attn_out_23_0 -> gate_23 [style=dashed]
	attn_out_23_1 -> gate_23 [style=dashed]
	gate_23 -> expert_23_0 [color=blue, label=routing, style=dashed]
	gate_23 -> expert_23_1 [color=blue, label=routing, style=dashed]
	gate_23 -> expert_23_2 [color=blue, label=routing, style=dashed]
	gate_23 -> expert_23_3 [color=blue, label=routing, style=dashed]
	expert_23_0 -> expert_agg_23
	expert_23_1 -> expert_agg_23
	expert_23_2 -> expert_agg_23
	expert_23_3 -> expert_agg_23
	expert_agg_23 -> attn_qkv_47_0
	expert_agg_23 -> attn_qkv_47_1
	attn_qkv_47_0 -> attn_score_47_0
	attn_qkv_47_1 -> attn_score_47_1
	attn_score_47_0 -> attn_out_47_0
	attn_score_47_1 -> attn_out_47_1
	attn_qkv_47_0 -> attn_score_47_1 [color=red, label="TP Comm", style=dashed]
	attn_qkv_47_1 -> attn_score_47_0 [color=red, label="TP Comm", style=dashed]
	attn_out_47_0 -> gate_47 [style=dashed]
	attn_out_47_1 -> gate_47 [style=dashed]
	gate_47 -> expert_47_0 [color=blue, label=routing, style=dashed]
	gate_47 -> expert_47_1 [color=blue, label=routing, style=dashed]
	gate_47 -> expert_47_2 [color=blue, label=routing, style=dashed]
	gate_47 -> expert_47_3 [color=blue, label=routing, style=dashed]
	expert_47_0 -> expert_agg_47
	expert_47_1 -> expert_agg_47
	expert_47_2 -> expert_agg_47
	expert_47_3 -> expert_agg_47
	expert_agg_47 -> attn_qkv_70_0
	expert_agg_47 -> attn_qkv_70_1
	attn_qkv_70_0 -> attn_score_70_0
	attn_qkv_70_1 -> attn_score_70_1
	attn_score_70_0 -> attn_out_70_0
	attn_score_70_1 -> attn_out_70_1
	attn_qkv_70_0 -> attn_score_70_1 [color=red, label="TP Comm", style=dashed]
	attn_qkv_70_1 -> attn_score_70_0 [color=red, label="TP Comm", style=dashed]
	attn_out_70_0 -> gate_70 [style=dashed]
	attn_out_70_1 -> gate_70 [style=dashed]
	gate_70 -> expert_70_0 [color=blue, label=routing, style=dashed]
	gate_70 -> expert_70_1 [color=blue, label=routing, style=dashed]
	gate_70 -> expert_70_2 [color=blue, label=routing, style=dashed]
	gate_70 -> expert_70_3 [color=blue, label=routing, style=dashed]
	expert_70_0 -> expert_agg_70
	expert_70_1 -> expert_agg_70
	expert_70_2 -> expert_agg_70
	expert_70_3 -> expert_agg_70
	expert_agg_70 -> output
}