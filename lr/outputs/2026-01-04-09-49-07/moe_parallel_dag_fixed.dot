// Qwen3-235B MoE Parallel Strategy DAG - Fixed Version
digraph {
    rankdir=TB
    size="20,30"
    node [fontname=Arial fontsize=10]
    
    // Define node styles
    node [fillcolor=lightblue shape=ellipse style=filled]  // Communication
    node [fillcolor=lightgreen shape=rectangle style=filled]  // Computation  
    node [fillcolor=lightyellow shape=parallelogram style=filled]  // Routing/Aggregation
    
    // Input node
    input [label="Input\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle]
    
    // Token Embedding on all GPUs
    embed_gpu0 [label="GPU0: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu0
    embed_gpu1 [label="GPU1: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu1
    embed_gpu2 [label="GPU2: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu2
    embed_gpu3 [label="GPU3: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu3
    embed_gpu4 [label="GPU4: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu4
    embed_gpu5 [label="GPU5: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu5
    embed_gpu6 [label="GPU6: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu6
    embed_gpu7 [label="GPU7: Token Embedding\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    input -> embed_gpu7

    // GPU0 Attention operations
    attn_qkv_gpu0_layer1 [label="GPU0: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu0_layer1 [label="GPU0: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu0_layer1 [label="GPU0: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu0_layer1 [label="GPU0: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu0 -> attn_qkv_gpu0_layer1
    attn_qkv_gpu0_layer1 -> comm_qkv_gpu0_layer1
    comm_qkv_gpu0_layer1 -> attn_comp_gpu0_layer1
    attn_comp_gpu0_layer1 -> comm_attn_out_gpu0_layer1

    // GPU1 Attention operations
    attn_qkv_gpu1_layer1 [label="GPU1: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu1_layer1 [label="GPU1: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu1_layer1 [label="GPU1: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu1_layer1 [label="GPU1: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu1 -> attn_qkv_gpu1_layer1
    attn_qkv_gpu1_layer1 -> comm_qkv_gpu1_layer1
    comm_qkv_gpu1_layer1 -> attn_comp_gpu1_layer1
    attn_comp_gpu1_layer1 -> comm_attn_out_gpu1_layer1

    // GPU2 Attention operations
    attn_qkv_gpu2_layer1 [label="GPU2: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu2_layer1 [label="GPU2: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu2_layer1 [label="GPU2: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu2_layer1 [label="GPU2: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu2 -> attn_qkv_gpu2_layer1
    attn_qkv_gpu2_layer1 -> comm_qkv_gpu2_layer1
    comm_qkv_gpu2_layer1 -> attn_comp_gpu2_layer1
    attn_comp_gpu2_layer1 -> comm_attn_out_gpu2_layer1

    // GPU3 Attention operations
    attn_qkv_gpu3_layer1 [label="GPU3: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu3_layer1 [label="GPU3: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu3_layer1 [label="GPU3: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu3_layer1 [label="GPU3: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu3 -> attn_qkv_gpu3_layer1
    attn_qkv_gpu3_layer1 -> comm_qkv_gpu3_layer1
    comm_qkv_gpu3_layer1 -> attn_comp_gpu3_layer1
    attn_comp_gpu3_layer1 -> comm_attn_out_gpu3_layer1

    // GPU4 Attention operations
    attn_qkv_gpu4_layer1 [label="GPU4: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu4_layer1 [label="GPU4: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu4_layer1 [label="GPU4: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu4_layer1 [label="GPU4: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu4 -> attn_qkv_gpu4_layer1
    attn_qkv_gpu4_layer1 -> comm_qkv_gpu4_layer1
    comm_qkv_gpu4_layer1 -> attn_comp_gpu4_layer1
    attn_comp_gpu4_layer1 -> comm_attn_out_gpu4_layer1

    // GPU5 Attention operations
    attn_qkv_gpu5_layer1 [label="GPU5: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu5_layer1 [label="GPU5: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu5_layer1 [label="GPU5: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu5_layer1 [label="GPU5: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu5 -> attn_qkv_gpu5_layer1
    attn_qkv_gpu5_layer1 -> comm_qkv_gpu5_layer1
    comm_qkv_gpu5_layer1 -> attn_comp_gpu5_layer1
    attn_comp_gpu5_layer1 -> comm_attn_out_gpu5_layer1

    // GPU6 Attention operations
    attn_qkv_gpu6_layer1 [label="GPU6: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu6_layer1 [label="GPU6: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu6_layer1 [label="GPU6: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu6_layer1 [label="GPU6: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu6 -> attn_qkv_gpu6_layer1
    attn_qkv_gpu6_layer1 -> comm_qkv_gpu6_layer1
    comm_qkv_gpu6_layer1 -> attn_comp_gpu6_layer1
    attn_comp_gpu6_layer1 -> comm_attn_out_gpu6_layer1

    // GPU7 Attention operations
    attn_qkv_gpu7_layer1 [label="GPU7: Layer1 Attention QKV Proj\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_qkv_gpu7_layer1 [label="GPU7: All-Reduce QKV\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightblue shape=ellipse]
    attn_comp_gpu7_layer1 [label="GPU7: Layer1 Attention Compute\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle]
    comm_attn_out_gpu7_layer1 [label="GPU7: All-Reduce Attention Output\nInput: [batch_size=128, seq_len=2048, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    embed_gpu7 -> attn_qkv_gpu7_layer1
    attn_qkv_gpu7_layer1 -> comm_qkv_gpu7_layer1
    comm_qkv_gpu7_layer1 -> attn_comp_gpu7_layer1
    attn_comp_gpu7_layer1 -> comm_attn_out_gpu7_layer1

    gate_gpu0_layer1 [label="GPU0: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu0_layer1 -> gate_gpu0_layer1

    gate_gpu1_layer1 [label="GPU1: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu1_layer1 -> gate_gpu1_layer1

    gate_gpu2_layer1 [label="GPU2: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu2_layer1 -> gate_gpu2_layer1

    gate_gpu3_layer1 [label="GPU3: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu3_layer1 -> gate_gpu3_layer1

    gate_gpu4_layer1 [label="GPU4: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu4_layer1 -> gate_gpu4_layer1

    gate_gpu5_layer1 [label="GPU5: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu5_layer1 -> gate_gpu5_layer1

    gate_gpu6_layer1 [label="GPU6: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu6_layer1 -> gate_gpu6_layer1

    gate_gpu7_layer1 [label="GPU7: Layer1 Gate Routing\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, experts=8]" fillcolor=lightyellow shape=parallelogram]
    comm_attn_out_gpu7_layer1 -> gate_gpu7_layer1
    expert_gpu0_exp0_layer1 [label="GPU0: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu0_exp1_layer1 [label="GPU0: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu0_exp2_layer1 [label="GPU0: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu0_exp3_layer1 [label="GPU0: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu1_exp0_layer1 [label="GPU1: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu1_exp1_layer1 [label="GPU1: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu1_exp2_layer1 [label="GPU1: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu1_exp3_layer1 [label="GPU1: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu2_exp0_layer1 [label="GPU2: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu2_exp1_layer1 [label="GPU2: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu2_exp2_layer1 [label="GPU2: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu2_exp3_layer1 [label="GPU2: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu3_exp0_layer1 [label="GPU3: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu3_exp1_layer1 [label="GPU3: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu3_exp2_layer1 [label="GPU3: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu3_exp3_layer1 [label="GPU3: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu4_exp0_layer1 [label="GPU4: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu4_exp1_layer1 [label="GPU4: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu4_exp2_layer1 [label="GPU4: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu4_exp3_layer1 [label="GPU4: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu5_exp0_layer1 [label="GPU5: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu5_exp1_layer1 [label="GPU5: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu5_exp2_layer1 [label="GPU5: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu5_exp3_layer1 [label="GPU5: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu6_exp0_layer1 [label="GPU6: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu6_exp1_layer1 [label="GPU6: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu6_exp2_layer1 [label="GPU6: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu6_exp3_layer1 [label="GPU6: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu7_exp0_layer1 [label="GPU7: Expert 0\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu7_exp1_layer1 [label="GPU7: Expert 1\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu7_exp2_layer1 [label="GPU7: Expert 2\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]
    expert_gpu7_exp3_layer1 [label="GPU7: Expert 3\nInput: [batch_size=?, seq_len=?, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=1536]" fillcolor=lightgreen shape=rectangle]

    comm_expert_send_gpu0_layer1 [label="GPU0: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu0_layer1 [label="GPU0: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu0_layer1 -> comm_expert_send_gpu0_layer1

    comm_expert_send_gpu1_layer1 [label="GPU1: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu1_layer1 [label="GPU1: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu1_layer1 -> comm_expert_send_gpu1_layer1

    comm_expert_send_gpu2_layer1 [label="GPU2: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu2_layer1 [label="GPU2: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu2_layer1 -> comm_expert_send_gpu2_layer1

    comm_expert_send_gpu3_layer1 [label="GPU3: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu3_layer1 [label="GPU3: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu3_layer1 -> comm_expert_send_gpu3_layer1

    comm_expert_send_gpu4_layer1 [label="GPU4: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu4_layer1 [label="GPU4: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu4_layer1 -> comm_expert_send_gpu4_layer1

    comm_expert_send_gpu5_layer1 [label="GPU5: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu5_layer1 [label="GPU5: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu5_layer1 -> comm_expert_send_gpu5_layer1

    comm_expert_send_gpu6_layer1 [label="GPU6: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu6_layer1 [label="GPU6: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu6_layer1 -> comm_expert_send_gpu6_layer1

    comm_expert_send_gpu7_layer1 [label="GPU7: Send Tokens to Experts\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=?, seq_len=?, hidden=4096]" fillcolor=lightblue shape=ellipse]
    comm_expert_recv_gpu7_layer1 [label="GPU7: Receive from Experts\nInput: [batch_size=?, seq_len=?, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=1536]" fillcolor=lightblue shape=ellipse]
    
    gate_gpu7_layer1 -> comm_expert_send_gpu7_layer1
    comm_expert_send_gpu0_layer1 -> expert_gpu0_exp0_layer1
    comm_expert_send_gpu0_layer1 -> expert_gpu0_exp1_layer1
    comm_expert_send_gpu0_layer1 -> expert_gpu0_exp2_layer1
    comm_expert_send_gpu0_layer1 -> expert_gpu0_exp3_layer1
    expert_gpu0_exp0_layer1 -> comm_expert_recv_gpu0_layer1
    expert_gpu0_exp1_layer1 -> comm_expert_recv_gpu0_layer1
    expert_gpu0_exp2_layer1 -> comm_expert_recv_gpu0_layer1
    expert_gpu0_exp3_layer1 -> comm_expert_recv_gpu0_layer1
    comm_expert_send_gpu1_layer1 -> expert_gpu1_exp0_layer1
    comm_expert_send_gpu1_layer1 -> expert_gpu1_exp1_layer1
    comm_expert_send_gpu1_layer1 -> expert_gpu1_exp2_layer1
    comm_expert_send_gpu1_layer1 -> expert_gpu1_exp3_layer1
    expert_gpu1_exp0_layer1 -> comm_expert_recv_gpu1_layer1
    expert_gpu1_exp1_layer1 -> comm_expert_recv_gpu1_layer1
    expert_gpu1_exp2_layer1 -> comm_expert_recv_gpu1_layer1
    expert_gpu1_exp3_layer1 -> comm_expert_recv_gpu1_layer1
    comm_expert_send_gpu2_layer1 -> expert_gpu2_exp0_layer1
    comm_expert_send_gpu2_layer1 -> expert_gpu2_exp1_layer1
    comm_expert_send_gpu2_layer1 -> expert_gpu2_exp2_layer1
    comm_expert_send_gpu2_layer1 -> expert_gpu2_exp3_layer1
    expert_gpu2_exp0_layer1 -> comm_expert_recv_gpu2_layer1
    expert_gpu2_exp1_layer1 -> comm_expert_recv_gpu2_layer1
    expert_gpu2_exp2_layer1 -> comm_expert_recv_gpu2_layer1
    expert_gpu2_exp3_layer1 -> comm_expert_recv_gpu2_layer1
    comm_expert_send_gpu3_layer1 -> expert_gpu3_exp0_layer1
    comm_expert_send_gpu3_layer1 -> expert_gpu3_exp1_layer1
    comm_expert_send_gpu3_layer1 -> expert_gpu3_exp2_layer1
    comm_expert_send_gpu3_layer1 -> expert_gpu3_exp3_layer1
    expert_gpu3_exp0_layer1 -> comm_expert_recv_gpu3_layer1
    expert_gpu3_exp1_layer1 -> comm_expert_recv_gpu3_layer1
    expert_gpu3_exp2_layer1 -> comm_expert_recv_gpu3_layer1
    expert_gpu3_exp3_layer1 -> comm_expert_recv_gpu3_layer1
    comm_expert_send_gpu4_layer1 -> expert_gpu4_exp0_layer1
    comm_expert_send_gpu4_layer1 -> expert_gpu4_exp1_layer1
    comm_expert_send_gpu4_layer1 -> expert_gpu4_exp2_layer1
    comm_expert_send_gpu4_layer1 -> expert_gpu4_exp3_layer1
    expert_gpu4_exp0_layer1 -> comm_expert_recv_gpu4_layer1
    expert_gpu4_exp1_layer1 -> comm_expert_recv_gpu4_layer1
    expert_gpu4_exp2_layer1 -> comm_expert_recv_gpu4_layer1
    expert_gpu4_exp3_layer1 -> comm_expert_recv_gpu4_layer1
    comm_expert_send_gpu5_layer1 -> expert_gpu5_exp0_layer1
    comm_expert_send_gpu5_layer1 -> expert_gpu5_exp1_layer1
    comm_expert_send_gpu5_layer1 -> expert_gpu5_exp2_layer1
    comm_expert_send_gpu5_layer1 -> expert_gpu5_exp3_layer1
    expert_gpu5_exp0_layer1 -> comm_expert_recv_gpu5_layer1
    expert_gpu5_exp1_layer1 -> comm_expert_recv_gpu5_layer1
    expert_gpu5_exp2_layer1 -> comm_expert_recv_gpu5_layer1
    expert_gpu5_exp3_layer1 -> comm_expert_recv_gpu5_layer1
    comm_expert_send_gpu6_layer1 -> expert_gpu6_exp0_layer1
    comm_expert_send_gpu6_layer1 -> expert_gpu6_exp1_layer1
    comm_expert_send_gpu6_layer1 -> expert_gpu6_exp2_layer1
    comm_expert_send_gpu6_layer1 -> expert_gpu6_exp3_layer1
    expert_gpu6_exp0_layer1 -> comm_expert_recv_gpu6_layer1
    expert_gpu6_exp1_layer1 -> comm_expert_recv_gpu6_layer1
    expert_gpu6_exp2_layer1 -> comm_expert_recv_gpu6_layer1
    expert_gpu6_exp3_layer1 -> comm_expert_recv_gpu6_layer1
    comm_expert_send_gpu7_layer1 -> expert_gpu7_exp0_layer1
    comm_expert_send_gpu7_layer1 -> expert_gpu7_exp1_layer1
    comm_expert_send_gpu7_layer1 -> expert_gpu7_exp2_layer1
    comm_expert_send_gpu7_layer1 -> expert_gpu7_exp3_layer1
    expert_gpu7_exp0_layer1 -> comm_expert_recv_gpu7_layer1
    expert_gpu7_exp1_layer1 -> comm_expert_recv_gpu7_layer1
    expert_gpu7_exp2_layer1 -> comm_expert_recv_gpu7_layer1
    expert_gpu7_exp3_layer1 -> comm_expert_recv_gpu7_layer1
    gate_gpu0_layer1 -> expert_gpu0_exp0_layer1 [label="GPU0 selects experts on GPU0" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu1_exp0_layer1 [label="GPU0 selects experts on GPU1" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu2_exp0_layer1 [label="GPU0 selects experts on GPU2" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu3_exp0_layer1 [label="GPU0 selects experts on GPU3" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu4_exp0_layer1 [label="GPU0 selects experts on GPU4" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu5_exp0_layer1 [label="GPU0 selects experts on GPU5" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu6_exp0_layer1 [label="GPU0 selects experts on GPU6" color=red style=dashed]
    gate_gpu0_layer1 -> expert_gpu7_exp0_layer1 [label="GPU0 selects experts on GPU7" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu0_exp0_layer1 [label="GPU1 selects experts on GPU0" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu1_exp0_layer1 [label="GPU1 selects experts on GPU1" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu2_exp0_layer1 [label="GPU1 selects experts on GPU2" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu3_exp0_layer1 [label="GPU1 selects experts on GPU3" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu4_exp0_layer1 [label="GPU1 selects experts on GPU4" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu5_exp0_layer1 [label="GPU1 selects experts on GPU5" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu6_exp0_layer1 [label="GPU1 selects experts on GPU6" color=red style=dashed]
    gate_gpu1_layer1 -> expert_gpu7_exp0_layer1 [label="GPU1 selects experts on GPU7" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu0_exp0_layer1 [label="GPU2 selects experts on GPU0" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu1_exp0_layer1 [label="GPU2 selects experts on GPU1" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu2_exp0_layer1 [label="GPU2 selects experts on GPU2" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu3_exp0_layer1 [label="GPU2 selects experts on GPU3" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu4_exp0_layer1 [label="GPU2 selects experts on GPU4" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu5_exp0_layer1 [label="GPU2 selects experts on GPU5" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu6_exp0_layer1 [label="GPU2 selects experts on GPU6" color=red style=dashed]
    gate_gpu2_layer1 -> expert_gpu7_exp0_layer1 [label="GPU2 selects experts on GPU7" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu0_exp0_layer1 [label="GPU3 selects experts on GPU0" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu1_exp0_layer1 [label="GPU3 selects experts on GPU1" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu2_exp0_layer1 [label="GPU3 selects experts on GPU2" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu3_exp0_layer1 [label="GPU3 selects experts on GPU3" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu4_exp0_layer1 [label="GPU3 selects experts on GPU4" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu5_exp0_layer1 [label="GPU3 selects experts on GPU5" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu6_exp0_layer1 [label="GPU3 selects experts on GPU6" color=red style=dashed]
    gate_gpu3_layer1 -> expert_gpu7_exp0_layer1 [label="GPU3 selects experts on GPU7" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu0_exp0_layer1 [label="GPU4 selects experts on GPU0" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu1_exp0_layer1 [label="GPU4 selects experts on GPU1" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu2_exp0_layer1 [label="GPU4 selects experts on GPU2" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu3_exp0_layer1 [label="GPU4 selects experts on GPU3" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu4_exp0_layer1 [label="GPU4 selects experts on GPU4" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu5_exp0_layer1 [label="GPU4 selects experts on GPU5" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu6_exp0_layer1 [label="GPU4 selects experts on GPU6" color=red style=dashed]
    gate_gpu4_layer1 -> expert_gpu7_exp0_layer1 [label="GPU4 selects experts on GPU7" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu0_exp0_layer1 [label="GPU5 selects experts on GPU0" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu1_exp0_layer1 [label="GPU5 selects experts on GPU1" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu2_exp0_layer1 [label="GPU5 selects experts on GPU2" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu3_exp0_layer1 [label="GPU5 selects experts on GPU3" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu4_exp0_layer1 [label="GPU5 selects experts on GPU4" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu5_exp0_layer1 [label="GPU5 selects experts on GPU5" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu6_exp0_layer1 [label="GPU5 selects experts on GPU6" color=red style=dashed]
    gate_gpu5_layer1 -> expert_gpu7_exp0_layer1 [label="GPU5 selects experts on GPU7" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu0_exp0_layer1 [label="GPU6 selects experts on GPU0" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu1_exp0_layer1 [label="GPU6 selects experts on GPU1" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu2_exp0_layer1 [label="GPU6 selects experts on GPU2" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu3_exp0_layer1 [label="GPU6 selects experts on GPU3" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu4_exp0_layer1 [label="GPU6 selects experts on GPU4" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu5_exp0_layer1 [label="GPU6 selects experts on GPU5" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu6_exp0_layer1 [label="GPU6 selects experts on GPU6" color=red style=dashed]
    gate_gpu6_layer1 -> expert_gpu7_exp0_layer1 [label="GPU6 selects experts on GPU7" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu0_exp0_layer1 [label="GPU7 selects experts on GPU0" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu1_exp0_layer1 [label="GPU7 selects experts on GPU1" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu2_exp0_layer1 [label="GPU7 selects experts on GPU2" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu3_exp0_layer1 [label="GPU7 selects experts on GPU3" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu4_exp0_layer1 [label="GPU7 selects experts on GPU4" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu5_exp0_layer1 [label="GPU7 selects experts on GPU5" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu6_exp0_layer1 [label="GPU7 selects experts on GPU6" color=red style=dashed]
    gate_gpu7_layer1 -> expert_gpu7_exp0_layer1 [label="GPU7 selects experts on GPU7" color=red style=dashed]

    expert_agg_gpu0_layer1 [label="GPU0: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu0_layer1 [label="GPU0: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu0_layer1 [label="GPU0: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu0_layer1 -> expert_agg_gpu0_layer1
    expert_agg_gpu0_layer1 -> mlp_gpu0_layer1
    mlp_gpu0_layer1 -> mlp_agg_gpu0_layer1

    expert_agg_gpu1_layer1 [label="GPU1: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu1_layer1 [label="GPU1: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu1_layer1 [label="GPU1: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu1_layer1 -> expert_agg_gpu1_layer1
    expert_agg_gpu1_layer1 -> mlp_gpu1_layer1
    mlp_gpu1_layer1 -> mlp_agg_gpu1_layer1

    expert_agg_gpu2_layer1 [label="GPU2: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu2_layer1 [label="GPU2: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu2_layer1 [label="GPU2: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu2_layer1 -> expert_agg_gpu2_layer1
    expert_agg_gpu2_layer1 -> mlp_gpu2_layer1
    mlp_gpu2_layer1 -> mlp_agg_gpu2_layer1

    expert_agg_gpu3_layer1 [label="GPU3: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu3_layer1 [label="GPU3: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu3_layer1 [label="GPU3: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu3_layer1 -> expert_agg_gpu3_layer1
    expert_agg_gpu3_layer1 -> mlp_gpu3_layer1
    mlp_gpu3_layer1 -> mlp_agg_gpu3_layer1

    expert_agg_gpu4_layer1 [label="GPU4: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu4_layer1 [label="GPU4: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu4_layer1 [label="GPU4: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu4_layer1 -> expert_agg_gpu4_layer1
    expert_agg_gpu4_layer1 -> mlp_gpu4_layer1
    mlp_gpu4_layer1 -> mlp_agg_gpu4_layer1

    expert_agg_gpu5_layer1 [label="GPU5: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu5_layer1 [label="GPU5: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu5_layer1 [label="GPU5: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu5_layer1 -> expert_agg_gpu5_layer1
    expert_agg_gpu5_layer1 -> mlp_gpu5_layer1
    mlp_gpu5_layer1 -> mlp_agg_gpu5_layer1

    expert_agg_gpu6_layer1 [label="GPU6: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu6_layer1 [label="GPU6: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu6_layer1 [label="GPU6: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu6_layer1 -> expert_agg_gpu6_layer1
    expert_agg_gpu6_layer1 -> mlp_gpu6_layer1
    mlp_gpu6_layer1 -> mlp_agg_gpu6_layer1

    expert_agg_gpu7_layer1 [label="GPU7: Expert Aggregation\nInput: [batch_size=128, seq_len=2048, experts=8, hidden=1536]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
    mlp_gpu7_layer1 [label="GPU7: MLP Compute\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightgreen shape=rectangle]
    mlp_agg_gpu7_layer1 [label="GPU7: MLP All-Reduce\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightblue shape=ellipse]
    
    comm_expert_recv_gpu7_layer1 -> expert_agg_gpu7_layer1
    expert_agg_gpu7_layer1 -> mlp_gpu7_layer1
    mlp_gpu7_layer1 -> mlp_agg_gpu7_layer1

    output [label="Output\nInput: [batch_size=128, seq_len=2048, hidden=4096]\nOutput: [batch_size=128, seq_len=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle]
    mlp_agg_gpu0_layer1 -> output
    mlp_agg_gpu1_layer1 -> output
    mlp_agg_gpu2_layer1 -> output
    mlp_agg_gpu3_layer1 -> output
    mlp_agg_gpu4_layer1 -> output
    mlp_agg_gpu5_layer1 -> output
    mlp_agg_gpu6_layer1 -> output
    mlp_agg_gpu7_layer1 -> output
}
