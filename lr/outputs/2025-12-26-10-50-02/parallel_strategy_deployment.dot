digraph G {
    // Graph settings
    nodesep=0.8;
    rankdir=TB;
    ranksep=1.2;
    splines=ortho;
    
    // Node style definitions
    node [fillcolor=lightblue, shape=ellipse, style=filled];  // Communication
    node [fillcolor=lightgreen, shape=rectangle, style=filled];  // Computation
    node [fillcolor=lightyellow, shape=rectangle, style=filled];  // Routing/Aggregation (changed from parallelogram to avoid rendering issues)
    
    // Input cluster
    subgraph cluster_input {
        label="Input Layer";
        style=rounded;
        fillcolor=lightgray;
        
        input [label="Input\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
               shape=box, fillcolor=white, style="rounded,filled"];
    }
    
    // Pipeline Stage 1: Layers 0-7 (GPUs 0-127)
    subgraph cluster_stage1 {
        label="Pipeline Stage 1: Layers 0-7 (GPUs 0-127)";
        style=rounded;
        fillcolor=lightcyan;
        
        // Layer 0
        layernorm0 [label="LayerNorm (Layer 0)\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                   fillcolor=lightgreen, shape=rectangle];
        
        // Attention block
        qkv_proj [label="QKV Projection TP=4\nGPU: 0-127 (TP groups)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, heads=4, d_k=32]", 
                 fillcolor=lightgreen, shape=rectangle];
        
        qkv_allreduce [label="TP All-Reduce QKV\nGPU: 0-127 (TP groups)\nInput: [batch_size=32, seq_len=1024, heads=4, d_k=32]\nOutput: [batch_size=32, seq_len=1024, heads=16, d_k=32]", 
                      fillcolor=lightblue, shape=ellipse];
        
        attention [label="Multi-Head Attention\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=32, seq_len=1024, heads=16, d_k=32]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        attn_out [label="Attention Output Projection TP=4\nGPU: 0-127 (TP groups)\nInput: [batch_size=32, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=32, seq_len=1024, token_dim=128]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        attn_allreduce [label="TP All-Reduce Attention\nGPU: 0-127 (TP groups)\nInput: [batch_size=32, seq_len=1024, token_dim=128]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                       fillcolor=lightblue, shape=ellipse];
        
        residual1 [label="Residual Connection\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        // MoE block
        moe_gate [label="MoE Gate (Expert Selection)\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, num_experts=16]", 
                  fillcolor=lightyellow, shape=rectangle];
        
        expert_route [label="Expert Routing EP=8\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, experts=2]", 
                      fillcolor=lightyellow, shape=rectangle];
        
        expert_0 [label="Expert 0 Computation\nGPU: 0-127 (2 experts per GPU)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        expert_1 [label="Expert 1 Computation\nGPU: 0-127 (2 experts per GPU)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        expert_combine [label="Expert Combination EP=8\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, experts=2]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                        fillcolor=lightyellow, shape=rectangle];
        
        moe_residual [label="MoE Residual Connection\nGPU: 0-127\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                      fillcolor=lightgreen, shape=rectangle];
    }
    
    // Pipeline communication between stages
    stage1_to_stage2 [label="Pipeline Communication PP=2\nGPU: 0-127 â†’ 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                      fillcolor=lightblue, shape=ellipse];
    
    // Pipeline Stage 2: Layers 8-15 (GPUs 128-255)
    subgraph cluster_stage2 {
        label="Pipeline Stage 2: Layers 8-15 (GPUs 128-255)";
        style=rounded;
        fillcolor=lightpink;
        
        layernorm8 [label="LayerNorm (Layer 8)\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                   fillcolor=lightgreen, shape=rectangle];
        
        qkv_proj2 [label="QKV Projection TP=4\nGPU: 128-255 (TP groups)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, heads=4, d_k=32]", 
                   fillcolor=lightgreen, shape=rectangle];
        
        qkv_allreduce2 [label="TP All-Reduce QKV\nGPU: 128-255 (TP groups)\nInput: [batch_size=32, seq_len=1024, heads=4, d_k=32]\nOutput: [batch_size=32, seq_len=1024, heads=16, d_k=32]", 
                        fillcolor=lightblue, shape=ellipse];
        
        attention2 [label="Multi-Head Attention\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=32, seq_len=1024, heads=16, d_k=32]", 
                    fillcolor=lightgreen, shape=rectangle];
        
        attn_out2 [label="Attention Output Projection TP=4\nGPU: 128-255 (TP groups)\nInput: [batch_size=32, seq_len=1024, heads=16, d_k=32]\nOutput: [batch_size=32, seq_len=1024, token_dim=128]", 
                   fillcolor=lightgreen, shape=rectangle];
        
        attn_allreduce2 [label="TP All-Reduce Attention\nGPU: 128-255 (TP groups)\nInput: [batch_size=32, seq_len=1024, token_dim=128]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                         fillcolor=lightblue, shape=ellipse];
        
        residual2 [label="Residual Connection\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                   fillcolor=lightgreen, shape=rectangle];
        
        // MoE block (ADDED - MISSING IN PREVIOUS VERSION)
        moe_gate2 [label="MoE Gate (Expert Selection)\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, num_experts=16]", 
                  fillcolor=lightyellow, shape=rectangle];
        
        expert_route2 [label="Expert Routing EP=8\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, experts=2]", 
                      fillcolor=lightyellow, shape=rectangle];
        
        expert_2 [label="Expert 2 Computation\nGPU: 128-255 (2 experts per GPU)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        expert_3 [label="Expert 3 Computation\nGPU: 128-255 (2 experts per GPU)\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                  fillcolor=lightgreen, shape=rectangle];
        
        expert_combine2 [label="Expert Combination EP=8\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, experts=2]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                        fillcolor=lightyellow, shape=rectangle];
        
        moe_residual2 [label="MoE Residual Connection\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                      fillcolor=lightgreen, shape=rectangle];
        
        final_layernorm [label="Final LayerNorm\nGPU: 128-255\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, token_dim=512]", 
                         fillcolor=lightgreen, shape=rectangle];
    }
    
    // Output cluster
    subgraph cluster_output {
        label="Output Layer";
        style=rounded;
        fillcolor=lightgray;
        
        output [label="Output\nInput: [batch_size=32, seq_len=1024, token_dim=512]\nOutput: [batch_size=32, seq_len=1024, vocab_size=V]", 
                shape=box, fillcolor=white, style="rounded,filled"];
    }
    
    // Edges connecting the nodes
    input -> layernorm0;
    layernorm0 -> qkv_proj;
    qkv_proj -> qkv_allreduce;
    qkv_allreduce -> attention;
    attention -> attn_out;
    attn_out -> attn_allreduce;
    attn_allreduce -> residual1;
    residual1 -> moe_gate;
    moe_gate -> expert_route;
    expert_route -> expert_0 [style=dashed];
    expert_route -> expert_1 [style=dashed];
    expert_0 -> expert_combine;
    expert_1 -> expert_combine;
    expert_combine -> moe_residual;
    moe_residual -> stage1_to_stage2;
    stage1_to_stage2 -> layernorm8;
    layernorm8 -> qkv_proj2;
    qkv_proj2 -> qkv_allreduce2;
    qkv_allreduce2 -> attention2;
    attention2 -> attn_out2;
    attn_out2 -> attn_allreduce2;
    attn_allreduce2 -> residual2;
    residual2 -> moe_gate2;  // FIXED: Connect to MoE block
    moe_gate2 -> expert_route2;
    expert_route2 -> expert_2 [style=dashed];
    expert_route2 -> expert_3 [style=dashed];
    expert_2 -> expert_combine2;
    expert_3 -> expert_combine2;
    expert_combine2 -> moe_residual2;
    moe_residual2 -> final_layernorm;
    final_layernorm -> output;
}