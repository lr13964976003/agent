// Detailed Parallel Strategy Deployment DAG
digraph {
	rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input Embedding
GPU: 0-3
Input: [batch=128, seq=10240, hidden=512]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightcoral shape=rectangle]
	layer_0_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	input -> layer_0_qkv_gpu0
	layer_0_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	input -> layer_0_qkv_gpu1
	layer_0_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	input -> layer_0_qkv_gpu2
	layer_0_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	input -> layer_0_qkv_gpu3
	layer_0_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_0_qkv_gpu0 -> layer_0_attn_gpu0
	layer_0_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_0_qkv_gpu1 -> layer_0_attn_gpu1
	layer_0_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_0_qkv_gpu2 -> layer_0_attn_gpu2
	layer_0_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_0_qkv_gpu3 -> layer_0_attn_gpu3
	layer_0_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_attn_gpu0 -> layer_0_attn_out_gpu0
	layer_0_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_attn_gpu1 -> layer_0_attn_out_gpu1
	layer_0_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_attn_gpu2 -> layer_0_attn_out_gpu2
	layer_0_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_attn_gpu3 -> layer_0_attn_out_gpu3
	layer_0_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_0_attn_out_gpu0 -> layer_0_attn_allreduce
	layer_0_attn_out_gpu1 -> layer_0_attn_allreduce
	layer_0_attn_out_gpu2 -> layer_0_attn_allreduce
	layer_0_attn_out_gpu3 -> layer_0_attn_allreduce
	layer_0_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_0_attn_allreduce -> layer_0_gate [style=dashed]
	layer_0_attn_allreduce -> layer_0_gate [style=dashed]
	layer_0_attn_allreduce -> layer_0_gate [style=dashed]
	layer_0_attn_allreduce -> layer_0_gate [style=dashed]
	layer_0_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_0 [style=dashed]
	layer_0_all2all_expert_0 -> layer_0_expert_0_gpu0
	layer_0_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_1 [style=dashed]
	layer_0_all2all_expert_1 -> layer_0_expert_1_gpu0
	layer_0_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_2 [style=dashed]
	layer_0_all2all_expert_2 -> layer_0_expert_2_gpu1
	layer_0_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_3 [style=dashed]
	layer_0_all2all_expert_3 -> layer_0_expert_3_gpu1
	layer_0_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_4 [style=dashed]
	layer_0_all2all_expert_4 -> layer_0_expert_4_gpu2
	layer_0_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_5 [style=dashed]
	layer_0_all2all_expert_5 -> layer_0_expert_5_gpu2
	layer_0_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_6 [style=dashed]
	layer_0_all2all_expert_6 -> layer_0_expert_6_gpu3
	layer_0_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_7 [style=dashed]
	layer_0_all2all_expert_7 -> layer_0_expert_7_gpu3
	layer_0_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_8 [style=dashed]
	layer_0_all2all_expert_8 -> layer_0_expert_8_gpu4
	layer_0_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_9 [style=dashed]
	layer_0_all2all_expert_9 -> layer_0_expert_9_gpu4
	layer_0_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_10 [style=dashed]
	layer_0_all2all_expert_10 -> layer_0_expert_10_gpu5
	layer_0_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_11 [style=dashed]
	layer_0_all2all_expert_11 -> layer_0_expert_11_gpu5
	layer_0_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_12 [style=dashed]
	layer_0_all2all_expert_12 -> layer_0_expert_12_gpu6
	layer_0_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_13 [style=dashed]
	layer_0_all2all_expert_13 -> layer_0_expert_13_gpu6
	layer_0_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_14 [style=dashed]
	layer_0_all2all_expert_14 -> layer_0_expert_14_gpu7
	layer_0_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_gate -> layer_0_all2all_expert_15 [style=dashed]
	layer_0_all2all_expert_15 -> layer_0_expert_15_gpu7
	layer_0_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_0_gpu0 -> layer_0_all2all_back_0
	layer_0_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_1_gpu0 -> layer_0_all2all_back_1
	layer_0_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_2_gpu1 -> layer_0_all2all_back_2
	layer_0_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_3_gpu1 -> layer_0_all2all_back_3
	layer_0_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_4_gpu2 -> layer_0_all2all_back_4
	layer_0_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_5_gpu2 -> layer_0_all2all_back_5
	layer_0_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_6_gpu3 -> layer_0_all2all_back_6
	layer_0_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_7_gpu3 -> layer_0_all2all_back_7
	layer_0_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_8_gpu4 -> layer_0_all2all_back_8
	layer_0_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_9_gpu4 -> layer_0_all2all_back_9
	layer_0_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_10_gpu5 -> layer_0_all2all_back_10
	layer_0_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_11_gpu5 -> layer_0_all2all_back_11
	layer_0_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_12_gpu6 -> layer_0_all2all_back_12
	layer_0_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_13_gpu6 -> layer_0_all2all_back_13
	layer_0_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_14_gpu7 -> layer_0_all2all_back_14
	layer_0_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_0_expert_15_gpu7 -> layer_0_all2all_back_15
	layer_0_output [label="Layer 0 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_0_all2all_back_0 -> layer_0_output
	layer_0_all2all_back_1 -> layer_0_output
	layer_0_all2all_back_2 -> layer_0_output
	layer_0_all2all_back_3 -> layer_0_output
	layer_0_all2all_back_4 -> layer_0_output
	layer_0_all2all_back_5 -> layer_0_output
	layer_0_all2all_back_6 -> layer_0_output
	layer_0_all2all_back_7 -> layer_0_output
	layer_0_all2all_back_8 -> layer_0_output
	layer_0_all2all_back_9 -> layer_0_output
	layer_0_all2all_back_10 -> layer_0_output
	layer_0_all2all_back_11 -> layer_0_output
	layer_0_all2all_back_12 -> layer_0_output
	layer_0_all2all_back_13 -> layer_0_output
	layer_0_all2all_back_14 -> layer_0_output
	layer_0_all2all_back_15 -> layer_0_output
	layer_1_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_1_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_0_output -> layer_1_tp_gather
	layer_1_tp_gather -> layer_1_qkv_gpu0
	layer_1_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_1_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_0_output -> layer_1_tp_gather
	layer_1_tp_gather -> layer_1_qkv_gpu1
	layer_1_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_1_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_0_output -> layer_1_tp_gather
	layer_1_tp_gather -> layer_1_qkv_gpu2
	layer_1_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_1_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_0_output -> layer_1_tp_gather
	layer_1_tp_gather -> layer_1_qkv_gpu3
	layer_1_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_1_qkv_gpu0 -> layer_1_attn_gpu0
	layer_1_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_1_qkv_gpu1 -> layer_1_attn_gpu1
	layer_1_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_1_qkv_gpu2 -> layer_1_attn_gpu2
	layer_1_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_1_qkv_gpu3 -> layer_1_attn_gpu3
	layer_1_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_attn_gpu0 -> layer_1_attn_out_gpu0
	layer_1_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_attn_gpu1 -> layer_1_attn_out_gpu1
	layer_1_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_attn_gpu2 -> layer_1_attn_out_gpu2
	layer_1_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_attn_gpu3 -> layer_1_attn_out_gpu3
	layer_1_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_1_attn_out_gpu0 -> layer_1_attn_allreduce
	layer_1_attn_out_gpu1 -> layer_1_attn_allreduce
	layer_1_attn_out_gpu2 -> layer_1_attn_allreduce
	layer_1_attn_out_gpu3 -> layer_1_attn_allreduce
	layer_1_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_1_attn_allreduce -> layer_1_gate [style=dashed]
	layer_1_attn_allreduce -> layer_1_gate [style=dashed]
	layer_1_attn_allreduce -> layer_1_gate [style=dashed]
	layer_1_attn_allreduce -> layer_1_gate [style=dashed]
	layer_1_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_0 [style=dashed]
	layer_1_all2all_expert_0 -> layer_1_expert_0_gpu0
	layer_1_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_1 [style=dashed]
	layer_1_all2all_expert_1 -> layer_1_expert_1_gpu0
	layer_1_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_2 [style=dashed]
	layer_1_all2all_expert_2 -> layer_1_expert_2_gpu1
	layer_1_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_3 [style=dashed]
	layer_1_all2all_expert_3 -> layer_1_expert_3_gpu1
	layer_1_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_4 [style=dashed]
	layer_1_all2all_expert_4 -> layer_1_expert_4_gpu2
	layer_1_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_5 [style=dashed]
	layer_1_all2all_expert_5 -> layer_1_expert_5_gpu2
	layer_1_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_6 [style=dashed]
	layer_1_all2all_expert_6 -> layer_1_expert_6_gpu3
	layer_1_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_7 [style=dashed]
	layer_1_all2all_expert_7 -> layer_1_expert_7_gpu3
	layer_1_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_8 [style=dashed]
	layer_1_all2all_expert_8 -> layer_1_expert_8_gpu4
	layer_1_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_9 [style=dashed]
	layer_1_all2all_expert_9 -> layer_1_expert_9_gpu4
	layer_1_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_10 [style=dashed]
	layer_1_all2all_expert_10 -> layer_1_expert_10_gpu5
	layer_1_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_11 [style=dashed]
	layer_1_all2all_expert_11 -> layer_1_expert_11_gpu5
	layer_1_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_12 [style=dashed]
	layer_1_all2all_expert_12 -> layer_1_expert_12_gpu6
	layer_1_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_13 [style=dashed]
	layer_1_all2all_expert_13 -> layer_1_expert_13_gpu6
	layer_1_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_14 [style=dashed]
	layer_1_all2all_expert_14 -> layer_1_expert_14_gpu7
	layer_1_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_gate -> layer_1_all2all_expert_15 [style=dashed]
	layer_1_all2all_expert_15 -> layer_1_expert_15_gpu7
	layer_1_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_0_gpu0 -> layer_1_all2all_back_0
	layer_1_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_1_gpu0 -> layer_1_all2all_back_1
	layer_1_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_2_gpu1 -> layer_1_all2all_back_2
	layer_1_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_3_gpu1 -> layer_1_all2all_back_3
	layer_1_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_4_gpu2 -> layer_1_all2all_back_4
	layer_1_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_5_gpu2 -> layer_1_all2all_back_5
	layer_1_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_6_gpu3 -> layer_1_all2all_back_6
	layer_1_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_7_gpu3 -> layer_1_all2all_back_7
	layer_1_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_8_gpu4 -> layer_1_all2all_back_8
	layer_1_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_9_gpu4 -> layer_1_all2all_back_9
	layer_1_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_10_gpu5 -> layer_1_all2all_back_10
	layer_1_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_11_gpu5 -> layer_1_all2all_back_11
	layer_1_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_12_gpu6 -> layer_1_all2all_back_12
	layer_1_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_13_gpu6 -> layer_1_all2all_back_13
	layer_1_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_14_gpu7 -> layer_1_all2all_back_14
	layer_1_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_1_expert_15_gpu7 -> layer_1_all2all_back_15
	layer_1_output [label="Layer 1 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_1_all2all_back_0 -> layer_1_output
	layer_1_all2all_back_1 -> layer_1_output
	layer_1_all2all_back_2 -> layer_1_output
	layer_1_all2all_back_3 -> layer_1_output
	layer_1_all2all_back_4 -> layer_1_output
	layer_1_all2all_back_5 -> layer_1_output
	layer_1_all2all_back_6 -> layer_1_output
	layer_1_all2all_back_7 -> layer_1_output
	layer_1_all2all_back_8 -> layer_1_output
	layer_1_all2all_back_9 -> layer_1_output
	layer_1_all2all_back_10 -> layer_1_output
	layer_1_all2all_back_11 -> layer_1_output
	layer_1_all2all_back_12 -> layer_1_output
	layer_1_all2all_back_13 -> layer_1_output
	layer_1_all2all_back_14 -> layer_1_output
	layer_1_all2all_back_15 -> layer_1_output
	layer_2_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_2_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_1_output -> layer_2_tp_gather
	layer_2_tp_gather -> layer_2_qkv_gpu0
	layer_2_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_2_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_1_output -> layer_2_tp_gather
	layer_2_tp_gather -> layer_2_qkv_gpu1
	layer_2_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_2_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_1_output -> layer_2_tp_gather
	layer_2_tp_gather -> layer_2_qkv_gpu2
	layer_2_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_2_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_1_output -> layer_2_tp_gather
	layer_2_tp_gather -> layer_2_qkv_gpu3
	layer_2_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_2_qkv_gpu0 -> layer_2_attn_gpu0
	layer_2_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_2_qkv_gpu1 -> layer_2_attn_gpu1
	layer_2_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_2_qkv_gpu2 -> layer_2_attn_gpu2
	layer_2_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_2_qkv_gpu3 -> layer_2_attn_gpu3
	layer_2_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_attn_gpu0 -> layer_2_attn_out_gpu0
	layer_2_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_attn_gpu1 -> layer_2_attn_out_gpu1
	layer_2_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_attn_gpu2 -> layer_2_attn_out_gpu2
	layer_2_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_attn_gpu3 -> layer_2_attn_out_gpu3
	layer_2_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_2_attn_out_gpu0 -> layer_2_attn_allreduce
	layer_2_attn_out_gpu1 -> layer_2_attn_allreduce
	layer_2_attn_out_gpu2 -> layer_2_attn_allreduce
	layer_2_attn_out_gpu3 -> layer_2_attn_allreduce
	layer_2_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_2_attn_allreduce -> layer_2_gate [style=dashed]
	layer_2_attn_allreduce -> layer_2_gate [style=dashed]
	layer_2_attn_allreduce -> layer_2_gate [style=dashed]
	layer_2_attn_allreduce -> layer_2_gate [style=dashed]
	layer_2_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_0 [style=dashed]
	layer_2_all2all_expert_0 -> layer_2_expert_0_gpu0
	layer_2_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_1 [style=dashed]
	layer_2_all2all_expert_1 -> layer_2_expert_1_gpu0
	layer_2_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_2 [style=dashed]
	layer_2_all2all_expert_2 -> layer_2_expert_2_gpu1
	layer_2_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_3 [style=dashed]
	layer_2_all2all_expert_3 -> layer_2_expert_3_gpu1
	layer_2_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_4 [style=dashed]
	layer_2_all2all_expert_4 -> layer_2_expert_4_gpu2
	layer_2_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_5 [style=dashed]
	layer_2_all2all_expert_5 -> layer_2_expert_5_gpu2
	layer_2_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_6 [style=dashed]
	layer_2_all2all_expert_6 -> layer_2_expert_6_gpu3
	layer_2_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_7 [style=dashed]
	layer_2_all2all_expert_7 -> layer_2_expert_7_gpu3
	layer_2_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_8 [style=dashed]
	layer_2_all2all_expert_8 -> layer_2_expert_8_gpu4
	layer_2_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_9 [style=dashed]
	layer_2_all2all_expert_9 -> layer_2_expert_9_gpu4
	layer_2_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_10 [style=dashed]
	layer_2_all2all_expert_10 -> layer_2_expert_10_gpu5
	layer_2_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_11 [style=dashed]
	layer_2_all2all_expert_11 -> layer_2_expert_11_gpu5
	layer_2_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_12 [style=dashed]
	layer_2_all2all_expert_12 -> layer_2_expert_12_gpu6
	layer_2_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_13 [style=dashed]
	layer_2_all2all_expert_13 -> layer_2_expert_13_gpu6
	layer_2_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_14 [style=dashed]
	layer_2_all2all_expert_14 -> layer_2_expert_14_gpu7
	layer_2_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_gate -> layer_2_all2all_expert_15 [style=dashed]
	layer_2_all2all_expert_15 -> layer_2_expert_15_gpu7
	layer_2_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_0_gpu0 -> layer_2_all2all_back_0
	layer_2_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_1_gpu0 -> layer_2_all2all_back_1
	layer_2_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_2_gpu1 -> layer_2_all2all_back_2
	layer_2_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_3_gpu1 -> layer_2_all2all_back_3
	layer_2_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_4_gpu2 -> layer_2_all2all_back_4
	layer_2_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_5_gpu2 -> layer_2_all2all_back_5
	layer_2_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_6_gpu3 -> layer_2_all2all_back_6
	layer_2_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_7_gpu3 -> layer_2_all2all_back_7
	layer_2_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_8_gpu4 -> layer_2_all2all_back_8
	layer_2_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_9_gpu4 -> layer_2_all2all_back_9
	layer_2_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_10_gpu5 -> layer_2_all2all_back_10
	layer_2_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_11_gpu5 -> layer_2_all2all_back_11
	layer_2_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_12_gpu6 -> layer_2_all2all_back_12
	layer_2_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_13_gpu6 -> layer_2_all2all_back_13
	layer_2_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_14_gpu7 -> layer_2_all2all_back_14
	layer_2_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_2_expert_15_gpu7 -> layer_2_all2all_back_15
	layer_2_output [label="Layer 2 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_2_all2all_back_0 -> layer_2_output
	layer_2_all2all_back_1 -> layer_2_output
	layer_2_all2all_back_2 -> layer_2_output
	layer_2_all2all_back_3 -> layer_2_output
	layer_2_all2all_back_4 -> layer_2_output
	layer_2_all2all_back_5 -> layer_2_output
	layer_2_all2all_back_6 -> layer_2_output
	layer_2_all2all_back_7 -> layer_2_output
	layer_2_all2all_back_8 -> layer_2_output
	layer_2_all2all_back_9 -> layer_2_output
	layer_2_all2all_back_10 -> layer_2_output
	layer_2_all2all_back_11 -> layer_2_output
	layer_2_all2all_back_12 -> layer_2_output
	layer_2_all2all_back_13 -> layer_2_output
	layer_2_all2all_back_14 -> layer_2_output
	layer_2_all2all_back_15 -> layer_2_output
	layer_3_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_3_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_2_output -> layer_3_tp_gather
	layer_3_tp_gather -> layer_3_qkv_gpu0
	layer_3_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_3_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_2_output -> layer_3_tp_gather
	layer_3_tp_gather -> layer_3_qkv_gpu1
	layer_3_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_3_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_2_output -> layer_3_tp_gather
	layer_3_tp_gather -> layer_3_qkv_gpu2
	layer_3_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_3_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_2_output -> layer_3_tp_gather
	layer_3_tp_gather -> layer_3_qkv_gpu3
	layer_3_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_3_qkv_gpu0 -> layer_3_attn_gpu0
	layer_3_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_3_qkv_gpu1 -> layer_3_attn_gpu1
	layer_3_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_3_qkv_gpu2 -> layer_3_attn_gpu2
	layer_3_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_3_qkv_gpu3 -> layer_3_attn_gpu3
	layer_3_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_attn_gpu0 -> layer_3_attn_out_gpu0
	layer_3_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_attn_gpu1 -> layer_3_attn_out_gpu1
	layer_3_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_attn_gpu2 -> layer_3_attn_out_gpu2
	layer_3_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_attn_gpu3 -> layer_3_attn_out_gpu3
	layer_3_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_3_attn_out_gpu0 -> layer_3_attn_allreduce
	layer_3_attn_out_gpu1 -> layer_3_attn_allreduce
	layer_3_attn_out_gpu2 -> layer_3_attn_allreduce
	layer_3_attn_out_gpu3 -> layer_3_attn_allreduce
	layer_3_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_3_attn_allreduce -> layer_3_gate [style=dashed]
	layer_3_attn_allreduce -> layer_3_gate [style=dashed]
	layer_3_attn_allreduce -> layer_3_gate [style=dashed]
	layer_3_attn_allreduce -> layer_3_gate [style=dashed]
	layer_3_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_0 [style=dashed]
	layer_3_all2all_expert_0 -> layer_3_expert_0_gpu0
	layer_3_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_1 [style=dashed]
	layer_3_all2all_expert_1 -> layer_3_expert_1_gpu0
	layer_3_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_2 [style=dashed]
	layer_3_all2all_expert_2 -> layer_3_expert_2_gpu1
	layer_3_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_3 [style=dashed]
	layer_3_all2all_expert_3 -> layer_3_expert_3_gpu1
	layer_3_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_4 [style=dashed]
	layer_3_all2all_expert_4 -> layer_3_expert_4_gpu2
	layer_3_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_5 [style=dashed]
	layer_3_all2all_expert_5 -> layer_3_expert_5_gpu2
	layer_3_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_6 [style=dashed]
	layer_3_all2all_expert_6 -> layer_3_expert_6_gpu3
	layer_3_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_7 [style=dashed]
	layer_3_all2all_expert_7 -> layer_3_expert_7_gpu3
	layer_3_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_8 [style=dashed]
	layer_3_all2all_expert_8 -> layer_3_expert_8_gpu4
	layer_3_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_9 [style=dashed]
	layer_3_all2all_expert_9 -> layer_3_expert_9_gpu4
	layer_3_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_10 [style=dashed]
	layer_3_all2all_expert_10 -> layer_3_expert_10_gpu5
	layer_3_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_11 [style=dashed]
	layer_3_all2all_expert_11 -> layer_3_expert_11_gpu5
	layer_3_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_12 [style=dashed]
	layer_3_all2all_expert_12 -> layer_3_expert_12_gpu6
	layer_3_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_13 [style=dashed]
	layer_3_all2all_expert_13 -> layer_3_expert_13_gpu6
	layer_3_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_14 [style=dashed]
	layer_3_all2all_expert_14 -> layer_3_expert_14_gpu7
	layer_3_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_gate -> layer_3_all2all_expert_15 [style=dashed]
	layer_3_all2all_expert_15 -> layer_3_expert_15_gpu7
	layer_3_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_0_gpu0 -> layer_3_all2all_back_0
	layer_3_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_1_gpu0 -> layer_3_all2all_back_1
	layer_3_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_2_gpu1 -> layer_3_all2all_back_2
	layer_3_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_3_gpu1 -> layer_3_all2all_back_3
	layer_3_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_4_gpu2 -> layer_3_all2all_back_4
	layer_3_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_5_gpu2 -> layer_3_all2all_back_5
	layer_3_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_6_gpu3 -> layer_3_all2all_back_6
	layer_3_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_7_gpu3 -> layer_3_all2all_back_7
	layer_3_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_8_gpu4 -> layer_3_all2all_back_8
	layer_3_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_9_gpu4 -> layer_3_all2all_back_9
	layer_3_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_10_gpu5 -> layer_3_all2all_back_10
	layer_3_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_11_gpu5 -> layer_3_all2all_back_11
	layer_3_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_12_gpu6 -> layer_3_all2all_back_12
	layer_3_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_13_gpu6 -> layer_3_all2all_back_13
	layer_3_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_14_gpu7 -> layer_3_all2all_back_14
	layer_3_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_3_expert_15_gpu7 -> layer_3_all2all_back_15
	layer_3_output [label="Layer 3 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_3_all2all_back_0 -> layer_3_output
	layer_3_all2all_back_1 -> layer_3_output
	layer_3_all2all_back_2 -> layer_3_output
	layer_3_all2all_back_3 -> layer_3_output
	layer_3_all2all_back_4 -> layer_3_output
	layer_3_all2all_back_5 -> layer_3_output
	layer_3_all2all_back_6 -> layer_3_output
	layer_3_all2all_back_7 -> layer_3_output
	layer_3_all2all_back_8 -> layer_3_output
	layer_3_all2all_back_9 -> layer_3_output
	layer_3_all2all_back_10 -> layer_3_output
	layer_3_all2all_back_11 -> layer_3_output
	layer_3_all2all_back_12 -> layer_3_output
	layer_3_all2all_back_13 -> layer_3_output
	layer_3_all2all_back_14 -> layer_3_output
	layer_3_all2all_back_15 -> layer_3_output
	layer_4_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_4_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_3_output -> layer_4_tp_gather
	layer_4_tp_gather -> layer_4_qkv_gpu0
	layer_4_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_4_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_3_output -> layer_4_tp_gather
	layer_4_tp_gather -> layer_4_qkv_gpu1
	layer_4_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_4_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_3_output -> layer_4_tp_gather
	layer_4_tp_gather -> layer_4_qkv_gpu2
	layer_4_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_4_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_3_output -> layer_4_tp_gather
	layer_4_tp_gather -> layer_4_qkv_gpu3
	layer_4_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_4_qkv_gpu0 -> layer_4_attn_gpu0
	layer_4_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_4_qkv_gpu1 -> layer_4_attn_gpu1
	layer_4_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_4_qkv_gpu2 -> layer_4_attn_gpu2
	layer_4_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_4_qkv_gpu3 -> layer_4_attn_gpu3
	layer_4_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_attn_gpu0 -> layer_4_attn_out_gpu0
	layer_4_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_attn_gpu1 -> layer_4_attn_out_gpu1
	layer_4_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_attn_gpu2 -> layer_4_attn_out_gpu2
	layer_4_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_attn_gpu3 -> layer_4_attn_out_gpu3
	layer_4_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_4_attn_out_gpu0 -> layer_4_attn_allreduce
	layer_4_attn_out_gpu1 -> layer_4_attn_allreduce
	layer_4_attn_out_gpu2 -> layer_4_attn_allreduce
	layer_4_attn_out_gpu3 -> layer_4_attn_allreduce
	layer_4_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_4_attn_allreduce -> layer_4_gate [style=dashed]
	layer_4_attn_allreduce -> layer_4_gate [style=dashed]
	layer_4_attn_allreduce -> layer_4_gate [style=dashed]
	layer_4_attn_allreduce -> layer_4_gate [style=dashed]
	layer_4_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_0 [style=dashed]
	layer_4_all2all_expert_0 -> layer_4_expert_0_gpu0
	layer_4_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_1 [style=dashed]
	layer_4_all2all_expert_1 -> layer_4_expert_1_gpu0
	layer_4_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_2 [style=dashed]
	layer_4_all2all_expert_2 -> layer_4_expert_2_gpu1
	layer_4_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_3 [style=dashed]
	layer_4_all2all_expert_3 -> layer_4_expert_3_gpu1
	layer_4_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_4 [style=dashed]
	layer_4_all2all_expert_4 -> layer_4_expert_4_gpu2
	layer_4_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_5 [style=dashed]
	layer_4_all2all_expert_5 -> layer_4_expert_5_gpu2
	layer_4_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_6 [style=dashed]
	layer_4_all2all_expert_6 -> layer_4_expert_6_gpu3
	layer_4_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_7 [style=dashed]
	layer_4_all2all_expert_7 -> layer_4_expert_7_gpu3
	layer_4_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_8 [style=dashed]
	layer_4_all2all_expert_8 -> layer_4_expert_8_gpu4
	layer_4_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_9 [style=dashed]
	layer_4_all2all_expert_9 -> layer_4_expert_9_gpu4
	layer_4_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_10 [style=dashed]
	layer_4_all2all_expert_10 -> layer_4_expert_10_gpu5
	layer_4_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_11 [style=dashed]
	layer_4_all2all_expert_11 -> layer_4_expert_11_gpu5
	layer_4_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_12 [style=dashed]
	layer_4_all2all_expert_12 -> layer_4_expert_12_gpu6
	layer_4_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_13 [style=dashed]
	layer_4_all2all_expert_13 -> layer_4_expert_13_gpu6
	layer_4_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_14 [style=dashed]
	layer_4_all2all_expert_14 -> layer_4_expert_14_gpu7
	layer_4_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_gate -> layer_4_all2all_expert_15 [style=dashed]
	layer_4_all2all_expert_15 -> layer_4_expert_15_gpu7
	layer_4_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_0_gpu0 -> layer_4_all2all_back_0
	layer_4_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_1_gpu0 -> layer_4_all2all_back_1
	layer_4_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_2_gpu1 -> layer_4_all2all_back_2
	layer_4_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_3_gpu1 -> layer_4_all2all_back_3
	layer_4_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_4_gpu2 -> layer_4_all2all_back_4
	layer_4_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_5_gpu2 -> layer_4_all2all_back_5
	layer_4_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_6_gpu3 -> layer_4_all2all_back_6
	layer_4_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_7_gpu3 -> layer_4_all2all_back_7
	layer_4_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_8_gpu4 -> layer_4_all2all_back_8
	layer_4_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_9_gpu4 -> layer_4_all2all_back_9
	layer_4_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_10_gpu5 -> layer_4_all2all_back_10
	layer_4_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_11_gpu5 -> layer_4_all2all_back_11
	layer_4_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_12_gpu6 -> layer_4_all2all_back_12
	layer_4_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_13_gpu6 -> layer_4_all2all_back_13
	layer_4_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_14_gpu7 -> layer_4_all2all_back_14
	layer_4_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_4_expert_15_gpu7 -> layer_4_all2all_back_15
	layer_4_output [label="Layer 4 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_4_all2all_back_0 -> layer_4_output
	layer_4_all2all_back_1 -> layer_4_output
	layer_4_all2all_back_2 -> layer_4_output
	layer_4_all2all_back_3 -> layer_4_output
	layer_4_all2all_back_4 -> layer_4_output
	layer_4_all2all_back_5 -> layer_4_output
	layer_4_all2all_back_6 -> layer_4_output
	layer_4_all2all_back_7 -> layer_4_output
	layer_4_all2all_back_8 -> layer_4_output
	layer_4_all2all_back_9 -> layer_4_output
	layer_4_all2all_back_10 -> layer_4_output
	layer_4_all2all_back_11 -> layer_4_output
	layer_4_all2all_back_12 -> layer_4_output
	layer_4_all2all_back_13 -> layer_4_output
	layer_4_all2all_back_14 -> layer_4_output
	layer_4_all2all_back_15 -> layer_4_output
	layer_5_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_5_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_4_output -> layer_5_tp_gather
	layer_5_tp_gather -> layer_5_qkv_gpu0
	layer_5_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_5_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_4_output -> layer_5_tp_gather
	layer_5_tp_gather -> layer_5_qkv_gpu1
	layer_5_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_5_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_4_output -> layer_5_tp_gather
	layer_5_tp_gather -> layer_5_qkv_gpu2
	layer_5_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_5_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_4_output -> layer_5_tp_gather
	layer_5_tp_gather -> layer_5_qkv_gpu3
	layer_5_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_5_qkv_gpu0 -> layer_5_attn_gpu0
	layer_5_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_5_qkv_gpu1 -> layer_5_attn_gpu1
	layer_5_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_5_qkv_gpu2 -> layer_5_attn_gpu2
	layer_5_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_5_qkv_gpu3 -> layer_5_attn_gpu3
	layer_5_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_attn_gpu0 -> layer_5_attn_out_gpu0
	layer_5_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_attn_gpu1 -> layer_5_attn_out_gpu1
	layer_5_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_attn_gpu2 -> layer_5_attn_out_gpu2
	layer_5_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_attn_gpu3 -> layer_5_attn_out_gpu3
	layer_5_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_5_attn_out_gpu0 -> layer_5_attn_allreduce
	layer_5_attn_out_gpu1 -> layer_5_attn_allreduce
	layer_5_attn_out_gpu2 -> layer_5_attn_allreduce
	layer_5_attn_out_gpu3 -> layer_5_attn_allreduce
	layer_5_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_5_attn_allreduce -> layer_5_gate [style=dashed]
	layer_5_attn_allreduce -> layer_5_gate [style=dashed]
	layer_5_attn_allreduce -> layer_5_gate [style=dashed]
	layer_5_attn_allreduce -> layer_5_gate [style=dashed]
	layer_5_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_0 [style=dashed]
	layer_5_all2all_expert_0 -> layer_5_expert_0_gpu0
	layer_5_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_1 [style=dashed]
	layer_5_all2all_expert_1 -> layer_5_expert_1_gpu0
	layer_5_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_2 [style=dashed]
	layer_5_all2all_expert_2 -> layer_5_expert_2_gpu1
	layer_5_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_3 [style=dashed]
	layer_5_all2all_expert_3 -> layer_5_expert_3_gpu1
	layer_5_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_4 [style=dashed]
	layer_5_all2all_expert_4 -> layer_5_expert_4_gpu2
	layer_5_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_5 [style=dashed]
	layer_5_all2all_expert_5 -> layer_5_expert_5_gpu2
	layer_5_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_6 [style=dashed]
	layer_5_all2all_expert_6 -> layer_5_expert_6_gpu3
	layer_5_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_7 [style=dashed]
	layer_5_all2all_expert_7 -> layer_5_expert_7_gpu3
	layer_5_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_8 [style=dashed]
	layer_5_all2all_expert_8 -> layer_5_expert_8_gpu4
	layer_5_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_9 [style=dashed]
	layer_5_all2all_expert_9 -> layer_5_expert_9_gpu4
	layer_5_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_10 [style=dashed]
	layer_5_all2all_expert_10 -> layer_5_expert_10_gpu5
	layer_5_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_11 [style=dashed]
	layer_5_all2all_expert_11 -> layer_5_expert_11_gpu5
	layer_5_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_12 [style=dashed]
	layer_5_all2all_expert_12 -> layer_5_expert_12_gpu6
	layer_5_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_13 [style=dashed]
	layer_5_all2all_expert_13 -> layer_5_expert_13_gpu6
	layer_5_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_14 [style=dashed]
	layer_5_all2all_expert_14 -> layer_5_expert_14_gpu7
	layer_5_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_gate -> layer_5_all2all_expert_15 [style=dashed]
	layer_5_all2all_expert_15 -> layer_5_expert_15_gpu7
	layer_5_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_0_gpu0 -> layer_5_all2all_back_0
	layer_5_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_1_gpu0 -> layer_5_all2all_back_1
	layer_5_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_2_gpu1 -> layer_5_all2all_back_2
	layer_5_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_3_gpu1 -> layer_5_all2all_back_3
	layer_5_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_4_gpu2 -> layer_5_all2all_back_4
	layer_5_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_5_gpu2 -> layer_5_all2all_back_5
	layer_5_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_6_gpu3 -> layer_5_all2all_back_6
	layer_5_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_7_gpu3 -> layer_5_all2all_back_7
	layer_5_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_8_gpu4 -> layer_5_all2all_back_8
	layer_5_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_9_gpu4 -> layer_5_all2all_back_9
	layer_5_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_10_gpu5 -> layer_5_all2all_back_10
	layer_5_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_11_gpu5 -> layer_5_all2all_back_11
	layer_5_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_12_gpu6 -> layer_5_all2all_back_12
	layer_5_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_13_gpu6 -> layer_5_all2all_back_13
	layer_5_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_14_gpu7 -> layer_5_all2all_back_14
	layer_5_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_5_expert_15_gpu7 -> layer_5_all2all_back_15
	layer_5_output [label="Layer 5 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_5_all2all_back_0 -> layer_5_output
	layer_5_all2all_back_1 -> layer_5_output
	layer_5_all2all_back_2 -> layer_5_output
	layer_5_all2all_back_3 -> layer_5_output
	layer_5_all2all_back_4 -> layer_5_output
	layer_5_all2all_back_5 -> layer_5_output
	layer_5_all2all_back_6 -> layer_5_output
	layer_5_all2all_back_7 -> layer_5_output
	layer_5_all2all_back_8 -> layer_5_output
	layer_5_all2all_back_9 -> layer_5_output
	layer_5_all2all_back_10 -> layer_5_output
	layer_5_all2all_back_11 -> layer_5_output
	layer_5_all2all_back_12 -> layer_5_output
	layer_5_all2all_back_13 -> layer_5_output
	layer_5_all2all_back_14 -> layer_5_output
	layer_5_all2all_back_15 -> layer_5_output
	layer_6_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_6_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_5_output -> layer_6_tp_gather
	layer_6_tp_gather -> layer_6_qkv_gpu0
	layer_6_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_6_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_5_output -> layer_6_tp_gather
	layer_6_tp_gather -> layer_6_qkv_gpu1
	layer_6_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_6_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_5_output -> layer_6_tp_gather
	layer_6_tp_gather -> layer_6_qkv_gpu2
	layer_6_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_6_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_5_output -> layer_6_tp_gather
	layer_6_tp_gather -> layer_6_qkv_gpu3
	layer_6_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_6_qkv_gpu0 -> layer_6_attn_gpu0
	layer_6_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_6_qkv_gpu1 -> layer_6_attn_gpu1
	layer_6_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_6_qkv_gpu2 -> layer_6_attn_gpu2
	layer_6_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_6_qkv_gpu3 -> layer_6_attn_gpu3
	layer_6_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_attn_gpu0 -> layer_6_attn_out_gpu0
	layer_6_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_attn_gpu1 -> layer_6_attn_out_gpu1
	layer_6_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_attn_gpu2 -> layer_6_attn_out_gpu2
	layer_6_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_attn_gpu3 -> layer_6_attn_out_gpu3
	layer_6_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_6_attn_out_gpu0 -> layer_6_attn_allreduce
	layer_6_attn_out_gpu1 -> layer_6_attn_allreduce
	layer_6_attn_out_gpu2 -> layer_6_attn_allreduce
	layer_6_attn_out_gpu3 -> layer_6_attn_allreduce
	layer_6_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_6_attn_allreduce -> layer_6_gate [style=dashed]
	layer_6_attn_allreduce -> layer_6_gate [style=dashed]
	layer_6_attn_allreduce -> layer_6_gate [style=dashed]
	layer_6_attn_allreduce -> layer_6_gate [style=dashed]
	layer_6_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_0 [style=dashed]
	layer_6_all2all_expert_0 -> layer_6_expert_0_gpu0
	layer_6_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_1 [style=dashed]
	layer_6_all2all_expert_1 -> layer_6_expert_1_gpu0
	layer_6_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_2 [style=dashed]
	layer_6_all2all_expert_2 -> layer_6_expert_2_gpu1
	layer_6_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_3 [style=dashed]
	layer_6_all2all_expert_3 -> layer_6_expert_3_gpu1
	layer_6_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_4 [style=dashed]
	layer_6_all2all_expert_4 -> layer_6_expert_4_gpu2
	layer_6_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_5 [style=dashed]
	layer_6_all2all_expert_5 -> layer_6_expert_5_gpu2
	layer_6_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_6 [style=dashed]
	layer_6_all2all_expert_6 -> layer_6_expert_6_gpu3
	layer_6_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_7 [style=dashed]
	layer_6_all2all_expert_7 -> layer_6_expert_7_gpu3
	layer_6_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_8 [style=dashed]
	layer_6_all2all_expert_8 -> layer_6_expert_8_gpu4
	layer_6_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_9 [style=dashed]
	layer_6_all2all_expert_9 -> layer_6_expert_9_gpu4
	layer_6_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_10 [style=dashed]
	layer_6_all2all_expert_10 -> layer_6_expert_10_gpu5
	layer_6_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_11 [style=dashed]
	layer_6_all2all_expert_11 -> layer_6_expert_11_gpu5
	layer_6_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_12 [style=dashed]
	layer_6_all2all_expert_12 -> layer_6_expert_12_gpu6
	layer_6_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_13 [style=dashed]
	layer_6_all2all_expert_13 -> layer_6_expert_13_gpu6
	layer_6_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_14 [style=dashed]
	layer_6_all2all_expert_14 -> layer_6_expert_14_gpu7
	layer_6_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_gate -> layer_6_all2all_expert_15 [style=dashed]
	layer_6_all2all_expert_15 -> layer_6_expert_15_gpu7
	layer_6_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_0_gpu0 -> layer_6_all2all_back_0
	layer_6_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_1_gpu0 -> layer_6_all2all_back_1
	layer_6_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_2_gpu1 -> layer_6_all2all_back_2
	layer_6_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_3_gpu1 -> layer_6_all2all_back_3
	layer_6_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_4_gpu2 -> layer_6_all2all_back_4
	layer_6_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_5_gpu2 -> layer_6_all2all_back_5
	layer_6_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_6_gpu3 -> layer_6_all2all_back_6
	layer_6_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_7_gpu3 -> layer_6_all2all_back_7
	layer_6_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_8_gpu4 -> layer_6_all2all_back_8
	layer_6_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_9_gpu4 -> layer_6_all2all_back_9
	layer_6_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_10_gpu5 -> layer_6_all2all_back_10
	layer_6_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_11_gpu5 -> layer_6_all2all_back_11
	layer_6_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_12_gpu6 -> layer_6_all2all_back_12
	layer_6_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_13_gpu6 -> layer_6_all2all_back_13
	layer_6_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_14_gpu7 -> layer_6_all2all_back_14
	layer_6_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_6_expert_15_gpu7 -> layer_6_all2all_back_15
	layer_6_output [label="Layer 6 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_6_all2all_back_0 -> layer_6_output
	layer_6_all2all_back_1 -> layer_6_output
	layer_6_all2all_back_2 -> layer_6_output
	layer_6_all2all_back_3 -> layer_6_output
	layer_6_all2all_back_4 -> layer_6_output
	layer_6_all2all_back_5 -> layer_6_output
	layer_6_all2all_back_6 -> layer_6_output
	layer_6_all2all_back_7 -> layer_6_output
	layer_6_all2all_back_8 -> layer_6_output
	layer_6_all2all_back_9 -> layer_6_output
	layer_6_all2all_back_10 -> layer_6_output
	layer_6_all2all_back_11 -> layer_6_output
	layer_6_all2all_back_12 -> layer_6_output
	layer_6_all2all_back_13 -> layer_6_output
	layer_6_all2all_back_14 -> layer_6_output
	layer_6_all2all_back_15 -> layer_6_output
	layer_7_qkv_gpu0 [label="Q/K/V Projection
GPU: 0
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_7_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_6_output -> layer_7_tp_gather
	layer_7_tp_gather -> layer_7_qkv_gpu0
	layer_7_qkv_gpu1 [label="Q/K/V Projection
GPU: 1
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_7_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_6_output -> layer_7_tp_gather
	layer_7_tp_gather -> layer_7_qkv_gpu1
	layer_7_qkv_gpu2 [label="Q/K/V Projection
GPU: 2
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_7_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_6_output -> layer_7_tp_gather
	layer_7_tp_gather -> layer_7_qkv_gpu2
	layer_7_qkv_gpu3 [label="Q/K/V Projection
GPU: 3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_7_tp_gather [label="TP All-Gather
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_6_output -> layer_7_tp_gather
	layer_7_tp_gather -> layer_7_qkv_gpu3
	layer_7_attn_gpu0 [label="Attention Computation
GPU: 0
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_7_qkv_gpu0 -> layer_7_attn_gpu0
	layer_7_attn_gpu1 [label="Attention Computation
GPU: 1
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_7_qkv_gpu1 -> layer_7_attn_gpu1
	layer_7_attn_gpu2 [label="Attention Computation
GPU: 2
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_7_qkv_gpu2 -> layer_7_attn_gpu2
	layer_7_attn_gpu3 [label="Attention Computation
GPU: 3
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_7_qkv_gpu3 -> layer_7_attn_gpu3
	layer_7_attn_out_gpu0 [label="Attention Output Projection
GPU: 0
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_attn_gpu0 -> layer_7_attn_out_gpu0
	layer_7_attn_out_gpu1 [label="Attention Output Projection
GPU: 1
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_attn_gpu1 -> layer_7_attn_out_gpu1
	layer_7_attn_out_gpu2 [label="Attention Output Projection
GPU: 2
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_attn_gpu2 -> layer_7_attn_out_gpu2
	layer_7_attn_out_gpu3 [label="Attention Output Projection
GPU: 3
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_attn_gpu3 -> layer_7_attn_out_gpu3
	layer_7_attn_allreduce [label="Attention TP All-Reduce
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_7_attn_out_gpu0 -> layer_7_attn_allreduce
	layer_7_attn_out_gpu1 -> layer_7_attn_allreduce
	layer_7_attn_out_gpu2 -> layer_7_attn_allreduce
	layer_7_attn_out_gpu3 -> layer_7_attn_allreduce
	layer_7_gate [label="Expert Gate Routing
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_7_attn_allreduce -> layer_7_gate [style=dashed]
	layer_7_attn_allreduce -> layer_7_gate [style=dashed]
	layer_7_attn_allreduce -> layer_7_gate [style=dashed]
	layer_7_attn_allreduce -> layer_7_gate [style=dashed]
	layer_7_expert_0_gpu0 [label="Expert 0
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_0 [label="All-to-All Expert 0
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_0 [style=dashed]
	layer_7_all2all_expert_0 -> layer_7_expert_0_gpu0
	layer_7_expert_1_gpu0 [label="Expert 1
GPU: 0
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_1 [label="All-to-All Expert 1
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_1 [style=dashed]
	layer_7_all2all_expert_1 -> layer_7_expert_1_gpu0
	layer_7_expert_2_gpu1 [label="Expert 2
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_2 [label="All-to-All Expert 2
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_2 [style=dashed]
	layer_7_all2all_expert_2 -> layer_7_expert_2_gpu1
	layer_7_expert_3_gpu1 [label="Expert 3
GPU: 1
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_3 [label="All-to-All Expert 3
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_3 [style=dashed]
	layer_7_all2all_expert_3 -> layer_7_expert_3_gpu1
	layer_7_expert_4_gpu2 [label="Expert 4
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_4 [label="All-to-All Expert 4
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_4 [style=dashed]
	layer_7_all2all_expert_4 -> layer_7_expert_4_gpu2
	layer_7_expert_5_gpu2 [label="Expert 5
GPU: 2
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_5 [label="All-to-All Expert 5
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_5 [style=dashed]
	layer_7_all2all_expert_5 -> layer_7_expert_5_gpu2
	layer_7_expert_6_gpu3 [label="Expert 6
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_6 [label="All-to-All Expert 6
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_6 [style=dashed]
	layer_7_all2all_expert_6 -> layer_7_expert_6_gpu3
	layer_7_expert_7_gpu3 [label="Expert 7
GPU: 3
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_7 [label="All-to-All Expert 7
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_7 [style=dashed]
	layer_7_all2all_expert_7 -> layer_7_expert_7_gpu3
	layer_7_expert_8_gpu4 [label="Expert 8
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_8 [label="All-to-All Expert 8
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_8 [style=dashed]
	layer_7_all2all_expert_8 -> layer_7_expert_8_gpu4
	layer_7_expert_9_gpu4 [label="Expert 9
GPU: 4
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_9 [label="All-to-All Expert 9
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_9 [style=dashed]
	layer_7_all2all_expert_9 -> layer_7_expert_9_gpu4
	layer_7_expert_10_gpu5 [label="Expert 10
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_10 [label="All-to-All Expert 10
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_10 [style=dashed]
	layer_7_all2all_expert_10 -> layer_7_expert_10_gpu5
	layer_7_expert_11_gpu5 [label="Expert 11
GPU: 5
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_11 [label="All-to-All Expert 11
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_11 [style=dashed]
	layer_7_all2all_expert_11 -> layer_7_expert_11_gpu5
	layer_7_expert_12_gpu6 [label="Expert 12
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_12 [label="All-to-All Expert 12
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_12 [style=dashed]
	layer_7_all2all_expert_12 -> layer_7_expert_12_gpu6
	layer_7_expert_13_gpu6 [label="Expert 13
GPU: 6
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_13 [label="All-to-All Expert 13
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_13 [style=dashed]
	layer_7_all2all_expert_13 -> layer_7_expert_13_gpu6
	layer_7_expert_14_gpu7 [label="Expert 14
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_14 [label="All-to-All Expert 14
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_14 [style=dashed]
	layer_7_all2all_expert_14 -> layer_7_expert_14_gpu7
	layer_7_expert_15_gpu7 [label="Expert 15
GPU: 7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_expert_15 [label="All-to-All Expert 15
GPU: 0-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_gate -> layer_7_all2all_expert_15 [style=dashed]
	layer_7_all2all_expert_15 -> layer_7_expert_15_gpu7
	layer_7_all2all_back_0 [label="All-to-All Back 0
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_0_gpu0 -> layer_7_all2all_back_0
	layer_7_all2all_back_1 [label="All-to-All Back 1
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_1_gpu0 -> layer_7_all2all_back_1
	layer_7_all2all_back_2 [label="All-to-All Back 2
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_2_gpu1 -> layer_7_all2all_back_2
	layer_7_all2all_back_3 [label="All-to-All Back 3
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_3_gpu1 -> layer_7_all2all_back_3
	layer_7_all2all_back_4 [label="All-to-All Back 4
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_4_gpu2 -> layer_7_all2all_back_4
	layer_7_all2all_back_5 [label="All-to-All Back 5
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_5_gpu2 -> layer_7_all2all_back_5
	layer_7_all2all_back_6 [label="All-to-All Back 6
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_6_gpu3 -> layer_7_all2all_back_6
	layer_7_all2all_back_7 [label="All-to-All Back 7
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_7_gpu3 -> layer_7_all2all_back_7
	layer_7_all2all_back_8 [label="All-to-All Back 8
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_8_gpu4 -> layer_7_all2all_back_8
	layer_7_all2all_back_9 [label="All-to-All Back 9
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_9_gpu4 -> layer_7_all2all_back_9
	layer_7_all2all_back_10 [label="All-to-All Back 10
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_10_gpu5 -> layer_7_all2all_back_10
	layer_7_all2all_back_11 [label="All-to-All Back 11
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_11_gpu5 -> layer_7_all2all_back_11
	layer_7_all2all_back_12 [label="All-to-All Back 12
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_12_gpu6 -> layer_7_all2all_back_12
	layer_7_all2all_back_13 [label="All-to-All Back 13
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_13_gpu6 -> layer_7_all2all_back_13
	layer_7_all2all_back_14 [label="All-to-All Back 14
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_14_gpu7 -> layer_7_all2all_back_14
	layer_7_all2all_back_15 [label="All-to-All Back 15
GPU: 0-7
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_7_expert_15_gpu7 -> layer_7_all2all_back_15
	layer_7_output [label="Layer 7 Output
GPU: 0-3
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_7_all2all_back_0 -> layer_7_output
	layer_7_all2all_back_1 -> layer_7_output
	layer_7_all2all_back_2 -> layer_7_output
	layer_7_all2all_back_3 -> layer_7_output
	layer_7_all2all_back_4 -> layer_7_output
	layer_7_all2all_back_5 -> layer_7_output
	layer_7_all2all_back_6 -> layer_7_output
	layer_7_all2all_back_7 -> layer_7_output
	layer_7_all2all_back_8 -> layer_7_output
	layer_7_all2all_back_9 -> layer_7_output
	layer_7_all2all_back_10 -> layer_7_output
	layer_7_all2all_back_11 -> layer_7_output
	layer_7_all2all_back_12 -> layer_7_output
	layer_7_all2all_back_13 -> layer_7_output
	layer_7_all2all_back_14 -> layer_7_output
	layer_7_all2all_back_15 -> layer_7_output
	pipeline_stage0_to_stage1 [label="Pipeline Stage 01
GPU: 0-3  4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_7_output -> pipeline_stage0_to_stage1
	layer_8_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	pipeline_stage0_to_stage1 -> layer_8_qkv_gpu4
	layer_8_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	pipeline_stage0_to_stage1 -> layer_8_qkv_gpu5
	layer_8_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	pipeline_stage0_to_stage1 -> layer_8_qkv_gpu6
	layer_8_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	pipeline_stage0_to_stage1 -> layer_8_qkv_gpu7
	layer_8_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_8_qkv_gpu4 -> layer_8_attn_gpu4
	layer_8_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_8_qkv_gpu5 -> layer_8_attn_gpu5
	layer_8_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_8_qkv_gpu6 -> layer_8_attn_gpu6
	layer_8_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_8_qkv_gpu7 -> layer_8_attn_gpu7
	layer_8_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_attn_gpu4 -> layer_8_attn_out_gpu4
	layer_8_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_attn_gpu5 -> layer_8_attn_out_gpu5
	layer_8_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_attn_gpu6 -> layer_8_attn_out_gpu6
	layer_8_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_attn_gpu7 -> layer_8_attn_out_gpu7
	layer_8_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_8_attn_out_gpu4 -> layer_8_attn_allreduce
	layer_8_attn_out_gpu5 -> layer_8_attn_allreduce
	layer_8_attn_out_gpu6 -> layer_8_attn_allreduce
	layer_8_attn_out_gpu7 -> layer_8_attn_allreduce
	layer_8_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_8_attn_allreduce -> layer_8_gate [style=dashed]
	layer_8_attn_allreduce -> layer_8_gate [style=dashed]
	layer_8_attn_allreduce -> layer_8_gate [style=dashed]
	layer_8_attn_allreduce -> layer_8_gate [style=dashed]
	layer_8_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_0 [style=dashed]
	layer_8_all2all_expert_0 -> layer_8_expert_0_gpu8
	layer_8_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_1 [style=dashed]
	layer_8_all2all_expert_1 -> layer_8_expert_1_gpu8
	layer_8_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_2 [style=dashed]
	layer_8_all2all_expert_2 -> layer_8_expert_2_gpu9
	layer_8_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_3 [style=dashed]
	layer_8_all2all_expert_3 -> layer_8_expert_3_gpu9
	layer_8_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_4 [style=dashed]
	layer_8_all2all_expert_4 -> layer_8_expert_4_gpu10
	layer_8_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_5 [style=dashed]
	layer_8_all2all_expert_5 -> layer_8_expert_5_gpu10
	layer_8_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_6 [style=dashed]
	layer_8_all2all_expert_6 -> layer_8_expert_6_gpu11
	layer_8_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_7 [style=dashed]
	layer_8_all2all_expert_7 -> layer_8_expert_7_gpu11
	layer_8_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_8 [style=dashed]
	layer_8_all2all_expert_8 -> layer_8_expert_8_gpu12
	layer_8_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_9 [style=dashed]
	layer_8_all2all_expert_9 -> layer_8_expert_9_gpu12
	layer_8_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_10 [style=dashed]
	layer_8_all2all_expert_10 -> layer_8_expert_10_gpu13
	layer_8_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_11 [style=dashed]
	layer_8_all2all_expert_11 -> layer_8_expert_11_gpu13
	layer_8_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_12 [style=dashed]
	layer_8_all2all_expert_12 -> layer_8_expert_12_gpu14
	layer_8_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_13 [style=dashed]
	layer_8_all2all_expert_13 -> layer_8_expert_13_gpu14
	layer_8_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_14 [style=dashed]
	layer_8_all2all_expert_14 -> layer_8_expert_14_gpu15
	layer_8_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_gate -> layer_8_all2all_expert_15 [style=dashed]
	layer_8_all2all_expert_15 -> layer_8_expert_15_gpu15
	layer_8_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_0_gpu8 -> layer_8_all2all_back_0
	layer_8_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_1_gpu8 -> layer_8_all2all_back_1
	layer_8_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_2_gpu9 -> layer_8_all2all_back_2
	layer_8_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_3_gpu9 -> layer_8_all2all_back_3
	layer_8_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_4_gpu10 -> layer_8_all2all_back_4
	layer_8_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_5_gpu10 -> layer_8_all2all_back_5
	layer_8_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_6_gpu11 -> layer_8_all2all_back_6
	layer_8_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_7_gpu11 -> layer_8_all2all_back_7
	layer_8_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_8_gpu12 -> layer_8_all2all_back_8
	layer_8_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_9_gpu12 -> layer_8_all2all_back_9
	layer_8_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_10_gpu13 -> layer_8_all2all_back_10
	layer_8_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_11_gpu13 -> layer_8_all2all_back_11
	layer_8_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_12_gpu14 -> layer_8_all2all_back_12
	layer_8_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_13_gpu14 -> layer_8_all2all_back_13
	layer_8_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_14_gpu15 -> layer_8_all2all_back_14
	layer_8_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_8_expert_15_gpu15 -> layer_8_all2all_back_15
	layer_8_output [label="Layer 8 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_8_all2all_back_0 -> layer_8_output
	layer_8_all2all_back_1 -> layer_8_output
	layer_8_all2all_back_2 -> layer_8_output
	layer_8_all2all_back_3 -> layer_8_output
	layer_8_all2all_back_4 -> layer_8_output
	layer_8_all2all_back_5 -> layer_8_output
	layer_8_all2all_back_6 -> layer_8_output
	layer_8_all2all_back_7 -> layer_8_output
	layer_8_all2all_back_8 -> layer_8_output
	layer_8_all2all_back_9 -> layer_8_output
	layer_8_all2all_back_10 -> layer_8_output
	layer_8_all2all_back_11 -> layer_8_output
	layer_8_all2all_back_12 -> layer_8_output
	layer_8_all2all_back_13 -> layer_8_output
	layer_8_all2all_back_14 -> layer_8_output
	layer_8_all2all_back_15 -> layer_8_output
	layer_9_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_9_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_8_output -> layer_9_tp_gather
	layer_9_tp_gather -> layer_9_qkv_gpu4
	layer_9_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_9_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_8_output -> layer_9_tp_gather
	layer_9_tp_gather -> layer_9_qkv_gpu5
	layer_9_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_9_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_8_output -> layer_9_tp_gather
	layer_9_tp_gather -> layer_9_qkv_gpu6
	layer_9_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_9_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_8_output -> layer_9_tp_gather
	layer_9_tp_gather -> layer_9_qkv_gpu7
	layer_9_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_9_qkv_gpu4 -> layer_9_attn_gpu4
	layer_9_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_9_qkv_gpu5 -> layer_9_attn_gpu5
	layer_9_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_9_qkv_gpu6 -> layer_9_attn_gpu6
	layer_9_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_9_qkv_gpu7 -> layer_9_attn_gpu7
	layer_9_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_attn_gpu4 -> layer_9_attn_out_gpu4
	layer_9_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_attn_gpu5 -> layer_9_attn_out_gpu5
	layer_9_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_attn_gpu6 -> layer_9_attn_out_gpu6
	layer_9_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_attn_gpu7 -> layer_9_attn_out_gpu7
	layer_9_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_9_attn_out_gpu4 -> layer_9_attn_allreduce
	layer_9_attn_out_gpu5 -> layer_9_attn_allreduce
	layer_9_attn_out_gpu6 -> layer_9_attn_allreduce
	layer_9_attn_out_gpu7 -> layer_9_attn_allreduce
	layer_9_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_9_attn_allreduce -> layer_9_gate [style=dashed]
	layer_9_attn_allreduce -> layer_9_gate [style=dashed]
	layer_9_attn_allreduce -> layer_9_gate [style=dashed]
	layer_9_attn_allreduce -> layer_9_gate [style=dashed]
	layer_9_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_0 [style=dashed]
	layer_9_all2all_expert_0 -> layer_9_expert_0_gpu8
	layer_9_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_1 [style=dashed]
	layer_9_all2all_expert_1 -> layer_9_expert_1_gpu8
	layer_9_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_2 [style=dashed]
	layer_9_all2all_expert_2 -> layer_9_expert_2_gpu9
	layer_9_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_3 [style=dashed]
	layer_9_all2all_expert_3 -> layer_9_expert_3_gpu9
	layer_9_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_4 [style=dashed]
	layer_9_all2all_expert_4 -> layer_9_expert_4_gpu10
	layer_9_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_5 [style=dashed]
	layer_9_all2all_expert_5 -> layer_9_expert_5_gpu10
	layer_9_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_6 [style=dashed]
	layer_9_all2all_expert_6 -> layer_9_expert_6_gpu11
	layer_9_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_7 [style=dashed]
	layer_9_all2all_expert_7 -> layer_9_expert_7_gpu11
	layer_9_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_8 [style=dashed]
	layer_9_all2all_expert_8 -> layer_9_expert_8_gpu12
	layer_9_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_9 [style=dashed]
	layer_9_all2all_expert_9 -> layer_9_expert_9_gpu12
	layer_9_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_10 [style=dashed]
	layer_9_all2all_expert_10 -> layer_9_expert_10_gpu13
	layer_9_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_11 [style=dashed]
	layer_9_all2all_expert_11 -> layer_9_expert_11_gpu13
	layer_9_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_12 [style=dashed]
	layer_9_all2all_expert_12 -> layer_9_expert_12_gpu14
	layer_9_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_13 [style=dashed]
	layer_9_all2all_expert_13 -> layer_9_expert_13_gpu14
	layer_9_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_14 [style=dashed]
	layer_9_all2all_expert_14 -> layer_9_expert_14_gpu15
	layer_9_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_gate -> layer_9_all2all_expert_15 [style=dashed]
	layer_9_all2all_expert_15 -> layer_9_expert_15_gpu15
	layer_9_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_0_gpu8 -> layer_9_all2all_back_0
	layer_9_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_1_gpu8 -> layer_9_all2all_back_1
	layer_9_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_2_gpu9 -> layer_9_all2all_back_2
	layer_9_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_3_gpu9 -> layer_9_all2all_back_3
	layer_9_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_4_gpu10 -> layer_9_all2all_back_4
	layer_9_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_5_gpu10 -> layer_9_all2all_back_5
	layer_9_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_6_gpu11 -> layer_9_all2all_back_6
	layer_9_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_7_gpu11 -> layer_9_all2all_back_7
	layer_9_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_8_gpu12 -> layer_9_all2all_back_8
	layer_9_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_9_gpu12 -> layer_9_all2all_back_9
	layer_9_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_10_gpu13 -> layer_9_all2all_back_10
	layer_9_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_11_gpu13 -> layer_9_all2all_back_11
	layer_9_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_12_gpu14 -> layer_9_all2all_back_12
	layer_9_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_13_gpu14 -> layer_9_all2all_back_13
	layer_9_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_14_gpu15 -> layer_9_all2all_back_14
	layer_9_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_9_expert_15_gpu15 -> layer_9_all2all_back_15
	layer_9_output [label="Layer 9 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_9_all2all_back_0 -> layer_9_output
	layer_9_all2all_back_1 -> layer_9_output
	layer_9_all2all_back_2 -> layer_9_output
	layer_9_all2all_back_3 -> layer_9_output
	layer_9_all2all_back_4 -> layer_9_output
	layer_9_all2all_back_5 -> layer_9_output
	layer_9_all2all_back_6 -> layer_9_output
	layer_9_all2all_back_7 -> layer_9_output
	layer_9_all2all_back_8 -> layer_9_output
	layer_9_all2all_back_9 -> layer_9_output
	layer_9_all2all_back_10 -> layer_9_output
	layer_9_all2all_back_11 -> layer_9_output
	layer_9_all2all_back_12 -> layer_9_output
	layer_9_all2all_back_13 -> layer_9_output
	layer_9_all2all_back_14 -> layer_9_output
	layer_9_all2all_back_15 -> layer_9_output
	layer_10_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_10_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_9_output -> layer_10_tp_gather
	layer_10_tp_gather -> layer_10_qkv_gpu4
	layer_10_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_10_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_9_output -> layer_10_tp_gather
	layer_10_tp_gather -> layer_10_qkv_gpu5
	layer_10_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_10_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_9_output -> layer_10_tp_gather
	layer_10_tp_gather -> layer_10_qkv_gpu6
	layer_10_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_10_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_9_output -> layer_10_tp_gather
	layer_10_tp_gather -> layer_10_qkv_gpu7
	layer_10_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_10_qkv_gpu4 -> layer_10_attn_gpu4
	layer_10_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_10_qkv_gpu5 -> layer_10_attn_gpu5
	layer_10_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_10_qkv_gpu6 -> layer_10_attn_gpu6
	layer_10_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_10_qkv_gpu7 -> layer_10_attn_gpu7
	layer_10_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_attn_gpu4 -> layer_10_attn_out_gpu4
	layer_10_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_attn_gpu5 -> layer_10_attn_out_gpu5
	layer_10_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_attn_gpu6 -> layer_10_attn_out_gpu6
	layer_10_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_attn_gpu7 -> layer_10_attn_out_gpu7
	layer_10_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_10_attn_out_gpu4 -> layer_10_attn_allreduce
	layer_10_attn_out_gpu5 -> layer_10_attn_allreduce
	layer_10_attn_out_gpu6 -> layer_10_attn_allreduce
	layer_10_attn_out_gpu7 -> layer_10_attn_allreduce
	layer_10_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_10_attn_allreduce -> layer_10_gate [style=dashed]
	layer_10_attn_allreduce -> layer_10_gate [style=dashed]
	layer_10_attn_allreduce -> layer_10_gate [style=dashed]
	layer_10_attn_allreduce -> layer_10_gate [style=dashed]
	layer_10_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_0 [style=dashed]
	layer_10_all2all_expert_0 -> layer_10_expert_0_gpu8
	layer_10_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_1 [style=dashed]
	layer_10_all2all_expert_1 -> layer_10_expert_1_gpu8
	layer_10_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_2 [style=dashed]
	layer_10_all2all_expert_2 -> layer_10_expert_2_gpu9
	layer_10_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_3 [style=dashed]
	layer_10_all2all_expert_3 -> layer_10_expert_3_gpu9
	layer_10_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_4 [style=dashed]
	layer_10_all2all_expert_4 -> layer_10_expert_4_gpu10
	layer_10_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_5 [style=dashed]
	layer_10_all2all_expert_5 -> layer_10_expert_5_gpu10
	layer_10_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_6 [style=dashed]
	layer_10_all2all_expert_6 -> layer_10_expert_6_gpu11
	layer_10_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_7 [style=dashed]
	layer_10_all2all_expert_7 -> layer_10_expert_7_gpu11
	layer_10_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_8 [style=dashed]
	layer_10_all2all_expert_8 -> layer_10_expert_8_gpu12
	layer_10_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_9 [style=dashed]
	layer_10_all2all_expert_9 -> layer_10_expert_9_gpu12
	layer_10_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_10 [style=dashed]
	layer_10_all2all_expert_10 -> layer_10_expert_10_gpu13
	layer_10_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_11 [style=dashed]
	layer_10_all2all_expert_11 -> layer_10_expert_11_gpu13
	layer_10_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_12 [style=dashed]
	layer_10_all2all_expert_12 -> layer_10_expert_12_gpu14
	layer_10_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_13 [style=dashed]
	layer_10_all2all_expert_13 -> layer_10_expert_13_gpu14
	layer_10_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_14 [style=dashed]
	layer_10_all2all_expert_14 -> layer_10_expert_14_gpu15
	layer_10_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_gate -> layer_10_all2all_expert_15 [style=dashed]
	layer_10_all2all_expert_15 -> layer_10_expert_15_gpu15
	layer_10_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_0_gpu8 -> layer_10_all2all_back_0
	layer_10_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_1_gpu8 -> layer_10_all2all_back_1
	layer_10_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_2_gpu9 -> layer_10_all2all_back_2
	layer_10_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_3_gpu9 -> layer_10_all2all_back_3
	layer_10_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_4_gpu10 -> layer_10_all2all_back_4
	layer_10_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_5_gpu10 -> layer_10_all2all_back_5
	layer_10_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_6_gpu11 -> layer_10_all2all_back_6
	layer_10_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_7_gpu11 -> layer_10_all2all_back_7
	layer_10_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_8_gpu12 -> layer_10_all2all_back_8
	layer_10_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_9_gpu12 -> layer_10_all2all_back_9
	layer_10_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_10_gpu13 -> layer_10_all2all_back_10
	layer_10_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_11_gpu13 -> layer_10_all2all_back_11
	layer_10_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_12_gpu14 -> layer_10_all2all_back_12
	layer_10_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_13_gpu14 -> layer_10_all2all_back_13
	layer_10_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_14_gpu15 -> layer_10_all2all_back_14
	layer_10_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_10_expert_15_gpu15 -> layer_10_all2all_back_15
	layer_10_output [label="Layer 10 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_10_all2all_back_0 -> layer_10_output
	layer_10_all2all_back_1 -> layer_10_output
	layer_10_all2all_back_2 -> layer_10_output
	layer_10_all2all_back_3 -> layer_10_output
	layer_10_all2all_back_4 -> layer_10_output
	layer_10_all2all_back_5 -> layer_10_output
	layer_10_all2all_back_6 -> layer_10_output
	layer_10_all2all_back_7 -> layer_10_output
	layer_10_all2all_back_8 -> layer_10_output
	layer_10_all2all_back_9 -> layer_10_output
	layer_10_all2all_back_10 -> layer_10_output
	layer_10_all2all_back_11 -> layer_10_output
	layer_10_all2all_back_12 -> layer_10_output
	layer_10_all2all_back_13 -> layer_10_output
	layer_10_all2all_back_14 -> layer_10_output
	layer_10_all2all_back_15 -> layer_10_output
	layer_11_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_11_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_10_output -> layer_11_tp_gather
	layer_11_tp_gather -> layer_11_qkv_gpu4
	layer_11_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_11_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_10_output -> layer_11_tp_gather
	layer_11_tp_gather -> layer_11_qkv_gpu5
	layer_11_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_11_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_10_output -> layer_11_tp_gather
	layer_11_tp_gather -> layer_11_qkv_gpu6
	layer_11_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_11_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_10_output -> layer_11_tp_gather
	layer_11_tp_gather -> layer_11_qkv_gpu7
	layer_11_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_11_qkv_gpu4 -> layer_11_attn_gpu4
	layer_11_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_11_qkv_gpu5 -> layer_11_attn_gpu5
	layer_11_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_11_qkv_gpu6 -> layer_11_attn_gpu6
	layer_11_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_11_qkv_gpu7 -> layer_11_attn_gpu7
	layer_11_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_attn_gpu4 -> layer_11_attn_out_gpu4
	layer_11_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_attn_gpu5 -> layer_11_attn_out_gpu5
	layer_11_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_attn_gpu6 -> layer_11_attn_out_gpu6
	layer_11_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_attn_gpu7 -> layer_11_attn_out_gpu7
	layer_11_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_11_attn_out_gpu4 -> layer_11_attn_allreduce
	layer_11_attn_out_gpu5 -> layer_11_attn_allreduce
	layer_11_attn_out_gpu6 -> layer_11_attn_allreduce
	layer_11_attn_out_gpu7 -> layer_11_attn_allreduce
	layer_11_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_11_attn_allreduce -> layer_11_gate [style=dashed]
	layer_11_attn_allreduce -> layer_11_gate [style=dashed]
	layer_11_attn_allreduce -> layer_11_gate [style=dashed]
	layer_11_attn_allreduce -> layer_11_gate [style=dashed]
	layer_11_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_0 [style=dashed]
	layer_11_all2all_expert_0 -> layer_11_expert_0_gpu8
	layer_11_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_1 [style=dashed]
	layer_11_all2all_expert_1 -> layer_11_expert_1_gpu8
	layer_11_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_2 [style=dashed]
	layer_11_all2all_expert_2 -> layer_11_expert_2_gpu9
	layer_11_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_3 [style=dashed]
	layer_11_all2all_expert_3 -> layer_11_expert_3_gpu9
	layer_11_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_4 [style=dashed]
	layer_11_all2all_expert_4 -> layer_11_expert_4_gpu10
	layer_11_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_5 [style=dashed]
	layer_11_all2all_expert_5 -> layer_11_expert_5_gpu10
	layer_11_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_6 [style=dashed]
	layer_11_all2all_expert_6 -> layer_11_expert_6_gpu11
	layer_11_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_7 [style=dashed]
	layer_11_all2all_expert_7 -> layer_11_expert_7_gpu11
	layer_11_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_8 [style=dashed]
	layer_11_all2all_expert_8 -> layer_11_expert_8_gpu12
	layer_11_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_9 [style=dashed]
	layer_11_all2all_expert_9 -> layer_11_expert_9_gpu12
	layer_11_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_10 [style=dashed]
	layer_11_all2all_expert_10 -> layer_11_expert_10_gpu13
	layer_11_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_11 [style=dashed]
	layer_11_all2all_expert_11 -> layer_11_expert_11_gpu13
	layer_11_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_12 [style=dashed]
	layer_11_all2all_expert_12 -> layer_11_expert_12_gpu14
	layer_11_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_13 [style=dashed]
	layer_11_all2all_expert_13 -> layer_11_expert_13_gpu14
	layer_11_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_14 [style=dashed]
	layer_11_all2all_expert_14 -> layer_11_expert_14_gpu15
	layer_11_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_gate -> layer_11_all2all_expert_15 [style=dashed]
	layer_11_all2all_expert_15 -> layer_11_expert_15_gpu15
	layer_11_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_0_gpu8 -> layer_11_all2all_back_0
	layer_11_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_1_gpu8 -> layer_11_all2all_back_1
	layer_11_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_2_gpu9 -> layer_11_all2all_back_2
	layer_11_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_3_gpu9 -> layer_11_all2all_back_3
	layer_11_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_4_gpu10 -> layer_11_all2all_back_4
	layer_11_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_5_gpu10 -> layer_11_all2all_back_5
	layer_11_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_6_gpu11 -> layer_11_all2all_back_6
	layer_11_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_7_gpu11 -> layer_11_all2all_back_7
	layer_11_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_8_gpu12 -> layer_11_all2all_back_8
	layer_11_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_9_gpu12 -> layer_11_all2all_back_9
	layer_11_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_10_gpu13 -> layer_11_all2all_back_10
	layer_11_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_11_gpu13 -> layer_11_all2all_back_11
	layer_11_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_12_gpu14 -> layer_11_all2all_back_12
	layer_11_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_13_gpu14 -> layer_11_all2all_back_13
	layer_11_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_14_gpu15 -> layer_11_all2all_back_14
	layer_11_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_11_expert_15_gpu15 -> layer_11_all2all_back_15
	layer_11_output [label="Layer 11 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_11_all2all_back_0 -> layer_11_output
	layer_11_all2all_back_1 -> layer_11_output
	layer_11_all2all_back_2 -> layer_11_output
	layer_11_all2all_back_3 -> layer_11_output
	layer_11_all2all_back_4 -> layer_11_output
	layer_11_all2all_back_5 -> layer_11_output
	layer_11_all2all_back_6 -> layer_11_output
	layer_11_all2all_back_7 -> layer_11_output
	layer_11_all2all_back_8 -> layer_11_output
	layer_11_all2all_back_9 -> layer_11_output
	layer_11_all2all_back_10 -> layer_11_output
	layer_11_all2all_back_11 -> layer_11_output
	layer_11_all2all_back_12 -> layer_11_output
	layer_11_all2all_back_13 -> layer_11_output
	layer_11_all2all_back_14 -> layer_11_output
	layer_11_all2all_back_15 -> layer_11_output
	layer_12_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_12_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_11_output -> layer_12_tp_gather
	layer_12_tp_gather -> layer_12_qkv_gpu4
	layer_12_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_12_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_11_output -> layer_12_tp_gather
	layer_12_tp_gather -> layer_12_qkv_gpu5
	layer_12_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_12_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_11_output -> layer_12_tp_gather
	layer_12_tp_gather -> layer_12_qkv_gpu6
	layer_12_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_12_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_11_output -> layer_12_tp_gather
	layer_12_tp_gather -> layer_12_qkv_gpu7
	layer_12_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_12_qkv_gpu4 -> layer_12_attn_gpu4
	layer_12_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_12_qkv_gpu5 -> layer_12_attn_gpu5
	layer_12_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_12_qkv_gpu6 -> layer_12_attn_gpu6
	layer_12_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_12_qkv_gpu7 -> layer_12_attn_gpu7
	layer_12_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_attn_gpu4 -> layer_12_attn_out_gpu4
	layer_12_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_attn_gpu5 -> layer_12_attn_out_gpu5
	layer_12_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_attn_gpu6 -> layer_12_attn_out_gpu6
	layer_12_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_attn_gpu7 -> layer_12_attn_out_gpu7
	layer_12_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_12_attn_out_gpu4 -> layer_12_attn_allreduce
	layer_12_attn_out_gpu5 -> layer_12_attn_allreduce
	layer_12_attn_out_gpu6 -> layer_12_attn_allreduce
	layer_12_attn_out_gpu7 -> layer_12_attn_allreduce
	layer_12_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_12_attn_allreduce -> layer_12_gate [style=dashed]
	layer_12_attn_allreduce -> layer_12_gate [style=dashed]
	layer_12_attn_allreduce -> layer_12_gate [style=dashed]
	layer_12_attn_allreduce -> layer_12_gate [style=dashed]
	layer_12_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_0 [style=dashed]
	layer_12_all2all_expert_0 -> layer_12_expert_0_gpu8
	layer_12_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_1 [style=dashed]
	layer_12_all2all_expert_1 -> layer_12_expert_1_gpu8
	layer_12_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_2 [style=dashed]
	layer_12_all2all_expert_2 -> layer_12_expert_2_gpu9
	layer_12_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_3 [style=dashed]
	layer_12_all2all_expert_3 -> layer_12_expert_3_gpu9
	layer_12_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_4 [style=dashed]
	layer_12_all2all_expert_4 -> layer_12_expert_4_gpu10
	layer_12_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_5 [style=dashed]
	layer_12_all2all_expert_5 -> layer_12_expert_5_gpu10
	layer_12_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_6 [style=dashed]
	layer_12_all2all_expert_6 -> layer_12_expert_6_gpu11
	layer_12_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_7 [style=dashed]
	layer_12_all2all_expert_7 -> layer_12_expert_7_gpu11
	layer_12_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_8 [style=dashed]
	layer_12_all2all_expert_8 -> layer_12_expert_8_gpu12
	layer_12_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_9 [style=dashed]
	layer_12_all2all_expert_9 -> layer_12_expert_9_gpu12
	layer_12_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_10 [style=dashed]
	layer_12_all2all_expert_10 -> layer_12_expert_10_gpu13
	layer_12_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_11 [style=dashed]
	layer_12_all2all_expert_11 -> layer_12_expert_11_gpu13
	layer_12_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_12 [style=dashed]
	layer_12_all2all_expert_12 -> layer_12_expert_12_gpu14
	layer_12_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_13 [style=dashed]
	layer_12_all2all_expert_13 -> layer_12_expert_13_gpu14
	layer_12_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_14 [style=dashed]
	layer_12_all2all_expert_14 -> layer_12_expert_14_gpu15
	layer_12_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_gate -> layer_12_all2all_expert_15 [style=dashed]
	layer_12_all2all_expert_15 -> layer_12_expert_15_gpu15
	layer_12_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_0_gpu8 -> layer_12_all2all_back_0
	layer_12_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_1_gpu8 -> layer_12_all2all_back_1
	layer_12_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_2_gpu9 -> layer_12_all2all_back_2
	layer_12_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_3_gpu9 -> layer_12_all2all_back_3
	layer_12_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_4_gpu10 -> layer_12_all2all_back_4
	layer_12_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_5_gpu10 -> layer_12_all2all_back_5
	layer_12_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_6_gpu11 -> layer_12_all2all_back_6
	layer_12_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_7_gpu11 -> layer_12_all2all_back_7
	layer_12_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_8_gpu12 -> layer_12_all2all_back_8
	layer_12_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_9_gpu12 -> layer_12_all2all_back_9
	layer_12_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_10_gpu13 -> layer_12_all2all_back_10
	layer_12_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_11_gpu13 -> layer_12_all2all_back_11
	layer_12_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_12_gpu14 -> layer_12_all2all_back_12
	layer_12_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_13_gpu14 -> layer_12_all2all_back_13
	layer_12_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_14_gpu15 -> layer_12_all2all_back_14
	layer_12_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_12_expert_15_gpu15 -> layer_12_all2all_back_15
	layer_12_output [label="Layer 12 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_12_all2all_back_0 -> layer_12_output
	layer_12_all2all_back_1 -> layer_12_output
	layer_12_all2all_back_2 -> layer_12_output
	layer_12_all2all_back_3 -> layer_12_output
	layer_12_all2all_back_4 -> layer_12_output
	layer_12_all2all_back_5 -> layer_12_output
	layer_12_all2all_back_6 -> layer_12_output
	layer_12_all2all_back_7 -> layer_12_output
	layer_12_all2all_back_8 -> layer_12_output
	layer_12_all2all_back_9 -> layer_12_output
	layer_12_all2all_back_10 -> layer_12_output
	layer_12_all2all_back_11 -> layer_12_output
	layer_12_all2all_back_12 -> layer_12_output
	layer_12_all2all_back_13 -> layer_12_output
	layer_12_all2all_back_14 -> layer_12_output
	layer_12_all2all_back_15 -> layer_12_output
	layer_13_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_13_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_12_output -> layer_13_tp_gather
	layer_13_tp_gather -> layer_13_qkv_gpu4
	layer_13_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_13_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_12_output -> layer_13_tp_gather
	layer_13_tp_gather -> layer_13_qkv_gpu5
	layer_13_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_13_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_12_output -> layer_13_tp_gather
	layer_13_tp_gather -> layer_13_qkv_gpu6
	layer_13_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_13_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_12_output -> layer_13_tp_gather
	layer_13_tp_gather -> layer_13_qkv_gpu7
	layer_13_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_13_qkv_gpu4 -> layer_13_attn_gpu4
	layer_13_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_13_qkv_gpu5 -> layer_13_attn_gpu5
	layer_13_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_13_qkv_gpu6 -> layer_13_attn_gpu6
	layer_13_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_13_qkv_gpu7 -> layer_13_attn_gpu7
	layer_13_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_attn_gpu4 -> layer_13_attn_out_gpu4
	layer_13_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_attn_gpu5 -> layer_13_attn_out_gpu5
	layer_13_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_attn_gpu6 -> layer_13_attn_out_gpu6
	layer_13_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_attn_gpu7 -> layer_13_attn_out_gpu7
	layer_13_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_13_attn_out_gpu4 -> layer_13_attn_allreduce
	layer_13_attn_out_gpu5 -> layer_13_attn_allreduce
	layer_13_attn_out_gpu6 -> layer_13_attn_allreduce
	layer_13_attn_out_gpu7 -> layer_13_attn_allreduce
	layer_13_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_13_attn_allreduce -> layer_13_gate [style=dashed]
	layer_13_attn_allreduce -> layer_13_gate [style=dashed]
	layer_13_attn_allreduce -> layer_13_gate [style=dashed]
	layer_13_attn_allreduce -> layer_13_gate [style=dashed]
	layer_13_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_0 [style=dashed]
	layer_13_all2all_expert_0 -> layer_13_expert_0_gpu8
	layer_13_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_1 [style=dashed]
	layer_13_all2all_expert_1 -> layer_13_expert_1_gpu8
	layer_13_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_2 [style=dashed]
	layer_13_all2all_expert_2 -> layer_13_expert_2_gpu9
	layer_13_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_3 [style=dashed]
	layer_13_all2all_expert_3 -> layer_13_expert_3_gpu9
	layer_13_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_4 [style=dashed]
	layer_13_all2all_expert_4 -> layer_13_expert_4_gpu10
	layer_13_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_5 [style=dashed]
	layer_13_all2all_expert_5 -> layer_13_expert_5_gpu10
	layer_13_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_6 [style=dashed]
	layer_13_all2all_expert_6 -> layer_13_expert_6_gpu11
	layer_13_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_7 [style=dashed]
	layer_13_all2all_expert_7 -> layer_13_expert_7_gpu11
	layer_13_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_8 [style=dashed]
	layer_13_all2all_expert_8 -> layer_13_expert_8_gpu12
	layer_13_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_9 [style=dashed]
	layer_13_all2all_expert_9 -> layer_13_expert_9_gpu12
	layer_13_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_10 [style=dashed]
	layer_13_all2all_expert_10 -> layer_13_expert_10_gpu13
	layer_13_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_11 [style=dashed]
	layer_13_all2all_expert_11 -> layer_13_expert_11_gpu13
	layer_13_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_12 [style=dashed]
	layer_13_all2all_expert_12 -> layer_13_expert_12_gpu14
	layer_13_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_13 [style=dashed]
	layer_13_all2all_expert_13 -> layer_13_expert_13_gpu14
	layer_13_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_14 [style=dashed]
	layer_13_all2all_expert_14 -> layer_13_expert_14_gpu15
	layer_13_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_gate -> layer_13_all2all_expert_15 [style=dashed]
	layer_13_all2all_expert_15 -> layer_13_expert_15_gpu15
	layer_13_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_0_gpu8 -> layer_13_all2all_back_0
	layer_13_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_1_gpu8 -> layer_13_all2all_back_1
	layer_13_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_2_gpu9 -> layer_13_all2all_back_2
	layer_13_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_3_gpu9 -> layer_13_all2all_back_3
	layer_13_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_4_gpu10 -> layer_13_all2all_back_4
	layer_13_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_5_gpu10 -> layer_13_all2all_back_5
	layer_13_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_6_gpu11 -> layer_13_all2all_back_6
	layer_13_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_7_gpu11 -> layer_13_all2all_back_7
	layer_13_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_8_gpu12 -> layer_13_all2all_back_8
	layer_13_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_9_gpu12 -> layer_13_all2all_back_9
	layer_13_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_10_gpu13 -> layer_13_all2all_back_10
	layer_13_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_11_gpu13 -> layer_13_all2all_back_11
	layer_13_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_12_gpu14 -> layer_13_all2all_back_12
	layer_13_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_13_gpu14 -> layer_13_all2all_back_13
	layer_13_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_14_gpu15 -> layer_13_all2all_back_14
	layer_13_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_13_expert_15_gpu15 -> layer_13_all2all_back_15
	layer_13_output [label="Layer 13 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_13_all2all_back_0 -> layer_13_output
	layer_13_all2all_back_1 -> layer_13_output
	layer_13_all2all_back_2 -> layer_13_output
	layer_13_all2all_back_3 -> layer_13_output
	layer_13_all2all_back_4 -> layer_13_output
	layer_13_all2all_back_5 -> layer_13_output
	layer_13_all2all_back_6 -> layer_13_output
	layer_13_all2all_back_7 -> layer_13_output
	layer_13_all2all_back_8 -> layer_13_output
	layer_13_all2all_back_9 -> layer_13_output
	layer_13_all2all_back_10 -> layer_13_output
	layer_13_all2all_back_11 -> layer_13_output
	layer_13_all2all_back_12 -> layer_13_output
	layer_13_all2all_back_13 -> layer_13_output
	layer_13_all2all_back_14 -> layer_13_output
	layer_13_all2all_back_15 -> layer_13_output
	layer_14_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_14_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_13_output -> layer_14_tp_gather
	layer_14_tp_gather -> layer_14_qkv_gpu4
	layer_14_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_14_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_13_output -> layer_14_tp_gather
	layer_14_tp_gather -> layer_14_qkv_gpu5
	layer_14_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_14_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_13_output -> layer_14_tp_gather
	layer_14_tp_gather -> layer_14_qkv_gpu6
	layer_14_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_14_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_13_output -> layer_14_tp_gather
	layer_14_tp_gather -> layer_14_qkv_gpu7
	layer_14_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_14_qkv_gpu4 -> layer_14_attn_gpu4
	layer_14_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_14_qkv_gpu5 -> layer_14_attn_gpu5
	layer_14_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_14_qkv_gpu6 -> layer_14_attn_gpu6
	layer_14_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_14_qkv_gpu7 -> layer_14_attn_gpu7
	layer_14_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_attn_gpu4 -> layer_14_attn_out_gpu4
	layer_14_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_attn_gpu5 -> layer_14_attn_out_gpu5
	layer_14_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_attn_gpu6 -> layer_14_attn_out_gpu6
	layer_14_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_attn_gpu7 -> layer_14_attn_out_gpu7
	layer_14_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_14_attn_out_gpu4 -> layer_14_attn_allreduce
	layer_14_attn_out_gpu5 -> layer_14_attn_allreduce
	layer_14_attn_out_gpu6 -> layer_14_attn_allreduce
	layer_14_attn_out_gpu7 -> layer_14_attn_allreduce
	layer_14_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_14_attn_allreduce -> layer_14_gate [style=dashed]
	layer_14_attn_allreduce -> layer_14_gate [style=dashed]
	layer_14_attn_allreduce -> layer_14_gate [style=dashed]
	layer_14_attn_allreduce -> layer_14_gate [style=dashed]
	layer_14_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_0 [style=dashed]
	layer_14_all2all_expert_0 -> layer_14_expert_0_gpu8
	layer_14_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_1 [style=dashed]
	layer_14_all2all_expert_1 -> layer_14_expert_1_gpu8
	layer_14_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_2 [style=dashed]
	layer_14_all2all_expert_2 -> layer_14_expert_2_gpu9
	layer_14_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_3 [style=dashed]
	layer_14_all2all_expert_3 -> layer_14_expert_3_gpu9
	layer_14_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_4 [style=dashed]
	layer_14_all2all_expert_4 -> layer_14_expert_4_gpu10
	layer_14_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_5 [style=dashed]
	layer_14_all2all_expert_5 -> layer_14_expert_5_gpu10
	layer_14_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_6 [style=dashed]
	layer_14_all2all_expert_6 -> layer_14_expert_6_gpu11
	layer_14_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_7 [style=dashed]
	layer_14_all2all_expert_7 -> layer_14_expert_7_gpu11
	layer_14_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_8 [style=dashed]
	layer_14_all2all_expert_8 -> layer_14_expert_8_gpu12
	layer_14_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_9 [style=dashed]
	layer_14_all2all_expert_9 -> layer_14_expert_9_gpu12
	layer_14_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_10 [style=dashed]
	layer_14_all2all_expert_10 -> layer_14_expert_10_gpu13
	layer_14_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_11 [style=dashed]
	layer_14_all2all_expert_11 -> layer_14_expert_11_gpu13
	layer_14_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_12 [style=dashed]
	layer_14_all2all_expert_12 -> layer_14_expert_12_gpu14
	layer_14_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_13 [style=dashed]
	layer_14_all2all_expert_13 -> layer_14_expert_13_gpu14
	layer_14_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_14 [style=dashed]
	layer_14_all2all_expert_14 -> layer_14_expert_14_gpu15
	layer_14_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_gate -> layer_14_all2all_expert_15 [style=dashed]
	layer_14_all2all_expert_15 -> layer_14_expert_15_gpu15
	layer_14_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_0_gpu8 -> layer_14_all2all_back_0
	layer_14_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_1_gpu8 -> layer_14_all2all_back_1
	layer_14_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_2_gpu9 -> layer_14_all2all_back_2
	layer_14_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_3_gpu9 -> layer_14_all2all_back_3
	layer_14_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_4_gpu10 -> layer_14_all2all_back_4
	layer_14_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_5_gpu10 -> layer_14_all2all_back_5
	layer_14_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_6_gpu11 -> layer_14_all2all_back_6
	layer_14_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_7_gpu11 -> layer_14_all2all_back_7
	layer_14_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_8_gpu12 -> layer_14_all2all_back_8
	layer_14_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_9_gpu12 -> layer_14_all2all_back_9
	layer_14_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_10_gpu13 -> layer_14_all2all_back_10
	layer_14_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_11_gpu13 -> layer_14_all2all_back_11
	layer_14_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_12_gpu14 -> layer_14_all2all_back_12
	layer_14_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_13_gpu14 -> layer_14_all2all_back_13
	layer_14_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_14_gpu15 -> layer_14_all2all_back_14
	layer_14_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_14_expert_15_gpu15 -> layer_14_all2all_back_15
	layer_14_output [label="Layer 14 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_14_all2all_back_0 -> layer_14_output
	layer_14_all2all_back_1 -> layer_14_output
	layer_14_all2all_back_2 -> layer_14_output
	layer_14_all2all_back_3 -> layer_14_output
	layer_14_all2all_back_4 -> layer_14_output
	layer_14_all2all_back_5 -> layer_14_output
	layer_14_all2all_back_6 -> layer_14_output
	layer_14_all2all_back_7 -> layer_14_output
	layer_14_all2all_back_8 -> layer_14_output
	layer_14_all2all_back_9 -> layer_14_output
	layer_14_all2all_back_10 -> layer_14_output
	layer_14_all2all_back_11 -> layer_14_output
	layer_14_all2all_back_12 -> layer_14_output
	layer_14_all2all_back_13 -> layer_14_output
	layer_14_all2all_back_14 -> layer_14_output
	layer_14_all2all_back_15 -> layer_14_output
	layer_15_qkv_gpu4 [label="Q/K/V Projection
GPU: 4
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_15_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_14_output -> layer_15_tp_gather
	layer_15_tp_gather -> layer_15_qkv_gpu4
	layer_15_qkv_gpu5 [label="Q/K/V Projection
GPU: 5
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_15_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_14_output -> layer_15_tp_gather
	layer_15_tp_gather -> layer_15_qkv_gpu5
	layer_15_qkv_gpu6 [label="Q/K/V Projection
GPU: 6
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_15_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_14_output -> layer_15_tp_gather
	layer_15_tp_gather -> layer_15_qkv_gpu6
	layer_15_qkv_gpu7 [label="Q/K/V Projection
GPU: 7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, qkv=128]" fillcolor=lightblue shape=rectangle]
	layer_15_tp_gather [label="TP All-Gather
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=512]" fillcolor=lightgreen shape=ellipse]
	layer_14_output -> layer_15_tp_gather
	layer_15_tp_gather -> layer_15_qkv_gpu7
	layer_15_attn_gpu4 [label="Attention Computation
GPU: 4
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_15_qkv_gpu4 -> layer_15_attn_gpu4
	layer_15_attn_gpu5 [label="Attention Computation
GPU: 5
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_15_qkv_gpu5 -> layer_15_attn_gpu5
	layer_15_attn_gpu6 [label="Attention Computation
GPU: 6
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_15_qkv_gpu6 -> layer_15_attn_gpu6
	layer_15_attn_gpu7 [label="Attention Computation
GPU: 7
Input: [batch=128, seq=10240, qkv=128]
Output: [batch=128, seq=10240, attn=128]" fillcolor=lightblue shape=rectangle]
	layer_15_qkv_gpu7 -> layer_15_attn_gpu7
	layer_15_attn_out_gpu4 [label="Attention Output Projection
GPU: 4
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_attn_gpu4 -> layer_15_attn_out_gpu4
	layer_15_attn_out_gpu5 [label="Attention Output Projection
GPU: 5
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_attn_gpu5 -> layer_15_attn_out_gpu5
	layer_15_attn_out_gpu6 [label="Attention Output Projection
GPU: 6
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_attn_gpu6 -> layer_15_attn_out_gpu6
	layer_15_attn_out_gpu7 [label="Attention Output Projection
GPU: 7
Input: [batch=128, seq=10240, attn=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_attn_gpu7 -> layer_15_attn_out_gpu7
	layer_15_attn_allreduce [label="Attention TP All-Reduce
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse]
	layer_15_attn_out_gpu4 -> layer_15_attn_allreduce
	layer_15_attn_out_gpu5 -> layer_15_attn_allreduce
	layer_15_attn_out_gpu6 -> layer_15_attn_allreduce
	layer_15_attn_out_gpu7 -> layer_15_attn_allreduce
	layer_15_gate [label="Expert Gate Routing
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, expert_ids]" fillcolor=lightyellow shape=parallelogram]
	layer_15_attn_allreduce -> layer_15_gate [style=dashed]
	layer_15_attn_allreduce -> layer_15_gate [style=dashed]
	layer_15_attn_allreduce -> layer_15_gate [style=dashed]
	layer_15_attn_allreduce -> layer_15_gate [style=dashed]
	layer_15_expert_0_gpu8 [label="Expert 0
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_0 [label="All-to-All Expert 0
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_0 [style=dashed]
	layer_15_all2all_expert_0 -> layer_15_expert_0_gpu8
	layer_15_expert_1_gpu8 [label="Expert 1
GPU: 8
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_1 [label="All-to-All Expert 1
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_1 [style=dashed]
	layer_15_all2all_expert_1 -> layer_15_expert_1_gpu8
	layer_15_expert_2_gpu9 [label="Expert 2
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_2 [label="All-to-All Expert 2
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_2 [style=dashed]
	layer_15_all2all_expert_2 -> layer_15_expert_2_gpu9
	layer_15_expert_3_gpu9 [label="Expert 3
GPU: 9
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_3 [label="All-to-All Expert 3
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_3 [style=dashed]
	layer_15_all2all_expert_3 -> layer_15_expert_3_gpu9
	layer_15_expert_4_gpu10 [label="Expert 4
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_4 [label="All-to-All Expert 4
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_4 [style=dashed]
	layer_15_all2all_expert_4 -> layer_15_expert_4_gpu10
	layer_15_expert_5_gpu10 [label="Expert 5
GPU: 10
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_5 [label="All-to-All Expert 5
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_5 [style=dashed]
	layer_15_all2all_expert_5 -> layer_15_expert_5_gpu10
	layer_15_expert_6_gpu11 [label="Expert 6
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_6 [label="All-to-All Expert 6
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_6 [style=dashed]
	layer_15_all2all_expert_6 -> layer_15_expert_6_gpu11
	layer_15_expert_7_gpu11 [label="Expert 7
GPU: 11
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_7 [label="All-to-All Expert 7
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_7 [style=dashed]
	layer_15_all2all_expert_7 -> layer_15_expert_7_gpu11
	layer_15_expert_8_gpu12 [label="Expert 8
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_8 [label="All-to-All Expert 8
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_8 [style=dashed]
	layer_15_all2all_expert_8 -> layer_15_expert_8_gpu12
	layer_15_expert_9_gpu12 [label="Expert 9
GPU: 12
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_9 [label="All-to-All Expert 9
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_9 [style=dashed]
	layer_15_all2all_expert_9 -> layer_15_expert_9_gpu12
	layer_15_expert_10_gpu13 [label="Expert 10
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_10 [label="All-to-All Expert 10
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_10 [style=dashed]
	layer_15_all2all_expert_10 -> layer_15_expert_10_gpu13
	layer_15_expert_11_gpu13 [label="Expert 11
GPU: 13
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_11 [label="All-to-All Expert 11
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_11 [style=dashed]
	layer_15_all2all_expert_11 -> layer_15_expert_11_gpu13
	layer_15_expert_12_gpu14 [label="Expert 12
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_12 [label="All-to-All Expert 12
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_12 [style=dashed]
	layer_15_all2all_expert_12 -> layer_15_expert_12_gpu14
	layer_15_expert_13_gpu14 [label="Expert 13
GPU: 14
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_13 [label="All-to-All Expert 13
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_13 [style=dashed]
	layer_15_all2all_expert_13 -> layer_15_expert_13_gpu14
	layer_15_expert_14_gpu15 [label="Expert 14
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_14 [label="All-to-All Expert 14
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_14 [style=dashed]
	layer_15_all2all_expert_14 -> layer_15_expert_14_gpu15
	layer_15_expert_15_gpu15 [label="Expert 15
GPU: 15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_expert_15 [label="All-to-All Expert 15
GPU: 8-15
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=16, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_gate -> layer_15_all2all_expert_15 [style=dashed]
	layer_15_all2all_expert_15 -> layer_15_expert_15_gpu15
	layer_15_all2all_back_0 [label="All-to-All Back 0
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_0_gpu8 -> layer_15_all2all_back_0
	layer_15_all2all_back_1 [label="All-to-All Back 1
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_1_gpu8 -> layer_15_all2all_back_1
	layer_15_all2all_back_2 [label="All-to-All Back 2
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_2_gpu9 -> layer_15_all2all_back_2
	layer_15_all2all_back_3 [label="All-to-All Back 3
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_3_gpu9 -> layer_15_all2all_back_3
	layer_15_all2all_back_4 [label="All-to-All Back 4
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_4_gpu10 -> layer_15_all2all_back_4
	layer_15_all2all_back_5 [label="All-to-All Back 5
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_5_gpu10 -> layer_15_all2all_back_5
	layer_15_all2all_back_6 [label="All-to-All Back 6
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_6_gpu11 -> layer_15_all2all_back_6
	layer_15_all2all_back_7 [label="All-to-All Back 7
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_7_gpu11 -> layer_15_all2all_back_7
	layer_15_all2all_back_8 [label="All-to-All Back 8
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_8_gpu12 -> layer_15_all2all_back_8
	layer_15_all2all_back_9 [label="All-to-All Back 9
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_9_gpu12 -> layer_15_all2all_back_9
	layer_15_all2all_back_10 [label="All-to-All Back 10
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_10_gpu13 -> layer_15_all2all_back_10
	layer_15_all2all_back_11 [label="All-to-All Back 11
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_11_gpu13 -> layer_15_all2all_back_11
	layer_15_all2all_back_12 [label="All-to-All Back 12
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_12_gpu14 -> layer_15_all2all_back_12
	layer_15_all2all_back_13 [label="All-to-All Back 13
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_13_gpu14 -> layer_15_all2all_back_13
	layer_15_all2all_back_14 [label="All-to-All Back 14
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_14_gpu15 -> layer_15_all2all_back_14
	layer_15_all2all_back_15 [label="All-to-All Back 15
GPU: 8-15
Input: [batch=16, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightgreen shape=ellipse style=dashed]
	layer_15_expert_15_gpu15 -> layer_15_all2all_back_15
	layer_15_output [label="Layer 15 Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, hidden=128]" fillcolor=lightblue shape=rectangle]
	layer_15_all2all_back_0 -> layer_15_output
	layer_15_all2all_back_1 -> layer_15_output
	layer_15_all2all_back_2 -> layer_15_output
	layer_15_all2all_back_3 -> layer_15_output
	layer_15_all2all_back_4 -> layer_15_output
	layer_15_all2all_back_5 -> layer_15_output
	layer_15_all2all_back_6 -> layer_15_output
	layer_15_all2all_back_7 -> layer_15_output
	layer_15_all2all_back_8 -> layer_15_output
	layer_15_all2all_back_9 -> layer_15_output
	layer_15_all2all_back_10 -> layer_15_output
	layer_15_all2all_back_11 -> layer_15_output
	layer_15_all2all_back_12 -> layer_15_output
	layer_15_all2all_back_13 -> layer_15_output
	layer_15_all2all_back_14 -> layer_15_output
	layer_15_all2all_back_15 -> layer_15_output
	output [label="Final Output
GPU: 4-7
Input: [batch=128, seq=10240, hidden=128]
Output: [batch=128, seq=10240, vocab=50000]" fillcolor=lightcoral shape=rectangle]
	layer_15_output -> output
}
